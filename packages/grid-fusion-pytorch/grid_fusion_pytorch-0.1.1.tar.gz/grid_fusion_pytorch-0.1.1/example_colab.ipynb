{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JanNogga/grid_fusion_pytorch/blob/main/example_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ninja\n",
        "!pip install grid-fusion-pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuTm7xHHZFNE",
        "outputId": "34c21e1e-69ff-4923-8221-e940688540d0"
      },
      "id": "cuTm7xHHZFNE",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (1.11.1)\n",
            "Requirement already satisfied: grid-fusion-pytorch in /usr/local/lib/python3.10/dist-packages (0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9e019253",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e019253",
        "outputId": "9a85eb70-8e59-4b46-ff5a-b9b66586994e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/counting_model_util_cuda/build.ninja...\n",
            "Building extension module counting_model_util_cuda...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "Loading extension module counting_model_util_cuda...\n",
            "Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
            "Creating extension directory /root/.cache/torch_extensions/py310_cu118/point_cloud_fusion_util_cuda...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/point_cloud_fusion_util_cuda/build.ninja...\n",
            "Building extension module point_cloud_fusion_util_cuda...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "Loading extension module point_cloud_fusion_util_cuda...\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import timeit\n",
        "import numpy as np\n",
        "\n",
        "# these modules are compiled the first time they are loaded\n",
        "from torch_fusion.util import sample_rays, download_example_data, soften_semseg\n",
        "from torch_fusion.counting_model import apply_counting_model\n",
        "from torch_fusion.point_cloud_fusion import apply_point_cloud_fusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d34caa95",
      "metadata": {
        "id": "d34caa95"
      },
      "outputs": [],
      "source": [
        "# make sure to use a GPU session in colab\n",
        "assert torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0ee71f41",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ee71f41",
        "outputId": "3da68d78-55c7-47ed-d852-172f43bbe1f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data.\n",
            "Unpacking data archive.\n",
            "Removing data archive after unpacking.\n"
          ]
        }
      ],
      "source": [
        "# to show examples we need some example data - download it\n",
        "example_data_path = 'example_data_download'\n",
        "download_example_data(out_dir=example_data_path)\n",
        "local_files = os.path.join(example_data_path, 'grid_fusion_pytorch_example_data')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55b78542",
      "metadata": {
        "id": "55b78542"
      },
      "source": [
        "### Counting model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8f7e91f9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f7e91f9",
        "outputId": "0f2238cd-b4cd-45bf-d957-0dd3a36a5a1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded world dimensions lower limits. Shape: torch.Size([3])\n",
            "Loaded world dimensions upper limits. Shape: torch.Size([3])\n"
          ]
        }
      ],
      "source": [
        "# define batch size and number of classes for this example\n",
        "B, C = 1, 39\n",
        "# define resolution of the map\n",
        "H, W, D = 140, 112, 100\n",
        "# initialize empty counter map - the two channels are hits + misses\n",
        "counter_map_in = torch.zeros([B, 2, H, W, D]).to(device)\n",
        "# intialize semantic map with uniform distribution\n",
        "semantic_map_in = torch.ones([B, C, H, W, D]).to(device)\n",
        "semantic_map_in *= 1 / (semantic_map_in.shape[1])\n",
        "# convert probabilities to log probs\n",
        "semantic_map_in = torch.log(semantic_map_in)\n",
        "\n",
        "# furthermore we need to specify the dimensions of this map in the real world\n",
        "range_min = torch.load(os.path.join(local_files,'example_world_limits_lower.pt')).cuda()\n",
        "print('Loaded world dimensions lower limits. Shape:', range_min.shape)\n",
        "range_max = torch.load(os.path.join(local_files,'example_world_limits_upper.pt')).cuda()\n",
        "print('Loaded world dimensions upper limits. Shape:', range_max.shape)\n",
        "# this can be specified per batch element, else assume each map in the batch is equally large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9339446f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9339446f",
        "outputId": "375cf24a-9bf8-4733-c154-083758816f86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded batch of camera poses. Shape: torch.Size([1, 22, 4, 4])\n",
            "Loaded batch of camera intrinsics. Shape: torch.Size([1, 22, 3, 3])\n",
            "Loaded batch of depth images. Shape: torch.Size([1, 22, 1, 480, 640])\n",
            "Loaded batch of semantic segmentation masks. Shape: torch.Size([1, 22, 1, 480, 640])\n",
            "Calculated ray origins. Shape: torch.Size([1, 22, 3])\n",
            "Calculated ray directions. Shape: torch.Size([1, 22, 307200, 3])\n",
            "Reshaped depth accordingly. Shape: torch.Size([1, 22, 307200])\n",
            "Converted semantic segmentation to eps-soft log probs. Shape: torch.Size([1, 22, 307200, 39])\n"
          ]
        }
      ],
      "source": [
        "cam_pose_batch = torch.load(os.path.join(local_files,'example_cam_pose_batch.pt')).cuda()\n",
        "print('Loaded batch of camera poses. Shape:', cam_pose_batch.shape)\n",
        "cam_k_batch = torch.load(os.path.join(local_files,'example_cam_k_batch.pt')).cuda()\n",
        "print('Loaded batch of camera intrinsics. Shape:', cam_k_batch.shape)\n",
        "depth_batch = torch.load(os.path.join(local_files,'example_depth_batch.pt')).cuda()\n",
        "print('Loaded batch of depth images. Shape:', depth_batch.shape)\n",
        "semseg_batch = torch.load(os.path.join(local_files,'example_semseg_batch.pt')).cuda()\n",
        "print('Loaded batch of semantic segmentation masks. Shape:', semseg_batch.shape)\n",
        "\n",
        "ray_origs, ray_dirs, _, _ = sample_rays(cam_pose_batch, cam_k_batch, depth=depth_batch, normalize=False)\n",
        "print('Calculated ray origins. Shape:', ray_origs.shape)\n",
        "print('Calculated ray directions. Shape:', ray_dirs.shape)\n",
        "depth_reshape = depth_batch.squeeze(2).flatten(-2,-1)\n",
        "print('Reshaped depth accordingly. Shape:', depth_reshape.shape)\n",
        "# reshape semseg to match rays\n",
        "semseg_reshape = semseg_batch.squeeze(2).flatten(-2,-1)\n",
        "# convert class labels to eps-soft log probs\n",
        "semseg_soft = torch.log(soften_semseg(semseg_reshape))\n",
        "# mask background values with nan\n",
        "semseg_soft[semseg_reshape == -1] = torch.full((semseg_soft.shape[-1],), fill_value=float('nan'), device=semseg_soft.device)\n",
        "print('Converted semantic segmentation to eps-soft log probs. Shape:', semseg_soft.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "23954e5b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23954e5b",
        "outputId": "2cce7c4a-dfd7-422c-e850-7c0738e057f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: min_range should be 2D, but is not. Adding dummy batch dimension.\n",
            "Warning: max_range should be 2D, but is not. Adding dummy batch dimension.\n",
            "Computed output counter map. Shape: torch.Size([1, 2, 140, 112, 100]) \n",
            "\n",
            "Warning: min_range should be 2D, but is not. Adding dummy batch dimension.\n",
            "Warning: max_range should be 2D, but is not. Adding dummy batch dimension.\n",
            "Warning: semantic segmentation along rays detected, but no semantic map given! Applying counting model without Bayes filter.\n",
            "Computed output counter map. Shape: torch.Size([1, 2, 140, 112, 100]) \n",
            "\n",
            "Warning: min_range should be 2D, but is not. Adding dummy batch dimension.\n",
            "Warning: max_range should be 2D, but is not. Adding dummy batch dimension.\n",
            "Warning: semantic voxel grid detected, but no semantic segmentation provided for rays! Applying counting model without Bayes filter.\n",
            "Computed output counter map. Shape: torch.Size([1, 2, 140, 112, 100]) \n",
            "\n",
            "Warning: min_range should be 2D, but is not. Adding dummy batch dimension.\n",
            "Warning: max_range should be 2D, but is not. Adding dummy batch dimension.\n",
            "Computed output semantic map. Shape: torch.Size([1, 39, 140, 112, 100])\n",
            "Computed output counter map. Shape: torch.Size([1, 2, 140, 112, 100]) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# no semantic annotation for rays, no semantic maps\n",
        "counter_map_out_A = apply_counting_model(counter_map_in, ray_origs, ray_dirs,\n",
        "                                                         depth_reshape, range_min, range_max,\n",
        "                                                         grid_semantic=None,\n",
        "                                                         ray_semseg=None, n_steps=4096, verbose=True)\n",
        "print('Computed output counter map. Shape:', counter_map_out_A.shape, '\\n')\n",
        "# semantic annotation for rays, but no semantic maps\n",
        "counter_map_out_B = apply_counting_model(counter_map_in, ray_origs, ray_dirs,\n",
        "                                                         depth_reshape, range_min, range_max,\n",
        "                                                         grid_semantic=None,\n",
        "                                                         ray_semseg=semseg_soft, n_steps=4096, verbose=True)\n",
        "print('Computed output counter map. Shape:', counter_map_out_B.shape, '\\n')\n",
        "# no semantic annotation for rays, but semantic maps\n",
        "counter_map_out_C = apply_counting_model(counter_map_in, ray_origs, ray_dirs,\n",
        "                                                         depth_reshape, range_min, range_max,\n",
        "                                                         grid_semantic=semantic_map_in,\n",
        "                                                         ray_semseg=None, n_steps=4096, verbose=True)\n",
        "print('Computed output counter map. Shape:', counter_map_out_C.shape, '\\n')\n",
        "# semantic annotation for rays and semantic maps\n",
        "counter_map_out_D, semantic_map_out_D = apply_counting_model(counter_map_in, ray_origs, ray_dirs,\n",
        "                                                         depth_reshape, range_min, range_max,\n",
        "                                                         grid_semantic=semantic_map_in,\n",
        "                                                         ray_semseg=semseg_soft, n_steps=4096, verbose=True)\n",
        "print('Computed output semantic map. Shape:', semantic_map_out_D.shape)\n",
        "print('Computed output counter map. Shape:', counter_map_out_D.shape, '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3b7ea0bb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b7ea0bb",
        "outputId": "0707a5e8-67a4-4bf5-98a1-c58c2250414f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bayesian fusion took 0.07382672158999923 seconds per call using 6758400 rays.\n",
            "This corresponds to about 91.5 million ray casts per second.\n"
          ]
        }
      ],
      "source": [
        "n_trials = 10\n",
        "n_calls = 10\n",
        "timed_func = timeit.Timer(lambda: apply_counting_model(counter_map_in, ray_origs, ray_dirs, depth_reshape, range_min,\n",
        "                                                       range_max, grid_semantic=semantic_map_in, ray_semseg=semseg_soft,\n",
        "                                                       n_steps=4096, verbose=False, assert_inputs=False))\n",
        "exec_times = timed_func.repeat(repeat=n_trials, number=n_calls)\n",
        "num_rays = ray_dirs.shape[0]*ray_dirs.shape[1]*ray_dirs.shape[2]\n",
        "print(f\"Bayesian fusion took {np.mean(exec_times)/n_calls} seconds per call using {num_rays} rays.\")\n",
        "print(f\"This corresponds to about {np.round((num_rays/(np.mean(exec_times)/n_calls))/1000000,1)} million ray casts per second.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d254bf92",
      "metadata": {
        "id": "d254bf92"
      },
      "source": [
        "#### Variation - only count hits\n",
        "\n",
        "Just provide the same input but leave out the channel for misses in the counter maps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "64332bed",
      "metadata": {
        "id": "64332bed"
      },
      "outputs": [],
      "source": [
        "# define batch size and number of classes for this example\n",
        "B, C = 1, 39\n",
        "# define resolution of the map\n",
        "H, W, D = 140, 112, 100\n",
        "# initialize empty counter map - the one channel is for the hits counter\n",
        "counter_map_in = torch.zeros([B, 1, H, W, D]).to(device) # <--- !!! ONLY DIFFERENCE COMPARED TO ABOVE !!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "83456ada",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83456ada",
        "outputId": "a7564a5b-9c8c-40d8-d849-0184d1f5fd63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: min_range should be 2D, but is not. Adding dummy batch dimension.\n",
            "Warning: max_range should be 2D, but is not. Adding dummy batch dimension.\n",
            "Computed output occupancy map. Shape: torch.Size([1, 1, 140, 112, 100]) \n",
            "\n",
            "Warning: min_range should be 2D, but is not. Adding dummy batch dimension.\n",
            "Warning: max_range should be 2D, but is not. Adding dummy batch dimension.\n",
            "Warning: semantic segmentation along rays detected, but no semantic map given! Incrementing hit counter for foreground pixels without Bayes filter.\n",
            "Computed output occupancy map. Shape: torch.Size([1, 1, 140, 112, 100]) \n",
            "\n",
            "Warning: min_range should be 2D, but is not. Adding dummy batch dimension.\n",
            "Warning: max_range should be 2D, but is not. Adding dummy batch dimension.\n",
            "Warning: semantic voxel grid detected, but no semantic segmentation provided for rays! Counting hits without Bayes filter.\n",
            "Computed output occupancy map. Shape: torch.Size([1, 1, 140, 112, 100]) \n",
            "\n",
            "Warning: min_range should be 2D, but is not. Adding dummy batch dimension.\n",
            "Warning: max_range should be 2D, but is not. Adding dummy batch dimension.\n",
            "Computed output semantic map. Shape: torch.Size([1, 39, 140, 112, 100])\n",
            "Computed output occupancy map. Shape: torch.Size([1, 1, 140, 112, 100]) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# no semantic annotation for rays, no semantic maps\n",
        "occ_map_out_A = apply_counting_model(counter_map_in, ray_origs, ray_dirs,\n",
        "                                                         depth_reshape, range_min, range_max,\n",
        "                                                         grid_semantic=None,\n",
        "                                                         ray_semseg=None, n_steps=4096, verbose=True)\n",
        "print('Computed output occupancy map. Shape:', occ_map_out_A.shape, '\\n')\n",
        "# semantic annotation for rays, but no semantic maps\n",
        "occ_map_out_B = apply_counting_model(counter_map_in, ray_origs, ray_dirs,\n",
        "                                                         depth_reshape, range_min, range_max,\n",
        "                                                         grid_semantic=None,\n",
        "                                                         ray_semseg=semseg_soft, n_steps=4096, verbose=True)\n",
        "print('Computed output occupancy map. Shape:',occ_map_out_B.shape, '\\n')\n",
        "# no semantic annotation for rays, but semantic maps\n",
        "occ_map_out_C = apply_counting_model(counter_map_in, ray_origs, ray_dirs,\n",
        "                                                         depth_reshape, range_min, range_max,\n",
        "                                                         grid_semantic=semantic_map_in,\n",
        "                                                         ray_semseg=None, n_steps=4096, verbose=True)\n",
        "print('Computed output occupancy map. Shape:',occ_map_out_C.shape, '\\n')\n",
        "# semantic annotation for rays and semantic maps\n",
        "occ_map_out_D, semantic_map_out_D = apply_counting_model(counter_map_in, ray_origs, ray_dirs,\n",
        "                                                         depth_reshape, range_min, range_max,\n",
        "                                                         grid_semantic=semantic_map_in,\n",
        "                                                         ray_semseg=semseg_soft, n_steps=4096, verbose=True)\n",
        "print('Computed output semantic map. Shape:',semantic_map_out_D.shape)\n",
        "print('Computed output occupancy map. Shape:',occ_map_out_D.shape, '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "8cc2e003",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cc2e003",
        "outputId": "bc191041-3d57-4df1-b0ea-f10ff5adb346"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bayesian fusion took 0.003551652680000643 seconds per call using 6758400 rays.\n",
            "This corresponds to about 1.9 billion ray endpoints per second.\n"
          ]
        }
      ],
      "source": [
        "n_trials = 10\n",
        "n_calls = 10\n",
        "timed_func = timeit.Timer(lambda: apply_counting_model(counter_map_in, ray_origs, ray_dirs, depth_reshape, range_min,\n",
        "                                                       range_max, grid_semantic=semantic_map_in, ray_semseg=semseg_soft,\n",
        "                                                       n_steps=4096, verbose=False, assert_inputs=False))\n",
        "exec_times = timed_func.repeat(repeat=n_trials, number=n_calls)\n",
        "num_rays = ray_dirs.shape[0]*ray_dirs.shape[1]*ray_dirs.shape[2]\n",
        "print(f\"Bayesian fusion took {np.mean(exec_times)/n_calls} seconds per call using {num_rays} rays.\")\n",
        "print(f\"This corresponds to about {np.round((num_rays/(np.mean(exec_times)/n_calls))/1000000000,1)} billion ray endpoints per second.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "331a1f90",
      "metadata": {
        "id": "331a1f90"
      },
      "source": [
        "### Directly fuse points clouds\n",
        "\n",
        "Fusing point clouds with or without semantic annotation is possible too. Misses aren't counted in this setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "db021823",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db021823",
        "outputId": "7514a9c4-8cf2-43d9-f3f5-48a22e1ed209"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded world dimensions lower limits. Shape: torch.Size([3])\n",
            "Loaded world dimensions upper limits. Shape: torch.Size([3])\n"
          ]
        }
      ],
      "source": [
        "# define batch size and number of classes for this example\n",
        "B, C = 1, 39\n",
        "# define resolution of the map\n",
        "H, W, D = 140, 112, 100\n",
        "# initialize empty occupancy map\n",
        "occ_map_in = torch.zeros([B, 1, H, W, D]).to(device)\n",
        "# intialize semantic map with uniform distribution\n",
        "semantic_map_in = torch.ones([B, C, H, W, D]).to(device)\n",
        "semantic_map_in *= 1 / (semantic_map_in.shape[1])\n",
        "# convert probabilities to log probs\n",
        "semantic_map_in = torch.log(semantic_map_in)\n",
        "\n",
        "# furthermore we need to specify the dimensions of this map in the real world\n",
        "range_min = torch.load(os.path.join(local_files,'example_world_limits_lower.pt')).cuda()\n",
        "print('Loaded world dimensions lower limits. Shape:', range_min.shape)\n",
        "range_max = torch.load(os.path.join(local_files,'example_world_limits_upper.pt')).cuda()\n",
        "print('Loaded world dimensions upper limits. Shape:', range_max.shape)\n",
        "# this can be specified per batch element, else assume each map in the batch is equally large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c863ae3d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c863ae3d",
        "outputId": "0da81fb5-3bb0-4c72-ad94-366a393dcbf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded point cloud locations. Shape: torch.Size([1, 669621, 3])\n",
            "Loaded point cloud class probabilities. Shape: torch.Size([1, 669621, 39])\n"
          ]
        }
      ],
      "source": [
        "# load an example point cloud\n",
        "# load the 3D locations\n",
        "pc_locs = torch.load(os.path.join(local_files,'example_pointcloud_locations.pt')).cuda()\n",
        "print('Loaded point cloud locations. Shape:', pc_locs.shape)\n",
        "# next load the class probabilites per location\n",
        "pc_probs = torch.load(os.path.join(local_files,'example_pointcloud_semantics.pt')).cuda()\n",
        "print('Loaded point cloud class probabilities. Shape:', pc_probs.shape)\n",
        "# convert these to log probs\n",
        "pc_probs = torch.log(pc_probs)\n",
        "\n",
        "# to mask points so they are not fused, set their locations to nan\n",
        "test_masking = False\n",
        "if test_masking:\n",
        "    # arbitrarily decide which points to mask\n",
        "    invalid_mask = torch.rand_like(pc_locs[:,:,0]) > 0.5\n",
        "    print('Masking',invalid_mask.sum(),'locations!')\n",
        "    invalid_mask = invalid_mask.unsqueeze(-1).expand(-1,-1,pc_locs.shape[-1])\n",
        "    pc_locs[invalid_mask] = float('nan')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "4f192890",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f192890",
        "outputId": "a00a3446-d4f3-4043-93a5-bbd8104cfccb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: min_range should be 2D, but is not. Adding dummy batch dimension.\n",
            "Warning: max_range should be 2D, but is not. Adding dummy batch dimension.\n",
            "Computed output occupancy map. Shape: torch.Size([1, 1, 140, 112, 100]) \n",
            "\n",
            "Warning: min_range should be 2D, but is not. Adding dummy batch dimension.\n",
            "Warning: max_range should be 2D, but is not. Adding dummy batch dimension.\n",
            "Warning: semantic segmentation for point cloud detected, but no semantic map given! Incrementing hit counter without Bayes filter.\n",
            "Computed output occupancy map. Shape: torch.Size([1, 1, 140, 112, 100]) \n",
            "\n",
            "Warning: min_range should be 2D, but is not. Adding dummy batch dimension.\n",
            "Warning: max_range should be 2D, but is not. Adding dummy batch dimension.\n",
            "Warning: semantic voxel grid detected, but no semantic segmentation provided for point_clouds! Counting hits without Bayes filter.\n",
            "Computed output occupancy map. Shape: torch.Size([1, 1, 140, 112, 100]) \n",
            "\n",
            "Warning: min_range should be 2D, but is not. Adding dummy batch dimension.\n",
            "Warning: max_range should be 2D, but is not. Adding dummy batch dimension.\n",
            "Computed output semantic map. Shape: torch.Size([1, 39, 140, 112, 100])\n",
            "Computed output occupancy map. Shape: torch.Size([1, 1, 140, 112, 100]) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# no point cloud semantics, no semantic maps\n",
        "occ_map_out_A  = apply_point_cloud_fusion(occ_map_in, pc_locs, range_min, range_max,\n",
        "                                                      grid_semantic=None,\n",
        "                                                      point_cloud_logprobs=None, verbose=True)\n",
        "print('Computed output occupancy map. Shape:', occ_map_out_A.shape, '\\n')\n",
        "# point cloud semantics, but no semantic map\n",
        "occ_map_out_B  = apply_point_cloud_fusion(occ_map_in, pc_locs, range_min, range_max,\n",
        "                                                      grid_semantic=None,\n",
        "                                                      point_cloud_logprobs=pc_probs, verbose=True)\n",
        "print('Computed output occupancy map. Shape:', occ_map_out_B.shape, '\\n')\n",
        "# semantic map, but no point cloud semantics\n",
        "occ_map_out_C = apply_point_cloud_fusion(occ_map_in, pc_locs, range_min, range_max,\n",
        "                                                      grid_semantic=semantic_map_in,\n",
        "                                                      point_cloud_logprobs=None, verbose=True)\n",
        "print('Computed output occupancy map. Shape:', occ_map_out_C.shape, '\\n')\n",
        "# point cloud semantics and semantic map\n",
        "occ_map_out_D, semantic_map_out_D = apply_point_cloud_fusion(occ_map_in, pc_locs, range_min, range_max,\n",
        "                                                      grid_semantic=semantic_map_in,\n",
        "                                                      point_cloud_logprobs=pc_probs, verbose=True)\n",
        "print('Computed output semantic map. Shape:', semantic_map_out_D.shape)\n",
        "print('Computed output occupancy map. Shape:', occ_map_out_D.shape, '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "2ca87f50",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ca87f50",
        "outputId": "3b7d355f-bcf9-4dab-b709-f455d41d5dc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bayesian fusion took 0.0008241380300000855 seconds per call using 669621 points.\n",
            "This corresponds to about 812.5 million fused points per second.\n"
          ]
        }
      ],
      "source": [
        "n_trials = 10\n",
        "n_calls = 10\n",
        "timed_func = timeit.Timer(lambda: apply_point_cloud_fusion(occ_map_in, pc_locs, range_min, range_max,\n",
        "                                                           grid_semantic=semantic_map_in, point_cloud_logprobs=pc_probs,\n",
        "                                                           verbose=False, assert_inputs=False))\n",
        "exec_times = timed_func.repeat(repeat=n_trials, number=n_calls)\n",
        "num_points = pc_locs.shape[0] * pc_locs.shape[1]\n",
        "print(f\"Bayesian fusion took {np.mean(exec_times)/n_calls} seconds per call using {num_points} points.\")\n",
        "print(f\"This corresponds to about {np.round((num_points/(np.mean(exec_times)/n_calls))/1000000,1)} million fused points per second.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "7a518d3d",
      "metadata": {
        "id": "7a518d3d"
      },
      "outputs": [],
      "source": [
        "# TODO:\n",
        "# sanity-check for scene batch size > 1\n",
        "# document apply_ functions\n",
        "# make a simple vis for inputs and outputs in util"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f1af717",
      "metadata": {
        "id": "0f1af717"
      },
      "source": [
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "def animate_slice(counter, title='untitled', mode='w', fps=20, blit=True, vmax=128):\n",
        "    fig = plt.figure()\n",
        "    plt.axis('off')\n",
        "    if mode == 'w':\n",
        "        out_slice_w = counter[0,-1,:,:]\n",
        "        im=plt.imshow(out_slice_w.cpu().flip(-1).T,cmap='Greys', vmin=0, vmax=vmax)\n",
        "        frames = counter.shape[-3]\n",
        "        def init():\n",
        "            im.set_data(out_slice_w.cpu().flip(-1).T)\n",
        "            return [im]\n",
        "        def animate(i):\n",
        "            out_slice_w = counter[0,-(i+1),:,:]\n",
        "            im.set_array(out_slice_w.cpu().flip(-1).T)\n",
        "            return [im]\n",
        "    elif mode == 'h':\n",
        "        out_slice_h = counter[0,:,-1,:]\n",
        "        im=plt.imshow(out_slice_h.cpu().flip(-1).T,cmap='Greys', vmin=0, vmax=vmax)\n",
        "        frames = counter.shape[-2]\n",
        "        def init():\n",
        "            im.set_data(out_slice_h.cpu().flip(-1).T)\n",
        "            return [im]\n",
        "        def animate(i):\n",
        "            out_slice_h = counter[0,:,-(i+1),:]\n",
        "            im.set_array(out_slice_h.cpu().flip(-1).T)\n",
        "            return [im]\n",
        "    else:\n",
        "        out_slice_d = counter[0,:,:,-1]\n",
        "        im=plt.imshow(out_slice_d.cpu().T,cmap='Greys', vmin=0, vmax=vmax)\n",
        "        frames = counter.shape[-1]\n",
        "        def init():\n",
        "            im.set_data(out_slice_d.cpu().T)\n",
        "            return [im]\n",
        "        def animate(i):\n",
        "            out_slice_d = counter[0,:,:,-(i+1)]\n",
        "            im.set_array(out_slice_d.cpu().T)\n",
        "            return [im]\n",
        "    #print(frames)\n",
        "    anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
        "                               frames=frames, interval=20, blit=blit)\n",
        "    anim.save('slice_'+title+'_'+mode+'.gif', fps=fps)\n",
        "    plt.close()\n",
        "    return None"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}