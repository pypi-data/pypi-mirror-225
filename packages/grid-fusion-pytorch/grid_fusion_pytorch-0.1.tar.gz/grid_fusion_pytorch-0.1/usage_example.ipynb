{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "963e9df3",
   "metadata": {},
   "source": [
    "setup a dataset, do one fusion step, render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e019253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/nogga/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/nogga/.cache/torch_extensions/py38_cu121/counting_model_util_cuda/build.ninja...\n",
      "Building extension module counting_model_util_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module counting_model_util_cuda...\n",
      "Using /home/nogga/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/nogga/.cache/torch_extensions/py38_cu121/point_cloud_fusion_util_cuda/build.ninja...\n",
      "Building extension module point_cloud_fusion_util_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module point_cloud_fusion_util_cuda...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import timeit\n",
    "import numpy as np\n",
    "\n",
    "from util import sample_rays, download_example_data, soften_semseg\n",
    "from lib.counting_model import apply_counting_model\n",
    "from lib.point_cloud_fusion import apply_point_cloud_fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d34caa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1af717",
   "metadata": {},
   "source": [
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "def animate_slice(counter, title='untitled', mode='w', fps=20, blit=True, vmax=128):\n",
    "    fig = plt.figure()\n",
    "    plt.axis('off')\n",
    "    if mode == 'w':\n",
    "        out_slice_w = counter[0,-1,:,:]\n",
    "        im=plt.imshow(out_slice_w.cpu().flip(-1).T,cmap='Greys', vmin=0, vmax=vmax)\n",
    "        frames = counter.shape[-3]\n",
    "        def init():\n",
    "            im.set_data(out_slice_w.cpu().flip(-1).T)\n",
    "            return [im]\n",
    "        def animate(i):\n",
    "            out_slice_w = counter[0,-(i+1),:,:]\n",
    "            im.set_array(out_slice_w.cpu().flip(-1).T)\n",
    "            return [im]\n",
    "    elif mode == 'h':\n",
    "        out_slice_h = counter[0,:,-1,:]\n",
    "        im=plt.imshow(out_slice_h.cpu().flip(-1).T,cmap='Greys', vmin=0, vmax=vmax)\n",
    "        frames = counter.shape[-2]\n",
    "        def init():\n",
    "            im.set_data(out_slice_h.cpu().flip(-1).T)\n",
    "            return [im]\n",
    "        def animate(i):\n",
    "            out_slice_h = counter[0,:,-(i+1),:]\n",
    "            im.set_array(out_slice_h.cpu().flip(-1).T)\n",
    "            return [im]\n",
    "    else:\n",
    "        out_slice_d = counter[0,:,:,-1]\n",
    "        im=plt.imshow(out_slice_d.cpu().T,cmap='Greys', vmin=0, vmax=vmax)\n",
    "        frames = counter.shape[-1]\n",
    "        def init():\n",
    "            im.set_data(out_slice_d.cpu().T)\n",
    "            return [im]\n",
    "        def animate(i):\n",
    "            out_slice_d = counter[0,:,:,-(i+1)]\n",
    "            im.set_array(out_slice_d.cpu().T)\n",
    "            return [im]\n",
    "    #print(frames)\n",
    "    anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                               frames=frames, interval=20, blit=blit)\n",
    "    anim.save('slice_'+title+'_'+mode+'.gif', fps=fps)\n",
    "    plt.close()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a518d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# sanity-check for scene batch size > 1\n",
    "# document apply counting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ee71f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory already exists. Check if data is already downloaded, else delete example_data_download and try again.\n"
     ]
    }
   ],
   "source": [
    "# to show examples we need some example data - download it\n",
    "example_data_path = 'example_data_download'\n",
    "download_example_data(out_dir=example_data_path)\n",
    "local_files = os.path.join(example_data_path, 'grid_fusion_pytorch_example_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b78542",
   "metadata": {},
   "source": [
    "### Counting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f7e91f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded world dimensions lower limits. Shape: torch.Size([3])\n",
      "Loaded world dimensions upper limits. Shape: torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# define batch size and number of classes for this example\n",
    "B, C = 1, 39\n",
    "# define resolution of the map\n",
    "H, W, D = 140, 112, 100\n",
    "# initialize empty counter map - the two channels are hits + misses\n",
    "counter_map_in = torch.zeros([B, 2, H, W, D]).to(device)\n",
    "# intialize semantic map with uniform distribution\n",
    "semantic_map_in = torch.ones([B, C, H, W, D]).to(device)\n",
    "semantic_map_in *= 1 / (semantic_map_in.shape[1])\n",
    "# convert probabilities to log probs\n",
    "semantic_map_in = torch.log(semantic_map_in)\n",
    "\n",
    "# furthermore we need to specify the dimensions of this map in the real world\n",
    "range_min = torch.load(os.path.join(local_files,'example_world_limits_lower.pt')).cuda()\n",
    "print('Loaded world dimensions lower limits. Shape:', range_min.shape)\n",
    "range_max = torch.load(os.path.join(local_files,'example_world_limits_upper.pt')).cuda()\n",
    "print('Loaded world dimensions upper limits. Shape:', range_max.shape)\n",
    "# this can be specified per batch element, else assume each map in the batch is equally large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9339446f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded batch of camera poses. Shape: torch.Size([1, 22, 4, 4])\n",
      "Loaded batch of camera intrinsics. Shape: torch.Size([1, 22, 3, 3])\n",
      "Loaded batch of depth images. Shape: torch.Size([1, 22, 1, 480, 640])\n",
      "Loaded batch of semantic segmentation masks. Shape: torch.Size([1, 22, 1, 480, 640])\n",
      "Calculated ray origins. Shape: torch.Size([1, 22, 3])\n",
      "Calculated ray directions. Shape: torch.Size([1, 22, 307200, 3])\n",
      "Reshaped depth accordingly. Shape: torch.Size([1, 22, 307200])\n",
      "Converted semantic segmentation to eps-soft log probs. Shape: torch.Size([1, 22, 307200, 39])\n"
     ]
    }
   ],
   "source": [
    "cam_pose_batch = torch.load(os.path.join(local_files,'example_cam_pose_batch.pt')).cuda()\n",
    "print('Loaded batch of camera poses. Shape:', cam_pose_batch.shape)\n",
    "cam_k_batch = torch.load(os.path.join(local_files,'example_cam_k_batch.pt')).cuda()\n",
    "print('Loaded batch of camera intrinsics. Shape:', cam_k_batch.shape)\n",
    "depth_batch = torch.load(os.path.join(local_files,'example_depth_batch.pt')).cuda()\n",
    "print('Loaded batch of depth images. Shape:', depth_batch.shape)\n",
    "semseg_batch = torch.load(os.path.join(local_files,'example_semseg_batch.pt')).cuda()\n",
    "print('Loaded batch of semantic segmentation masks. Shape:', semseg_batch.shape)\n",
    "\n",
    "ray_origs, ray_dirs, _, _ = sample_rays(cam_pose_batch, cam_k_batch, depth=depth_batch, normalize=False)\n",
    "print('Calculated ray origins. Shape:', ray_origs.shape)\n",
    "print('Calculated ray directions. Shape:', ray_dirs.shape)\n",
    "depth_reshape = depth_batch.squeeze(2).flatten(-2,-1)\n",
    "print('Reshaped depth accordingly. Shape:', depth_reshape.shape)\n",
    "# reshape semseg to match rays\n",
    "semseg_reshape = semseg_batch.squeeze(2).flatten(-2,-1)\n",
    "# convert class labels to eps-soft log probs\n",
    "semseg_soft = torch.log(soften_semseg(semseg_reshape))\n",
    "# mask background values with nan\n",
    "semseg_soft[semseg_reshape == -1] = torch.full((semseg_soft.shape[-1],), fill_value=float('nan'), device=semseg_soft.device)\n",
    "print('Converted semantic segmentation to eps-soft log probs. Shape:', semseg_soft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23954e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: min_range should be 2D, but is not. Adding dummy batch dimension.\n",
      "Warning: max_range should be 2D, but is not. Adding dummy batch dimension.\n",
      "Computed output counter map. Shape: torch.Size([1, 2, 140, 112, 100]) \n",
      "\n",
      "Warning: min_range should be 2D, but is not. Adding dummy batch dimension.\n",
      "Warning: max_range should be 2D, but is not. Adding dummy batch dimension.\n",
      "Warning: semantic segmentation along rays detected, but no semantic map given! Applying counting model without Bayes filter.\n",
      "Computed output counter map. Shape: torch.Size([1, 2, 140, 112, 100]) \n",
      "\n",
      "Warning: min_range should be 2D, but is not. Adding dummy batch dimension.\n",
      "Warning: max_range should be 2D, but is not. Adding dummy batch dimension.\n",
      "Warning: semantic voxel grid detected, but no semantic segmentation provided for rays! Applying counting model without Bayes filter.\n",
      "Computed output counter map. Shape: torch.Size([1, 2, 140, 112, 100]) \n",
      "\n",
      "Warning: min_range should be 2D, but is not. Adding dummy batch dimension.\n",
      "Warning: max_range should be 2D, but is not. Adding dummy batch dimension.\n",
      "Computed output semantic map. Shape: torch.Size([1, 39, 140, 112, 100])\n",
      "Computed output counter map. Shape: torch.Size([1, 2, 140, 112, 100]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# no semantic annotation for rays, no semantic maps\n",
    "counter_map_out_A = apply_counting_model(counter_map_in, ray_origs, ray_dirs, \n",
    "                                                         depth_reshape, range_min, range_max, \n",
    "                                                         grid_semantic=None, \n",
    "                                                         ray_semseg=None, n_steps=4096, verbose=True)\n",
    "print('Computed output counter map. Shape:', counter_map_out_A.shape, '\\n')\n",
    "# semantic annotation for rays, but no semantic maps\n",
    "counter_map_out_B = apply_counting_model(counter_map_in, ray_origs, ray_dirs, \n",
    "                                                         depth_reshape, range_min, range_max, \n",
    "                                                         grid_semantic=None, \n",
    "                                                         ray_semseg=semseg_soft, n_steps=4096, verbose=True)\n",
    "print('Computed output counter map. Shape:', counter_map_out_B.shape, '\\n')\n",
    "# no semantic annotation for rays, but semantic maps\n",
    "counter_map_out_C = apply_counting_model(counter_map_in, ray_origs, ray_dirs, \n",
    "                                                         depth_reshape, range_min, range_max, \n",
    "                                                         grid_semantic=semantic_map_in, \n",
    "                                                         ray_semseg=None, n_steps=4096, verbose=True)\n",
    "print('Computed output counter map. Shape:', counter_map_out_C.shape, '\\n')\n",
    "# semantic annotation for rays and semantic maps\n",
    "counter_map_out_D, semantic_map_out_D = apply_counting_model(counter_map_in, ray_origs, ray_dirs, \n",
    "                                                         depth_reshape, range_min, range_max, \n",
    "                                                         grid_semantic=semantic_map_in, \n",
    "                                                         ray_semseg=semseg_soft, n_steps=4096, verbose=True)\n",
    "print('Computed output semantic map. Shape:', semantic_map_out_D.shape)\n",
    "print('Computed output counter map. Shape:', counter_map_out_D.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b7ea0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian fusion took 0.029628296189912362 seconds per call using 6758400 rays.\n",
      "This corresponds to about 228.1 million ray casts per second.\n"
     ]
    }
   ],
   "source": [
    "n_trials = 10\n",
    "n_calls = 10\n",
    "timed_func = timeit.Timer(lambda: apply_counting_model(counter_map_in, ray_origs, ray_dirs, depth_reshape, range_min, \n",
    "                                                       range_max, grid_semantic=semantic_map_in, ray_semseg=semseg_soft,\n",
    "                                                       n_steps=4096, verbose=False, assert_inputs=False))  \n",
    "exec_times = timed_func.repeat(repeat=n_trials, number=n_calls)\n",
    "#print(exec_times)\n",
    "num_rays = ray_dirs.shape[0]*ray_dirs.shape[1]*ray_dirs.shape[2]\n",
    "print(f\"Bayesian fusion took {np.mean(exec_times)/n_calls} seconds per call using {num_rays} rays.\")\n",
    "print(f\"This corresponds to about {np.round((num_rays/(np.mean(exec_times)/n_calls))/1000000,1)} million ray casts per second.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d254bf92",
   "metadata": {},
   "source": [
    "#### Variation - only count hits\n",
    "\n",
    "Just provide the same input but leave out the channel for misses in the counter maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64332bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define batch size and number of classes for this example\n",
    "B, C = 1, 39\n",
    "# define resolution of the map\n",
    "H, W, D = 140, 112, 100\n",
    "# initialize empty counter map - the one channel is for the hits counter\n",
    "counter_map_in = torch.zeros([B, 1, H, W, D]).to(device) # <--- !!! ONLY DIFFERENCE COMPARED TO ABOVE !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83456ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: min_range should be 2D, but is not. Adding dummy batch dimension.\n",
      "Warning: max_range should be 2D, but is not. Adding dummy batch dimension.\n",
      "Computed output occupancy map. Shape: torch.Size([1, 1, 140, 112, 100]) \n",
      "\n",
      "Warning: min_range should be 2D, but is not. Adding dummy batch dimension.\n",
      "Warning: max_range should be 2D, but is not. Adding dummy batch dimension.\n",
      "Warning: semantic segmentation along rays detected, but no semantic map given! Incrementing hit counter for foreground pixels without Bayes filter.\n",
      "Computed output occupancy map. Shape: torch.Size([1, 1, 140, 112, 100]) \n",
      "\n",
      "Warning: min_range should be 2D, but is not. Adding dummy batch dimension.\n",
      "Warning: max_range should be 2D, but is not. Adding dummy batch dimension.\n",
      "Warning: semantic voxel grid detected, but no semantic segmentation provided for rays! Counting hits without Bayes filter.\n",
      "Computed output occupancy map. Shape: torch.Size([1, 1, 140, 112, 100]) \n",
      "\n",
      "Warning: min_range should be 2D, but is not. Adding dummy batch dimension.\n",
      "Warning: max_range should be 2D, but is not. Adding dummy batch dimension.\n",
      "Computed output semantic map. Shape: torch.Size([1, 39, 140, 112, 100])\n",
      "Computed output occupancy map. Shape: torch.Size([1, 1, 140, 112, 100]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# no semantic annotation for rays, no semantic maps\n",
    "occ_map_out_A = apply_counting_model(counter_map_in, ray_origs, ray_dirs, \n",
    "                                                         depth_reshape, range_min, range_max, \n",
    "                                                         grid_semantic=None, \n",
    "                                                         ray_semseg=None, n_steps=4096, verbose=True)\n",
    "print('Computed output occupancy map. Shape:', occ_map_out_A.shape, '\\n')\n",
    "# semantic annotation for rays, but no semantic maps\n",
    "occ_map_out_B = apply_counting_model(counter_map_in, ray_origs, ray_dirs, \n",
    "                                                         depth_reshape, range_min, range_max, \n",
    "                                                         grid_semantic=None, \n",
    "                                                         ray_semseg=semseg_soft, n_steps=4096, verbose=True)\n",
    "print('Computed output occupancy map. Shape:',occ_map_out_B.shape, '\\n')\n",
    "# no semantic annotation for rays, but semantic maps\n",
    "occ_map_out_C = apply_counting_model(counter_map_in, ray_origs, ray_dirs, \n",
    "                                                         depth_reshape, range_min, range_max, \n",
    "                                                         grid_semantic=semantic_map_in, \n",
    "                                                         ray_semseg=None, n_steps=4096, verbose=True)\n",
    "print('Computed output occupancy map. Shape:',occ_map_out_C.shape, '\\n')\n",
    "# semantic annotation for rays and semantic maps\n",
    "occ_map_out_D, semantic_map_out_D = apply_counting_model(counter_map_in, ray_origs, ray_dirs, \n",
    "                                                         depth_reshape, range_min, range_max, \n",
    "                                                         grid_semantic=semantic_map_in, \n",
    "                                                         ray_semseg=semseg_soft, n_steps=4096, verbose=True)\n",
    "print('Computed output semantic map. Shape:',semantic_map_out_D.shape)\n",
    "print('Computed output occupancy map. Shape:',occ_map_out_D.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cc2e003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian fusion took 0.003427922219707398 seconds per call using 6758400 rays.\n",
      "This corresponds to about 2.0 billion ray endpoints per second.\n"
     ]
    }
   ],
   "source": [
    "n_trials = 10\n",
    "n_calls = 10\n",
    "timed_func = timeit.Timer(lambda: apply_counting_model(counter_map_in, ray_origs, ray_dirs, depth_reshape, range_min, \n",
    "                                                       range_max, grid_semantic=semantic_map_in, ray_semseg=semseg_soft,\n",
    "                                                       n_steps=4096, verbose=False, assert_inputs=False))  \n",
    "exec_times = timed_func.repeat(repeat=n_trials, number=n_calls)\n",
    "#print(exec_times)\n",
    "num_rays = ray_dirs.shape[0]*ray_dirs.shape[1]*ray_dirs.shape[2]\n",
    "print(f\"Bayesian fusion took {np.mean(exec_times)/n_calls} seconds per call using {num_rays} rays.\")\n",
    "print(f\"This corresponds to about {np.round((num_rays/(np.mean(exec_times)/n_calls))/1000000000,1)} billion ray endpoints per second.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331a1f90",
   "metadata": {},
   "source": [
    "### Directly fuse points clouds\n",
    "\n",
    "Fusing point clouds with or without semantic annotation is possible too. Misses aren't counted in this setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db021823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded world dimensions lower limits. Shape: torch.Size([3])\n",
      "Loaded world dimensions upper limits. Shape: torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# define batch size and number of classes for this example\n",
    "B, C = 1, 39\n",
    "# define resolution of the map\n",
    "H, W, D = 140, 112, 100\n",
    "# initialize empty occupancy map\n",
    "occ_map_in = torch.zeros([B, 1, H, W, D]).to(device)\n",
    "# intialize semantic map with uniform distribution\n",
    "semantic_map_in = torch.ones([B, C, H, W, D]).to(device)\n",
    "semantic_map_in *= 1 / (semantic_map_in.shape[1])\n",
    "# convert probabilities to log probs\n",
    "semantic_map_in = torch.log(semantic_map_in)\n",
    "\n",
    "# furthermore we need to specify the dimensions of this map in the real world\n",
    "range_min = torch.load(os.path.join(local_files,'example_world_limits_lower.pt')).cuda()\n",
    "print('Loaded world dimensions lower limits. Shape:', range_min.shape)\n",
    "range_max = torch.load(os.path.join(local_files,'example_world_limits_upper.pt')).cuda()\n",
    "print('Loaded world dimensions upper limits. Shape:', range_max.shape)\n",
    "# this can be specified per batch element, else assume each map in the batch is equally large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c863ae3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded point cloud locations. Shape: torch.Size([1, 669621, 3])\n",
      "Loaded point cloud class probabilities. Shape: torch.Size([1, 669621, 39])\n"
     ]
    }
   ],
   "source": [
    "# load an example point cloud\n",
    "# load the 3D locations\n",
    "pc_locs = torch.load(os.path.join(local_files,'example_pointcloud_locations.pt')).cuda()\n",
    "print('Loaded point cloud locations. Shape:', pc_locs.shape)\n",
    "# next load the class probabilites per location\n",
    "pc_probs = torch.load(os.path.join(local_files,'example_pointcloud_semantics.pt')).cuda()\n",
    "print('Loaded point cloud class probabilities. Shape:', pc_probs.shape)\n",
    "# convert these to log probs\n",
    "pc_probs = torch.log(pc_probs)\n",
    "\n",
    "# to mask points so they are not fused, set their locations to nan\n",
    "test_masking = False\n",
    "if test_masking:\n",
    "    # arbitrarily decide which points to mask\n",
    "    invalid_mask = torch.rand_like(pc_locs[:,:,0]) > 0.5\n",
    "    print('Masking',invalid_mask.sum(),'locations!')\n",
    "    invalid_mask = invalid_mask.unsqueeze(-1).expand(-1,-1,pc_locs.shape[-1])\n",
    "    pc_locs[invalid_mask] = float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f192890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: min_range should be 2D, but is not. Adding dummy batch dimension.\n",
      "Warning: max_range should be 2D, but is not. Adding dummy batch dimension.\n",
      "Computed output occupancy map. Shape: torch.Size([1, 1, 140, 112, 100]) \n",
      "\n",
      "Warning: min_range should be 2D, but is not. Adding dummy batch dimension.\n",
      "Warning: max_range should be 2D, but is not. Adding dummy batch dimension.\n",
      "Warning: semantic segmentation for point cloud detected, but no semantic map given! Incrementing hit counter without Bayes filter.\n",
      "Computed output occupancy map. Shape: torch.Size([1, 1, 140, 112, 100]) \n",
      "\n",
      "Warning: min_range should be 2D, but is not. Adding dummy batch dimension.\n",
      "Warning: max_range should be 2D, but is not. Adding dummy batch dimension.\n",
      "Warning: semantic voxel grid detected, but no semantic segmentation provided for point_clouds! Counting hits without Bayes filter.\n",
      "Computed output occupancy map. Shape: torch.Size([1, 1, 140, 112, 100]) \n",
      "\n",
      "Warning: min_range should be 2D, but is not. Adding dummy batch dimension.\n",
      "Warning: max_range should be 2D, but is not. Adding dummy batch dimension.\n",
      "Computed output semantic map. Shape: torch.Size([1, 39, 140, 112, 100])\n",
      "Computed output occupancy map. Shape: torch.Size([1, 1, 140, 112, 100]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# no point cloud semantics, no semantic maps\n",
    "occ_map_out_A  = apply_point_cloud_fusion(occ_map_in, pc_locs, range_min, range_max, \n",
    "                                                      grid_semantic=None, \n",
    "                                                      point_cloud_logprobs=None, verbose=True)\n",
    "print('Computed output occupancy map. Shape:', occ_map_out_A.shape, '\\n')\n",
    "# point cloud semantics, but no semantic map\n",
    "occ_map_out_B  = apply_point_cloud_fusion(occ_map_in, pc_locs, range_min, range_max, \n",
    "                                                      grid_semantic=None, \n",
    "                                                      point_cloud_logprobs=pc_probs, verbose=True)\n",
    "print('Computed output occupancy map. Shape:', occ_map_out_B.shape, '\\n')\n",
    "# semantic map, but no point cloud semantics\n",
    "occ_map_out_C = apply_point_cloud_fusion(occ_map_in, pc_locs, range_min, range_max, \n",
    "                                                      grid_semantic=semantic_map_in, \n",
    "                                                      point_cloud_logprobs=None, verbose=True)\n",
    "print('Computed output occupancy map. Shape:', occ_map_out_C.shape, '\\n')\n",
    "# point cloud semantics and semantic map\n",
    "occ_map_out_D, semantic_map_out_D = apply_point_cloud_fusion(occ_map_in, pc_locs, range_min, range_max, \n",
    "                                                      grid_semantic=semantic_map_in, \n",
    "                                                      point_cloud_logprobs=pc_probs, verbose=True)\n",
    "print('Computed output semantic map. Shape:', semantic_map_out_D.shape)\n",
    "print('Computed output occupancy map. Shape:', occ_map_out_D.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ca87f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian fusion took 0.0010991617297986523 seconds per call using 669621 points.\n",
      "This corresponds to about 609.2 million fused points per second.\n"
     ]
    }
   ],
   "source": [
    "n_trials = 10\n",
    "n_calls = 10\n",
    "timed_func = timeit.Timer(lambda: apply_point_cloud_fusion(occ_map_in, pc_locs, range_min, range_max, \n",
    "                                                           grid_semantic=semantic_map_in, point_cloud_logprobs=pc_probs, \n",
    "                                                           verbose=False, assert_inputs=False))  \n",
    "exec_times = timed_func.repeat(repeat=n_trials, number=n_calls)\n",
    "#print(exec_times)\n",
    "num_points = pc_locs.shape[0] * pc_locs.shape[1]\n",
    "print(f\"Bayesian fusion took {np.mean(exec_times)/n_calls} seconds per call using {num_points} points.\")\n",
    "print(f\"This corresponds to about {np.round((num_points/(np.mean(exec_times)/n_calls))/1000000,1)} million fused points per second.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
