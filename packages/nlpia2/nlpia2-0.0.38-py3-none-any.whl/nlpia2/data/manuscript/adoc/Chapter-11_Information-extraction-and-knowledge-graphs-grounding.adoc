= Natural Language Processing in Action, Second Edition
:chapter: 11
:part: 3
:sectnumoffset: 1
:secnums:
:imagesdir: .
:xrefstyle: short
:figure-caption: Figure {chapter}.
:listing-caption: Listing {chapter}.
:table-caption: Table {chapter}.
:leveloffset: 1
//:stem: latexmath
// :icons!:
:toc:
:source-highlighter: coderay
:bibliography-database: dl4nlp.bib
:bibliography-style: ieee
:index::[]

= Information extraction and knowledge graphs (grounding)

This chapter covers

* Extracting numerical and math expressions from text
* Building a knowledge graph from text
* Converting a dependency tree into a knowledge (fact)
* Querying a knowledge graph for fact checking
* Grounding generative language models with facts and beliefs
* Fact checking text with knowledge graphs

In the previous chapter (Chapter 10) you learned how to use transformers to generate smart _sounding_ words.
But language models on their own are just faking it by predicting the next word that will _sound_ reasonable to you.
Your AI can't reason about the real world until you give them access to facts and knowledge about the world.
In Chapter 2 you learned how to do exactly this, but you didn't know it then.
You were able to tag tokens with their part of speech and their logical role in the meaning of a sentence (dependency tree).
This old-fashioned token tagging algorithm is all you need to give your generative language models (AI) a _knowledge graph_ with actual facts about the real world (knowledge).
This is the _missing link_ in the NLP chain that you need to create true AI.

You will soon see how to use token tagging from Chapter 2 to extract facts from text.
And each fact you extract will create a new connection in a data structure called a _knowledge graph_ or _knowledge database_ (knowledge base) or a _semantic net_.
A knowledge graph will give your machines and algorithms a way to programmatically query this graph or database for facts about the world.
And your algorithms can then do fact-checking, not only on the text written by humans but also text generated by your NLP pipeline or AI.
Finally, your AI algorithms will be able to do introspection to let you know if what they are telling you might actually have some semblance of truth to it.
Your AI can use knowledge graphs to fill the _common sense knowledge_ gap in large language models and perhaps live up to a little bit of the hype around LLMs and AI.

You will need to store the facts you've extracted in a _knowledge base_ so you can make decisions based on those facts.
A knowledge base is a database that stores knowledge as relationships between concepts.
Though you can use a relational database to store the relations and concepts, sometimes it is more efficient to use data structure called a _graph_.
This allows you to ask questions about the relationships between things using a query language such as GraphQL or Cypher or even SQL.
And you can use a knowledge graph to programmatically generate text that makes sense because it is grounded in facts in your database.
You can even infer new facts or _logical inferences_ about the world that aren't yet included in your knowledge base.

You may remember hearing about "inference" when people talk about forward propagation or prediction using deep learning models.
A deep learning language model uses statistics to estimate or guess the next word in the text that you prompt it with.
And deep learning researchers hope that one day, neural networks will be able to match the natural human ability to logically infer things and reason about the world.
But this isn't possible, because words don't contain all the knowledge about the world that a machine would need to process to make factually correct inferences.
So you're going to use a tried and true logical inference approach called "symbolic reasoning."

If you're familiar with the concept of a compiler then you may want to think of the dependency tree as a parse tree or abstract syntax tree (AST).
An AST defines the logic of a machine language expression or program.
You're going to use the natural language dependency tree to extract the logical relations within natural language text.
And this logic will help you _ground_ the statistical deep learning models so they can do more than merely make statistical "guesses" about the world as they did in previous chapters.

== Grounding

Once you have a knowledge graph, your chatbots and AI agents will have a way to correctly reason about the world in an explainable way.
And if you can extract facts from the text your deep learning model generates, you can check to see if that text agrees with the knowledge you've collected in your knowledge graph.
This is called _grounding_  when you maintain a knowledge graph and then use it to double-check the facts and reasoning in the generated text.
When you ground your language model you attach it to some ground truth facts about the world.

Grounding can also benefit your NLP pipeline in other ways.
Using a knowledge graph for the reasoning part of your algorithm can free up your language model to do what it does best -- generate plausible, grammatical text.
So you can fine tune your language model to have the tone that you want, without trying to build a chameleon that pretends to understand and reason about the world.
And your knowledge graph can be designed to contain just the facts about a world that you want your AI to understand -- whether it is facts about the real world that you have in mind or some fictional world that you are creating.
By separating the reasoning from the language you can create an NLP pipeline that both sounds correct and _is_ correct.

There are a few other terms that are often used when referring to this grounding process.
If you have taken a philosophy or AI course you may remember the concepts of _symbolic reasoning_ as opposed to the probabilistic reasoning of machine learning models.
_First order logic_ is one system for symbolic reasoning.footnote:[Wikipedia article "Symbolic AI" (https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence)]
This was the preferred approach to building expert systems and theorem provers before the data and processing power was available for machine learning and deep learning.
It's also called Good Old Fashioned AI or GOFAI, pronounced "Go Fie".
GOFAI is back in fashion as researchers attempt to build generally intelligent systems that we can rely on to make important decisions.

Another advantage of grounding your NLP pipeline is that you can use the facts in your knowledge base to _explain_ its reasoning.
If you ask an ungrounded LLM to explain why it said something unreasonable, it will just keep digging a hole for itself (and you) by making up more and more nonsense reasons.
You saw this in the previous chapters when LLMs confidently hallucinated (fabricated) nonexistent but plausible references and fictional people to explain where they got their nonsense from.
The key to creating AI you can trust is to put a floor of reason underneath it using a knowledge graph.
The first, and perhaps most important algorithm in this grounding process is _knowledge extraction_.

So, let's start the journey of knowledge extraction and grounding! 
But we have to cover an important step in processing your documents, to generate a proper input to your knowledge extraction pipeline.
We need to break our text into smaller units. 

== First things first: segmenting your text into sentences 
// SUM: How and why to segment text into sentences.

Before you can dive into extracting your knowledge from raw text, you need to break it down into chunks that your pipeline can work on. 
Document "chunking" is useful for creating semi-structured data about documents that can make it easier to search, filter, and sort documents for information retrieval.
And for information extraction, if you're extracting relations to build a knowledge base such as NELL or Freebase (more about them in a bit), you need to break it into parts that are likely to contain a fact or two.
When you divide natural language text into meaningful pieces, it's called _segmentation_.
The resulting segments can be phrases, sentences, quotes, paragraphs, or even entire sections of a long document.

Sentences are the most common chunk for most information extraction problems.
Sentences are usually punctuated with one of a few symbols (".", "?", "!", or a new line).
And grammatically correct English language sentences must contain a subject (noun) and a verb, which means they'll usually have at least one fact worth extracting.
Sentences are often self-contained packets of meaning that don't rely too much on preceding text to convey most of their information.

In addition to facilitating information extraction, you can flag some of those statements and sentences as being part of a dialog or being suitable for replies in a dialog.
Using a sentence segmenter allows you to train your chatbot on longer texts, such as books.
Choosing those books appropriately gives your chatbot a more literary, intelligent style than if you trained it purely on Twitter streams or IRC chats.
And these books give your chatbot access to a much broader set of training documents to build its common sense knowledge about the world.

Sentence segmentation is the first step in your information extraction pipeline.
It helps isolate facts from each other.
Most sentences express a single coherent thought.
And many of those thoughts are about real things in the real world.
And, most importantly, all the natural languages have sentences or logically cohesive sections of text of some sort.
And all languages have a widely shared process for generating them (a set of grammar "rules" or habits).

But segmenting text and identifying sentence boundaries is a bit trickier than you might think.
In English, for example, no single punctuation mark or sequence of characters always marks the end of a sentence.

=== Why won't `split('.!?')` work?
// SUM: Why you need a sentence segmenter instead of naively splitting on EOS punctuation.

Even a human reader might have trouble finding an appropriate sentence boundary within each of the following quotes.
And if they did find multiple sentences from each, they would be wrong for four out of five of these difficult examples:

_I went to G.T.You?_

_She yelled "It's right here!" but I kept looking for a sentence boundary anyway._

_I stared dumbfounded on as things like "How did I get here?", "Where am I?", "Am I alive?" flittered across the screen._

_The author wrote "'I don't think it's conscious.' Turing said."_

Even a human reader would have trouble finding an appropriate sentence boundary within each of these quotes and nested quotes and stories within a story.

More sentence segmentation "edge cases" such as this are available at tm-town.com. footnote:[See the web page titled "Natural Language Processing : TM-Town" (https://www.tm-town.com/natural-language-processing#golden_rules).] 

Technical text is particularly difficult to segment into sentences because engineers, scientists, and mathematicians tend to use periods and exclamation points to signify a lot of things besides the end of a sentence.
When we tried to find the sentence boundaries in this book, we had to manually correct several of the extracted sentences.

If only we wrote English like telegrams, with a "STOP" or unique punctuation mark at the end of each sentence.
But since we don't, you'll need some more sophisticated NLP than just `split('.!?')`.
Hopefully, you're already imagining a solution in your head.
If so, it's probably based on one of the two approaches to NLP you've used throughout this book:

* Manually programmed algorithms (regular expressions and pattern-matching)
* Statistical models (data-based models or machine learning)

We use the sentence segmentation problem to revisit these two approaches by showing you how to use regular expressions as well as perceptrons to find sentence boundaries.
And you'll use the text of this book as a training and test set to show you some of the challenges.
Fortunately, you haven't inserted any newlines within sentences, to manually "wrap" text like in newspaper column layouts.
Otherwise, the problem would be even more difficult.
In fact, much of the source text for this book, in ASCIIdoc format, has been written with "old-school" sentence separators (two spaces after the end of every sentence), or with each sentence on a separate line.
This was so we could use this book as a training and test set for your segmenters.

=== Sentence segmentation with regular expressions
// SUM: Test some regular expressions on the TM-Town (translation memory company) dataset.

Regular expressions are just a shorthand way of expressing the tree of "`if...then`" rules (regular grammar rules) for finding character patterns in strings of characters.
As we mentioned in Chapters 1 and 2, regular expressions (regular grammars) are a particularly succinct way to specify the structure of a finite state machine.
Our regex or FSM has only one purpose: identifying sentence boundaries.

If you do a web search for sentence segmenters,footnote:[See the web page titled "Python sentence segment at DuckDuckGo" (https://duckduckgo.com/?q=Python+sentence+segment&t=canonical&ia=qa).] you're likely to be pointed to various regular expressions intended to capture the most common sentence boundaries.
Here are some of them, combined and enhanced to give you a fast, general-purpose sentence segmenter.

The following regex would work with a few "normal" sentences.

[source,python]
>>> re.split(r'[!.?]+[ $]', "Hello World.... Are you there?!?! I'm going to Mars!")
['Hello World', 'Are you there', "I'm going to Mars!"]

Unfortunately, this `re.split` approach gobbles up the sentence-terminating token, and only retains it if it is the last character in a document or string.
But it does do a good job of ignoring the trickery of periods within doubly-nested quotes:

[source,python]
>>> re.split(r'[!.?] ', "The author wrote \"'I don't think it's conscious.' Turing said.\"")
['The author wrote "\'I don\'t think it\'s conscious.\' Turing said."']

It also ignores periods in quotes that terminate an actual sentence.
This can be a good thing or a bad thing, depending on your information extraction steps that follow your sentence segmenter.

[source,python]
>>> re.split(r'[!.?] ', "The author wrote \"'I don't think it's conscious.' Turing said.\" But I stopped reading.")
['The author wrote "\'I don\'t think it\'s conscious.\' Turing said." But I stopped reading."']

What about abbreviated text, such as SMS messages and tweets?
Sometimes hurried humans squish sentences together, leaving no space surrounding periods.
Alone, the following regex could only deal with periods in SMS messages that have letters on either side, and it would safely skip over numerical values:

[source,python]
>>> re.split(r'(?<!\d)\.|\.(?!\d)', "I went to GT.You?")
['I went to GT', 'You?']

Even combining these two regexes into a monstrosity like `r'((?<!\d)\.|\.(?!\d))|([!.?]+)[ $]+'` isn't enough to get all the sentences right if we try to parse this chapter. 
You'd have to add a lot more "look-ahead" and "look-back" to improve the accuracy of a regex sentence segmenter.
And if looking for all the edge cases and designing rules around them feel cumbersome, that's because it is. 

=== Segment sentences with spaCy

A better approach for sentence segmentation is to use a machine learning algorithm (often a single-layer neural net or logistic regression) trained on a labeled set of sentences.
Several packages contain such a model you can use to improve your sentence segmenter, such as spaCy footnote:[See the web page titled "Facts & Figures : spaCy Usage Documentation" (https://spacy.io/usage/facts-figures).] and Punkt package of NLTK footnote:[See the web page titled "nltk.tokenize package — NLTK 3.3 documentation" (http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.punkt).]

You can use the spaCy sentence segmenter (built into the parser) for most of your mission-critical applications.
spaCy has few dependencies and compares well with the others on accuracy and speed.
And actually, it does the sentence segmentation automatically as part of its default pipeline. 

Here's the simplest way to segment your text into sentences using spaCy: 

[source, python]
----
>>> import spacy

>>> nlpsm = spacy.load("en_core_web_sm")
>>> doc = nlpsm("This is a sentence. This is another sentence.")
>>> for sent in doc.sents:
...   print(sent.text)
----

spaCy's default parser is based on dependency extraction that we'll cover later in this chapter. 
That means it actually looks at the structure of the sentences when trying to split the given text. 
This is why it can be a bit slower for large texts.
If you want to use spaCy's statistical parser, embedded in a pipeline component called `senter`, which is about 5x faster, you need to disable the default parser when making the import. 

[source, python]
----
>>> import spacy

>>> nlp_statistical = spacy.load("en_core_web_sm", exclude=["parser"])
>>> nlp_statistical.enable_pipe("senter")
>>> doc = nlp_statistical("This is a sentence. This is another sentence.")
>>> for sent in doc.sents:
...    print(sent.text)
----


Now that you have your text segmented, you're ready to start the process of knowledge extraction and putting it into a knowledge graph. 

== A knowledge extraction pipeline

Finally, we can start extracting concepts and relations from natural language text.
You can construct a graph from scratch using your own common sense knowledge or the domain knowledge that you want your AI to know about.
But if you can extract knowledge from text you can build much larger knowledge graphs much quicker.
Plus, you will need this algorithm to double-check any text generated by your language models.

Knowledge extraction requires four main steps:

.Four stages of knowledge extraction
image::../images/ch11/knowledge-graph-extraction.drawio.png["Knowledge extraction pipeline showing the entities -- 'Gebru' and the title of her paper about the dangers of large language models. extracted from the passage: Gebru had ... She and five others coauthored a research paper, "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?'", width=85%, link="../images/ch11/knowledge-graph-extraction.drawio.png"]

Fortunately, the spaCy language models include the building blocks for knowledge extraction: named entity recognition, coreference resolution, and relation extraction.
You only need to know how to combine the results of each of these steps to connect the pieces together.
Let's look at each stage separately by looking at an article about Timnit Gebru, a thought leader in AI ethics. 
We'll continue using the spaCy nlp model we initialized in the previous section. 

Let's start by downloading the Wikipedia article about Timnit Gebru.

[source,python]
----
>>> from nlpia2 import wikipedia as wiki
>>> page = wiki.page('Timnit Gebru')
>>> text = page.content
>>> text[:66]
'Timnit Gebru (Amharic: ትምኒት ገብሩ; born 13 May 1983) is an Ethiopian'
----

Have you heard of Timnit Gebru before?
She's famous among people in your area of interest and she's written several influential papers:

[source,python]
----
>>> i1 = text.index('Stochastic')
>>> text[i1:i1+51]
'Stochastic Parrots: Can Language Models Be Too Big?'
----

That's a pretty interesting research paper title.
It certainly seems like something her bosses would be interested in publishing.
Let's start looking at the text from the perspective of your information extraction pipeline. 

== Named Entity Recognition

The first step in extracting knowledge about some __thing__ is to find the _things_ that you want to know about.
The most important things in natural language text are the names of people, places, and things.
In linguistics named things are called "named entities."
SpaCy's knowledge extraction pipeline assumes that you want to work with named entities.
The 'ents' attribute on a doc object contains a list of all those named entities.

[source,python]
----
>>> doc = nlpsm(text)
>>> doc.ents[:6]  # <1>
(Timnit Gebru, Amharic, 13, May 1983, Ethiopian, Black in AI)
----
<1> Get the first 6 named entities in the Wikipedia article.

The challenge of named entity recognition is closely related to a more basic problem - part-of-speech (POS) tagging. 
To recognize named entities in the sentence, you need to know which part of speech each word belongs to. 
Your named entity will often be a proper noun - a noun that refers to a particular person, place or thing in the real world.
And the part of speech tag for relations is a _verb_.
The verb tokens will be used to connect the named entities to each other as the edges in your knowledge graph.

Part-of-speech tagging is also crucial to the next stage in our pipeline - dependency parsing. 
To determine the relationships between different entities inside the sentence, we will need to recognize the verbs in our sentence.

Luckily, spaCy already did that for you the moment you fed the text to it. 

[source, python]
----
>>> first_sentence = list(doc.sents)[0]
>>> ' '.join(['{}_{}'.format(tok, tok.pos_) for tok in first_sentence])
 'Timnit_PROPN Gebru_PROPN (_PUNCT Amharic_PROPN :_PUNCT ትምኒት_NOUN ገብሩ_ADV ;_PUNCT Tigrinya_PROPN :_PUNCT  _SPACE ትምኒት_NOUN ገብሩ_PROPN )_PUNCT born_VERB 13_NUM May_PROPN 1983_NUM is_AUX an_DET Eritrean_ADJ Ethiopian_PROPN -_PUNCT born_VERB computer_NOUN scientist_NOUN who_PRON works_VERB on_ADP algorithmic_ADJ bias_NOUN and_CCONJ data_NOUN mining_NOUN ._PUNCT'
----

Can you make sense of this?
PUNCT, NOUN and VERB are pretty self-explanatory; and you can guess that PROPN stands for Proper Noun. 
But what about CCONJ?
Luckily, you can let spaCy explain it to you.

[source,python]
----
>>> spacy.explain('CCONJ')
'coordinating conjunction'
----

Another tool spaCy gives you is the `tag_` property of each token. 
While the `pos_` tag gives you the part-of-speech or a particular token, the `tag_` gives you more information and details about the token. 
Let's see an example: 

[source,python]
----
>>> ' '.join(['{}_{}'.format(tok, tok.tag_) for tok in first_sentence])
'Timnit_NNP Gebru_NNP (_-LRB- Amharic_NNP :_: ትምኒት_NN ገብሩ_RB ;_: Tigrinya_NNP :_:  __SP ትምኒት_NN ገብሩ_NNP )_-RRB- born_VBN 13_CD May_NNP 1983_CD is_VBZ an_DT Eritrean_JJ Ethiopian_NNP -_HYPH born_VBN computer_NN scientist_NN who_WP works_VBZ on_IN algorithmic_JJ bias_NN and_CC data_NNS mining_NN ._.'
----

Wow, this looks much more cryptic. 
You can vaguely intuit the connection between PROPN and NNP, but what is VBZ?

[source,python]
----
>>> spacy.explain('VBZ')
'verb, 3rd person singular present'
----
That's for sure much more information, albeit served in a more cryptical form. 

Let's bring all the information about your tokens together in one table. 

// FIXME: nlpia2.spacy_pipes
[source,python]
----
>>> import pandas as pd
>>> from collections import OrderedDict
>>>
>>> def token_dict(token):
...    return OrderedDict( TOK=token.text,
...        POS=token.pos_, TAG=token.tag_, 
...        ENT_TYPE=token.ent_type_, DEP=token.dep_,)
>>>
>>> def doc_df(doc):
...    return pd.DataFrame([token_dict(tok) for tok in doc])
>>>
>>> doc_df(doc)
            TOK    POS    TAG ENT_TYPE       DEP
0        Timnit  PROPN    NNP           compound
1         Gebru  PROPN    NNP              nsubj
2             (  PUNCT  -LRB-              punct
3       Amharic  PROPN    NNP              appos
4             :  PUNCT      :              punct
         ...    ...    ...      ...       ...
3277     Timnit  PROPN    NNP      ORG  compound
3278      Gebru  PROPN    NNP      ORG      pobj
3279         at    ADP     IN               prep
3280  Wikimedia  PROPN    NNP      FAC  compound
3281    Commons  PROPN    NNP      FAC      pobj
----

You can guess what the columns 'TOK', 'POS' and 'TAG' contain. 
The fourth column 'ENT_TYPE', gives us information about the type of our named entity.
You can see that the small spaCy model didn't do great - it missed Timnit Gebru as a named entity in the first words of the text, and when it did recognize it later, it thought that its entity type is an organization. 

Let's see if a larger model would do better:

//TODO complete the example
[source,python]
----
>>> from nlpia2.spacy_pipes import nlp_df
>>> nlp = load('en_core_web_lg')

----

This looks better! 
Timnit Gebru is classified as a PERSON, and Wikimedia is properly tagged as ORG. 

So this will usually be the first algorithm in your pipeline, the spaCy language model that tokenizes your text and tags each token with the linguistic features you need for knowledge extraction.

Once you understand how a named entity recognizer works, you can expand the kinds of nouns and noun phrases you recognize and include in your knowledge graph.
This can help generalize your knowledge graph and help you create a more generally intelligent NLP pipeline.

But we haven't yet touched the last column of our dataframe, DEP.
Time for step 2 of your pipeline - dependency parsing! 

== Dependency Parsing

And the spaCy package knows how to recognize the relationships (dependencies) between words and phrases.
Once you have a _dependency tree_ of the hierarchy of grammatical relationships between words you have a way to process the logical meaning of a sentence.


[source,python]
----
>>> tags = []
>>> for tok in doc:
...     tags.append(dict(token=tok.text, pos=tok.pos_, dep=tok.dep_))
...     tags[-1].update({f'child{i}': c.text for (i, c) in enumerate(tok.children)})
...
>>> df = pd.DataFrame(tags).set_index('token').fillna('')
>>> df.head()
              pos       dep child0 child1 child2 child3 child4 child5 child6 child7
token
Gebru       PROPN     nsubj
had           AUX       aux
determined   VERB      ROOT  Gebru    had    was      .
that        SCONJ      mark
publishing   VERB  compound
----

[source,python]
----
>>> doc.ents
(Gebru,
 five,
 the Dangers of Stochastic Parrots: Can Language Models Be Too Big)
>>> doc
Gebru had determined that publishing research papers ...
"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?"
----

[source,python]
----
>>> from nlpia2.spacy_language_model import nlp
>>> import pandas as pd
>>> text = "Gebru was unethically fired from her Ethical AI team."
>>> doc = nlp(text)
>>> tags = []
... for tok in doc:
...     tags.append(dict(text=tok.text, pos=tok.pos_, dep=tok.dep_))
...     tags[-1].update({f'child_{i}': c.text for (i, c) in enumerate(tok.children)})
...
>>> df =
>>> df = pd.DataFrame(tags)
>>> df
     text    pos       dep child_0 child_1 child_2
0     Dr.  PROPN  compound     NaN     NaN     NaN
1    Kate  PROPN  compound     NaN     NaN     NaN
2   Moore  PROPN     nsubj     Dr.    Kate     NaN
3      is    AUX      ROOT   Moore  expert       .
4      an    DET       det     NaN     NaN     NaN
5      AI  PROPN  compound     NaN     NaN     NaN
6  Ethics  PROPN  compound      AI     NaN     NaN
7  expert   NOUN      attr      an  Ethics     NaN
8       .  PUNCT     punct     NaN     NaN     NaN
----

You'd like your machine to extract pieces of information and facts from text so it can know a little bit about what a user is saying.
For example, imagine a user says "Remind me to read aiindex.org on Monday."
You'd like that statement to trigger a calendar entry or alarm for the next Monday after the current date.
Easier said than done.



To trigger correct actions with natural language you need something like an NLU pipeline or parser that is a little less fuzzy than a transformer or large language model.
You need to know that "me" represents a particular kind of _named entity_: a person.
Named entities are natural language terms or n-grams that refer to a particular thing in the real world, such as a person, place or thing.
Sound familiar?
In English grammar, the _part of speech_ (POS) for a person, place or thing is "noun".
So you'll see that the POS tag that spaCy associates with the tokens for a named entity is "NOUN".


And the chatbot should know that it can expand or _resolve_ that word by replacing it with that person's username or other identifying information.
You'd also need your chatbot to recognize that "aiindex.org" is an abbreviated URL, which is a named entity - a name of a specific instance of something, like a website or company.
And you need to know that a normalized spelling of this particular kind of named entity might be "http://aiindex.org", "https://aiindex.org", or maybe even "https://www.aiindex.org".
Likewise, you need your chatbot to recognize that Monday is one of the days of the week (another kind of named entity called an "event") and be able to find it on the calendar.

For the chatbot to respond properly to that simple request, you also need it to extract the relation between the named entity "me" and the command "remind."
You'd even need to recognize the implied subject of the sentence, "you", referring to the chatbot, another person named entity.
And you need to teach the chatbot that reminders happen in the future, so it should find the soonest upcoming Monday to create the reminder.

A typical sentence may contain several named entities of various types, such as geographic entities, organizations, people, political entities, times (including dates), artifacts, events, and natural phenomena.
And a sentence can contain several relations, too -- facts about the relationship between the named entities in the sentence.

=== A knowledge base

Besides just extracting information from the text of a user statement, you can also use information extraction to help your chatbot train itself!
If you have your chatbot run information extraction on a large corpus, such as Wikipedia, that corpus will produce facts about the world that can inform future chatbot behaviors and replies.
Some chatbots record all the information they extract (from offline reading-assignment "homework") in a knowledge base.
That knowledge base can later be queried to help your chatbot make informed decisions or inferences about the world.

Chatbots can also store knowledge about the current user "session" or conversation.
Knowledge that is relevant only to the current conversation is called "context."
This contextual knowledge can be stored in the same global knowledge base that supports the chatbot, or it can be stored in a separate knowledge base.
Commercial chatbot APIs, such as IBM's Watson or Amazon's Lex, typically store context separate from the global knowledge base of facts that it uses to support conversations with all the other users.

Context can include facts about the user, the chatroom or channel, or the weather and news for that moment in time.
Context can even include the changing state of the chatbot itself, based on the conversation.
A smart chatbot keeps track of self-knowledge or subjective knowledge with the same gusto that it manages objective knowledge.
An example of self-knowledge is the history of all the things the chatbot has already said to someone, such as the questions it has already asked of the user. That way it won't repeat itself.

So that's the goal for this chapter, teaching your bot to understand what it reads.
And you'll put that understanding into a flexible data structure designed to store knowledge.
Then your bot can use that knowledge to make decisions and say smart stuff about the world.

In addition to the simple task of recognizing numbers and dates in text, you'd like your bot to be able to extract more general information about the world.
And you'd like it to do this on its own, rather than having you "program" everything you know about the world into it.
For example, you'd like it to be able to learn from natural language documents such as this sentence from Wikipedia:

_In 1983, Stanislav Petrov, a lieutenant colonel of the Soviet Air Defense Forces, saved the world from nuclear war._

If you were to take notes in a history class after reading or hearing something like that, you'd probably paraphrase things and create connections in your brain between concepts or words.
You might reduce it to a piece of knowledge, that thing that you "got out of it."
You'd like your bot to do the same thing.
You'd like it to "take note" of whatever it learns, such as the fact or knowledge that Stanislav Petrov was a lieutenant colonel.
This could be stored in a data structure something like this:

[source,python]
----
('Stanislav Petrov', 'is-a', 'lieutenant colonel')
----

This is an example of two named entity nodes ('Stanislav Petrov' and 'lieutenant colonel') and a relation or connection ('is a') between them in a knowledge graph or knowledge base.
When a relationship like this is stored in a form that complies with the RDF standard (relation description format) for knowledge graphs, it's referred to as an RDF triplet.
Historically these RDF triplets were stored in XML files, but they can be stored in any file format or database that can hold a graph of triplets in the form of `(subject, relation, object)`.

A collection of these triplets is a knowledge graph.
This is also sometimes called an ontology by linguists because it is storing structured information about words.
But when the graph is intended to represent facts about the world rather than merely words, it is referred to as a knowledge graph or knowledge base.
Figure 11.1 is a graphic representation of the knowledge graph you'd like to extract from a sentence like that.

.Stanislav knowledge graph
image::../images/ch11/Stanislav-Knowledge-Graph.png[Stanislav Knowledge Graph showing 'is-a' and 'is-famous-for' relations extracted, width=80%, link="../images/ch11/Stanislav-Knowledge-Graph.png"]

The red edge and node in this knowledge graph represent a fact that could not be directly extracted from the statement about Stanislav.
But this fact that "lieutenant colonel" is a military rank could be inferred from the fact that the title of a person who is a member of a military organization is a military rank.
This logical operation of deriving facts from a knowledge graph is called knowledge graph _inference_.
It can also be called querying a knowledge base, analogous to querying a relational database.

For this particular inference or query about Stanislov's military ranks, your knowledge graph would have to already contain facts about militaries and military ranks.
It might even help if the knowledge base had facts about the titles of people and how people relate to occupations (jobs).
Perhaps you can see now how a base of knowledge helps a machine understand more about a statement than it could without that knowledge.
Without this base of knowledge, many of the facts in a simple statement like this will be "over the head" of your chatbot.
You might even say that questions about occupational rank would be "above the pay grade" of a bot that only knew how to classify documents according to randomly allocated topics. (See Chapter 4 if you've forgotten about how random topic allocation can be.)

It may not be obvious how big a deal this is, but it is a _BIG_ deal.
If you've ever interacted with a chatbot that doesn't understand "which way is up", literally, you'd understand.
One of the most daunting challenges in AI research is the challenge of compiling and efficiently querying a knowledge graph of common sense knowledge.
We take commonsense knowledge for granted in our everyday conversations.

Humans start acquiring much of their common sense knowledge even before they acquire language skill.
We don't spend our childhood writing about how a day begins with light and sleep usually follows sunset.
And we don't edit Wikipedia articles about how an empty belly should only be filled with food rather than dirt or rocks.
This makes it hard for machines to find a corpus of common sense knowledge to read and learn from.
No common-sense knowledge Wikipedia articles exist for your bot to do information extraction on.
And some of that knowledge is instinct, hard-coded into our DNA.footnote:[There are hard-coded common-sense knowledge bases out there for you to build on. Google Scholar is your friend in this knowledge graph search.]

All kinds of factual relationships exist between things and people, such as "kind-of", "is-used-for", "has-a", "is-famous-for", "was-born", and "has-profession."
NELL, the Carnegie Mellon Never Ending Language Learning bot is focused almost entirely on the task of extracting information about the `'kind-of'` relationship.

Most knowledge bases normalize the strings that define these relationships, so that "kind of" and "type of" would be assigned a normalized string or ID to represent that particular relation.
And some knowledge bases also normalize the nouns representing the objects in a knowledge base.
So the bigram "Stanislav Petrov" might be assigned a particular ID.
Synonyms for "Stanislav Petrov", like "S. Petrov" and "Lt Col Petrov", would also be assigned to that same ID, if the NLP pipeline suspected they referred to the same person.

A knowledge base can be used to build a practical type of chatbot called a _question-answering system_ (QA system).
Many customer service chatbots, including university TA bots, rely on knowledge bases to generate their replies.footnote:[2016, AI Teaching Assistant at GaTech: http://www.news.gatech.edu/2016/05/09/artificial-intelligence-course-creates-ai-teaching-assistant]
Question-answering systems are great for helping humans find factual information, which frees up human brains to do the things they're better at, such as attempting to generalize from those facts.
Humans are bad at remembering facts accurately but good at finding connections and patterns between those facts, something machines have yet to master.
We talk more about question-answering chatbots in the next chapter.

=== A large knowledge graph

If you've ever heard of a "mind map" they can give a pretty good mental model of what knowledge graphs are: connections between concepts in your mind.
To give you a more concrete mental model of the concept of knowledge graphs you probably want to explore the oldest public knowledge graph on the web: NELL.
This is one small portion of the latest NELL knowledge graph, the first 150 entities out of about three million:

.First few entities in the NELL knowledge graph
image::../images/ch11/kg_150_biotech_company.graphviz.png[A network diagram of the first 150 entities in the NELL knowledge graph. The dataset starts with a biotech company named , width=85%, link="../images/ch11/kg_150_biotech_company.graphviz.png"]


The NLPiA2 Python package has several utilities for making the NELL knowledge graph a bit easier to wrap your head around.
Later in the chapter, you'll see the details about how these work so you can prettify whatever knowledge graph you are working with.

// FIXME: graph_plots.__main__ goes here
// 0. describe NELLs columns
// 1. optionally show what NELL df looks like
// 2. reduce columns to only first 3 or 4
// 3. simplify_*()

[source,python]
----
>>> import pandas as pd
>>> pd.options.display.max_colwidth = 20
>>> from nlpia2.nell import read_nell_tsv, simplify_names
>>> df = read_nell_tsv(nrows=1000)
>>> df[df.columns[:4]].head()
                entity            relation                value iteration
0  concept:biotechc...     generalizations  concept:biotechc...      1103
1  concept:company:...  concept:companyceo  concept:ceo:lesl...      1115
2  concept:company:...     generalizations  concept:retailstore      1097
3  concept:company:...     generalizations      concept:company      1104
4  concept:biotechc...     generalizations  concept:biotechc...      1095
----

The entity names are very precise and well-defined within a hierarchy, like paths for a file or name-spaced variable names in Python.
All of the entity and value names start with "concept:" so you can strip that from your name strings to make the data a bit easier to work with.
To simplify things further, you can eliminate the namespacing hierarchy and focus on just the last name in the hierarchy.

[source,python]
----
>>> pd.options.display.max_colwidth = 40
>>> df['entity'].str.split(':').str[1:].str.join(':')
0        biotechcompany:aspect_medical_systems
1                       company:limited_brands
2                       company:limited_brands
3                       company:limited_brands
4                biotechcompany:calavo_growers
                        ...
>>> df['entity'].str.split(':').str[-1]
0        aspect_medical_systems
1                limited_brands
2                limited_brands
3                limited_brands
4                calavo_growers
                 ...
----

The `nlpia2.nell` module simplifies the names of things even further.
This makes it easier to navigate the knowledge graph in a network diagram.
Otherwise, the names of entities can fill up the width of the plot and crowd each other out.

[source,python]
----
>>> df = simplify_names(df)  # <1>
>>> df[df.columns[[0, 1, 2, 4]]].head()
                   entity relation           value   prob
0  aspect_medical_systems     is_a  biotechcompany  0.924
1          limited_brands      ceo   leslie_wexner  0.938
2          limited_brands     is_a     retailstore  0.990
3          limited_brands     is_a         company  1.000
4          calavo_growers     is_a  biotechcompany  0.983
----
<1> Uses the `str.replace()` method to shorten the names of the entities, relations, and values

NELL scrapes text from Twitter, so the spelling and wording of facts can be quite varied.
In NELL the names of entities, relations and objects have been normalized by lowercasing them and removing all punctuation like apostrophes and hyphens.
Only proper names are allowed to retain their spaces, to help distinguish between names that contain spaces and those that are smashed together.
However, in NELL, just as in Word2vec token identifiers, proper names are joined with underscore ("\_") characters.

Entity and relation names are like variable names in Python.
You want to be able to query them like field names in a database, so they should not have ambiguous spellings.
The original NELL dataset contains one row per triple (fact).
Triples can be read like a terse, well-defined sentence.
Knowledge triples describe a single isolated fact about the world.
They give you one piece of information about an entity (object) in the world.

As a minimum, a knowledge triple consists of an entity, relation and value.
The first element of a knowledge triple gives you the name of the entity that the fact is about.
The second column, "relation," contains the relationship to some other quality (adjective) or object (noun) in the world called its value.
A relation is usually a verb phrase that starts with or implies words like "is" or "has."
The third column, "value," contains an identifier for some quality of that relation.
The "value" is the object of the relationship and is a named entity just as the subject ("entity") of the triple is.

Because NELL crowdsources the curation of the knowledge base, you also have a probability or confidence value that you can use to make inferences on conflicting pieces of information.
And NELL has 9 more columns of information about the fact.
It lists all the alternative phrases that were used to reference a particular entity, relation or value.
NELL also identifies the iteration (loop through Twitter) that the fact was created during.
The last column provides the source of the data - a list of all the texts that created the fact.

NELL contains facts about more than 800 unique relations and more than 2 million entities.
Because Twitter is mostly about people, places and businesses, it's a good knowledge base to use to augment a common sense knowledge base.
And it can be useful for doing fact-checking about famous people or businesses and places that are often the targets of misinformation campaigns.
There's even a "latitudelongitude" relation that you could use to verify any facts related to the location of things.

[source,python]
----
>>> islatlon = df['relation'] == 'latlon'
>>> df[islatlon].head()
               entity relation                 value
241          cheveron   latlon      40.4459,-79.9577
528        licancabur   latlon   -22.83333,-67.88333
1817             tacl   latlon     13.53333,37.48333
2967            okmok   latlon  53.448195,-168.15472
2975  redoubt_volcano   latlon   60.48528,-152.74306
----


== Extracting the structure of text
// SUM: Dependency parsing to create a sentence diagram and extract relations about the world.

In the previous section, you learned how to recognize and tag named entities in text.
Now you'll learn how to find relationships between these entities.
This can help your NLP pipeline "understand" more complex thoughts or ideas.
NLP researchers have identified two separate problems or models that can be used to identify how the words in a sentence work together to create meaning: _dependency parsing_ and _constituency parsing_.
_Dependency parsing_ will give your NLP pipelines the ability to diagram sentences like you learned to do in grammar school (elementary school).
And these tree data structures give your model a representation of the logic and grammar of a sentence.
This will help your bots become a bit smarter about how they interpret sentences and act on them.

_Constituency parsing_ is another technique, and it's concerned with identifying the _constituent subphrases_ in a sentence. 
While dependency parsing deals with relationships between words, constituency parsing aims to parse a sentence into a series of constituents. 
These constituents can be, for example, a noun phrase ("My new computer") or a verb phrase ("has memory issues").
Its approach is more top-down, trying to iteratively break constituents into smaller units and relationships between them. 
Though constituency parsing can capture more syntactic information about the sentence, its results are slower to compute and more difficult to interpret. 
So we will mostly discuss dependency parsing in this chapter.  

But wait, you're probably wondering why sentence diagrams are so important.
After all, you've probably already forgotten how to create them yourself and have probably never used them in real life.
But that's only because you've internalized this model of the world.
We need to create that understanding in bots so they can be used to do the same things you do without thinking:

- Grammar checkers
- Spell checkers
- Writing coaches
- Translation
- Common sense understanding
- Intent recognition
- Virtual assistants
- Prosocial AI (social intelligence)

Basically, dependency parsing will help your NLP pipelines for all those applications mentioned in Chapter 1... better.
Have you noticed how chatbots like GPT-3 often fall on their face when it comes to understanding simple sentences or having a substantive conversation?
As soon as you start to ask them about the logic or reasoning of the words they are "saying" they stumble.
Chatbot developers and conversation designers get around this limitation by using rule-based chatbots for substantive conversations like therapy and teaching.
The open-ended neural network models like PalM and GPT-3 are only used when the user tries to talk about something that hasn't yet been programmed into it.
And the language models are trained with the objective of steering the conversation back to something that the bot knows about and has rules for.
Jakub Konrád and his teammates at CTU Prague won the $1M SocialBot prize in 2021 with this approach.footnote:["Alquist 4.0: Towards Social Intelligence Using Generative Models and Dialogue Personalization" (https://arxiv.org/pdf/2109.07968.pdf)]

// HL: show example convo with Mitsuku

Dependency parsing, as the name suggests, relies on "dependencies" between the words in a sentence to extract information.
"Dependencies" between two words could refer to their grammatical, phrasal, or any custom relations.
But in the context of dependency parse trees, we refer to the grammatical relationships between word pairs of the sentence, one of them acting as the "head" and the other one the "dependent".
There exists one word in the sentence which isn't dependent on any other word in the parse tree, and this word is called the ROOT.
There are 37 "dependent" relations that a word could possibly have, and these relations are adapted from the *Universal Stanford Dependencies system*.

// HL:show dependency parse image

This technique can be really useful in rule-based information extraction, especially in chatbots.
Consider the example we used earlier in this chapter: "Remind me to read aiindex.org on Monday."
Running this sentence through a dependency parser reveals that the relationship between "read" and "aiindex.org" is "Direct Object" and that between "read" and "Monday" is "Prepositional Object".
How is this information useful to us?
Let us say the chatbot had to find out what exactly it needs to remind the user to read.
Examining the "Direct Object" would reveal that it is "aiindex.org" that it needs to remind the user to study.
Similarly, it can also infer that it needs to do this on Monday.

This way, all the chatbot needs to do to pinpoint the exact information it is looking for is to examine the dependencies between the words.
This kind of a rule-based algorithm is surprisingly powerful for general tasks in chatbots and other word-processing apps.

=== Why is it important?

Like in the example we discussed before, dependency parsing can play a really useful role in any application that tries to extract organized information from text.
The dependency trees can also be used to identify "Subject-Verb-Object" triplets using the "nsubj" and "dobj" tags of the ROOT word, and this task is also called *Relation Extraction*.
Sometimes, the dependency relations can be converted into semantic tags/labels between the words, and this task is called *Semantic Role Labeling*.

==== State-of-the-art dependency parsers

SpaCy and Hugging Face transformers have been the most popular libraries for Dependency parsing.
In version 3.0 spaCy incorporated Hugging Face transformers into its larger language model pipelines so it is now the leader.
Allen AI's parser is catching up in terms of accuracy on benchmark datasets, but you'll want to stick with spaCy for most information extraction and dependency parsing tasks.

[source,python]
----
>>> import spacy
>>> nlp = spacy.load("en_core_web_sm")
>>> sentence = "We will be learning NLP today!"
>>> print ("{:<15} | {:<8} | {:<15} | {:<30} | {:<20}".format('Token','Relation','Head', 'Children', 'Meaning'))
>>> print ("-" * 115)

>>> for token in doc:
...     # Print the token, dependency nature, head, all dependents of the token, and meaning of the dependency
...     print ("{:<15} | {:<8} | {:<15} | {:<30} | {:<20}"
...             .format(str(token.text), str(token.dep_), str(token.head.text), str([child for child in token.children]) , str(spacy.explain(token.dep_))[:17] ))

Token           | Relation | Head            | Children                       | Meaning
-------------------------------------------------------------------------------------------------
We              | nsubj    | learning        | []                             | nominal subject
will            | aux      | learning        | []                             | auxiliary
be              | aux      | learning        | []                             | auxiliary
learning        | ROOT     | learning        | [We, will, be, NLP, today, !]  | root
NLP             | dobj     | learning        | []                             | direct object
today           | npadvmod | learning        | []                             | noun phrase as ad
!               | punct    | learning        | []                             | punctuation

----

SpaCy labels every token's relation, syntactic head, syntactic children, and the meaning of the relation.
The token "learning" has been assigned the tag of `"ROOT"`.
This is because in our sentence, the word "learning" happens to be the main verb when you organize it into a Subject-Verb-Object triple.

Such verbs are called the _root_ verb and have the `"ROOT"` label in spaCy.
The root word in a sentence does do not have any additional words that they are building on.
All other words in the sentence depend on the _root_ word in a sentence to clarify their the meaning.
Linguists say that root words in a dependency tree have no ancestors or parents.
You can use this library to extract clauses by separating the subtrees attached to the root by the relation of `"advcl"` or `"relcl"`.
You can also use it to extract relation triplets by identifying the tokens with `"nsubj"`, `"ROOT"`, and `"dobj"` dependencies.


==== State-of-the-art constituency parsers 

Berkeley Neural Parser and Stanza have been the go-to options for the extraction of constituency relations in text.
Let's explore one of them, Berkeley Neural Parser.

This parser cannot be used on its own, and requires either spaCy or NLTK to load it along with their existing models.
You want to use spaCy as your tokenizer and dependency tree parse because it is continually improving.

.Download the necessary packages
[source,python]
----
>>> import benepar
>>> benepar.download('benepar_en3')
----

After downloading the packages, we can test it out with a sample sentence.
But we will be adding `benepar` to spaCy's pipeline first.

// FIXME: Can't find factory for 'benepar' ... component name that's not registered
[source,python]
----
>>> import spacy
>>> nlp = spacy.load("en_core_web_md")
>>> if spacy.__version__.startswith('2'):
...     nlp.add_pipe(benepar.BeneparComponent("benepar_en3"))
... else:
...     nlp.add_pipe("benepar", config={"model": "benepar_en3"})
>>> doc = nlp("Johnson was compelled to ask the EU for an extension of the deadline, which was granted")
>>> sent = list(doc.sents)[0]
>>> print(sent._.parse_string)
(S (NP (NNP Johnson)) (VP (VBD was) (VP (VBN compelled) (S (VP (TO to) (VP (VB ask) (NP (DT the) (NNP EU)) (PP (IN for) (NP (NP (DT an) (NN extension)) (PP (IN of) (NP (NP (DT the) (NN deadline)) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBD was) (VP (VBN granted)))))))))))))))
----

In the example above, we generated a parsed string for the test sentence. The parse string includes various phrases and the POS tags of the tokens in the sentence. Some common tags you may notice in our parse string are NP ("Noun Phrase"), VP ("Verb Phrase"), S ("Sentence"), and PP ("Prepositional Phrase").
You can use this module to identify all the phrases in the sentence and use them in sentence simplification and/or summarization.

== Relation extraction

Relation extraction is the process of identifying connections between named entities in any text.
Like information extraction, it is classified into two categories: closed and open.

In closed relation extraction, the model extracts relations only from a given list of relation types.
The advantages of this are that we can minimize the risk of getting untrue and bizarre relation labels between entities which makes us more confident about using them in real life.
But the limitation is that it needs human labelers to come up with a list of relevant labels for every category of text, which as you can imagine, can get tedious and expensive.

In open relation extraction, the model tries to come up with its own set of probable labels for the named entities in the text.
This is suitable for processing large and generally unknown texts like Wikipedia articles and news entries.

=== Current datasets and benchmarks
*1) TACRED*

The TAC Relation Extraction Dataset is a large scale dataset built with newswire and web text corpus.
With over 100,000 examples, it covers 41 relation types which are organized into triplets.
Over the past few years, efforts to address TACRED's limitations such as data quality and ambiguity in relation classes has given rise to datasets like Re-TACRED and DocRED.

*2) DocRED*

The Document Relation Extraction Dataset is the largest human-annotated dataset for document level relation extraction, where the model is required to go over multiple sentences in order to extract the relations between entities.
Compiled using Wikidata and Wikipedia, this dataset is considered the de-facto benchmark for relation extraction methods along with TACRED due to its generalizability and size.

*3) SemEval Task-8 dataset*

The SemEval Task-8 dataset is a triplet extraction dataset with over 10,000 entries, each having one of 9 semantic relations between its entities.
Though a much simpler dataset than TACRED and having only a few relation labels, this dataset is known for the quality of its sentence data and labels which is a big issue when it comes to TACRED, DocRED, and Re-TACRED.

=== Why is it important?
Relation extraction finds widespread application in finance and military, due to its significance in Information Extraction and Knowledge graph completion.
Traditionally considered a triplet extraction task, relation extraction methods are now venturing beyond duplet and triplet relations and are finding extensive usage in medical industry in the form of drug combo extraction and hormone chain identification.

=== Current state-of-the-art methods and the available open source platforms

Over the past few years, experiments with Deep Neural Networks have given strong results on triplet extraction and subsequently most of the research on the topic now follow neural methods.
In this section, we will be discussing two recent neural relation extraction methods which have reported state of the art results on TACRED and DocRED.

*1) LUKE:*

TODO add description and code

*2) Typed entity markers*

The concept of Typed entity markers was developed as an improvement over LUKE and other neural relation extraction frameworks.
In this method, typed markers are inserted before and after the entities in the text and fed into a multi-class classification model.
Consider the example below:

Sentence:"John Smith works at Tangible AI"

Entities and their tags: John Smith (PERSON), Tangible AI (ORGANIZATION)

Sentence with typed entities: "^/PER/John Smith^ works at ^/ORG/Tangible AI^"

Following the example above, the sentence with typed entities is fed into the classification model with relations as its labels.
As you may have guessed, NER is a necessary step before this process, for which we will be using spaCy as shown below:

[source,python]
----
>>> import spacy
>>> nlp = spacy.load("en_core_web_md")
>>> sent = "John Smith works at Tangible AI"
>>> doc = nlp(sent)
>>> entities = []
>>> for ent in doc.ents:
...     sent = sent.replace(ent.text, "^/" + ent.label_ + "/" + ent.text + "^")
>>> print(sent)
^/PER/John Smith^ works at ^/ORG/Tangible AI^

----

== Coreference resolution
Imagine you're running NER on a text, and you obtain the list of entities that the model has recognized.
On closer inspection, you realize over half of them are duplicates because they're referring to the same terms!
This is where *Coreference resolution* comes in handy because it identifies all the mentions of a noun in a sentence.
This will consolidate mentions of the same _things_ in your knowledge graph instead of creating redundant nodes and edges and potentially creating incorrect relations.

Can you see the coreferences to "Timnit Gebru" in this sentence about that paper and her bosses:

[source,python]
----
>>> i0 = text.index('Gebru had')
>>> text[i0:i0+171]
'Gebru had determined that publishing research papers
 was more effective at bringing forth the ethical change
 she was focused on than pressing her superiors in the company.'
----

[source,python]
----
>>> import spacy, coreferee
>>> nlptrf = spacy.load('en_coreference_web_trf')
>>> text_gebru = text[i0:i1]
>>> doc_gebru = nlp(text_gebru)
>>> doc_gebru
Gebru had determined that publishing research papers was more effective at bringing forth the ethical change she was focused on than pressing her superiors in the company. She and five others coauthored a research paper, "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?"
>>> doc_gebru.spans
{'coref_clusters_1': [Gebru had, she was, her superiors, She and]}

----
=== Current datasets and benchmarks

*1) Ontonotes 5.0:*
This dataset is a compilation of various corpora of text(news, conversational telephone speech, weblogs, usenet newsgroups, broadcast, talk shows) with annotations of the named entities and noun phrases and their mentions.
Available in three languages(English, Chinese, and Arabic), this dataset is the de facto benchmark for identifying coreferences in the industry.


*2) Winograd schema challenge:*
Consider this sentence- "The city councilmen refused the demonstrators a permit because they feared violence".
Who does "they" in the sentence refer to?
Our common sense tells us that it refers to the "city councilmen" and the answer seems to be easy for us, but this task of identifying mentions using common sense is surprisingly difficult for deep learning models.
This task is called the Winograd schema challenge, also framed as "Commonsense reasoning" or "Commonsense inference" problem.

=== Why is it important?

Duplicate mentions are a big problem not only in *NER*, but *Relation extraction*, *Information extraction*, *Semantic parsing*, and many other tasks.
Resolving all the pronouns saves the time and effort to extract the information associated with them.

Moreover, it also helps us identify which entity or term is being talked about the most in a text, helping us assign importance to certain words over others.
This technique has been experimented with in topic modeling and in constructing _knowledge graphs_.


=== State-of-the-art coreference resolution

NeuralCoref 4.0 was the fastest and most accurate entity resolver available in the open source community.
So spaCy incorporated it into its "Universe" collection of pipelines and models.
In order to use it you will need to import the `coreferee` package.
You cannot use the original `neuralcoref`` package within the latest spaCy package (3.5 or greater) because `neuralcoref` is no longer activately maintained.

[source,python]
----
>>> import spacy
>>> nlp = spacy.load('en_core_web_md')
>>>
>>> import neuralcoref
>>> neuralcoref.add_to_pipe(nlp)
>>>
>>> doc = nlp(u'My sister has a dog. She loves him.')
>>>
>>> doc._.coref_clusters

----

On running the code above, you'll get a list of indices in an array.
These are the indices of the words which the model identifies to be mentionings of the same noun phrases.


2) AllenNLP's Entity resolver

AllenNLP also provides a highly effective open source pipeline for Coreference resolution, though it is known to be much slower compared to NeuralCoref has a high memory requirement.
Let us see how it works:

[source,python]
----
>>> from allennlp.predictors.predictor import Predictor
>>> import allennlp_models.tagging
>>>
>>> predictor = Predictor.from_path("https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2021.03.10.tar.gz")
>>> predictor.predict(
    document="Paul Allen was born on January 21, 1953, in Seattle, Washington, to Kenneth Sam Allen and Edna Faye Allen. Allen attended Lakeside School, a private school in Seattle, where he befriended Bill Gates, two years younger, with whom he shared an enthusiasm for computers."
    )
>>>
----

== Information extraction

So you've learned that "information extraction" is converting unstructured text into structured information stored in a knowledge base or knowledge graph.
Information extraction is part of an area of research called natural language understanding (NLU), though that term is often used synonymously with natural language processing (NLP).

Information extraction and NLU is a different kind of learning than you may think of when researching data science.
It isn't only unsupervised learning; even the very "model" itself, the logic about how the world works, can be composed without human intervention.
Instead of giving your machine fish (facts), you're teaching it how to fish (extract information).
Nonetheless, machine learning techniques are often used to train the information extractor.

== Regular patterns

You need a pattern-matching algorithm that can identify sequences of characters or words that match the pattern so you can "extract" them from a longer string of text.
The straightforward way to build a pattern matching function is in Python, with a sequence of if/then statements that check for your pattern at each position of a string.
Say you wanted to find some common greeting words, such as "Hi", "Hello", and "Yo", at the beginning of a statement. You might do it something like this:

.Pattern hardcoded in Python
[source,python]
----
>>> def find_greeting(s):
...     """ Return greeting str (Hi, etc) if greeting pattern matches """
...     if s[0] == 'H':
...         if s[:3] in ['Hi', 'Hi ', 'Hi,', 'Hi!']:
...             return s[:2]
...         elif s[:6] in ['Hello', 'Hello ', 'Hello,', 'Hello!']:
...             return s[:5]
...     elif s[0] == 'Y':
...         if s[1] == 'o' and s[:3] in ['Yo', 'Yo,', 'Yo ', 'Yo!']:
...             return s[:2]
...     return None
----

And here's how it would work:

.Brittle pattern-matching example
[source,python]
----
>>> find_greeting('Hi Mr. Turing!')
'Hi'
>>> find_greeting('Hello, Rosa.')
'Hello'
>>> find_greeting("Yo, what's up?")
'Yo'
>>> find_greeting("Hello")
'Hello'
>>> print(find_greeting("hello"))
None
>>> print(find_greeting("HelloWorld"))
None
----

You can probably see how tedious it would be to program a pattern-matching algorithm this way.
And it's not even that good.
It's quite brittle because it relys on precise spellings, capitalization and the position of characters within a string.
It is extremely difficult to specify all the possible "delimiters" (punctuation, white space, NULL characters) that are on either side of the words you are looking for.

You could probably come up with a modular set of functions that to specify different words or strings that you want to look for.
But where have you seen this kind of pattern matching before?
Regular expressions!

Python has a regular expression interpreter (compiler and runner) in the standard library package `re`.
And if you need to allow for some fuzziness in your patterns, to make your patterns more robust to typos and mispellings, there is the Python `regex` package.
So if you ever need to match a well-defined text pattern, you probably want to use regular expressions.
It turns out that numerical mathematical expressions are very well-defined and lend themselves to regular expressions are a good application of regular expression pattern matching.
We used this approach to extract mathematical expressions for the Rori math tutor bot within the `mathtext` package now on PyPi.footnote:[The "mathtext" Python package on PyPi (https://pypi.org/projects/mathtext)] footnote:[The "mathtext" source code on GitLab (https://gitlab.com/tangibleai/community/mathtext)]


=== Regular expressions

Regular expressions are strings written in a special computer language that you can use to specify algorithms.
Regular expressions are a lot more powerful, flexible, and concise than the equivalent Python you'd need to write to match patterns like this.
So regular expressions are the pattern definition language of choice for many NLP problems involving pattern matching.
This NLP application is an extension of their original use for compiling and interpreting formal languages (computer languages).

Regular expressions define a _finite state machine_ or FSM -- a tree of "if-then" decisions about a sequence of symbols, such as the `find_greeting()` function in listing 11.1.
The symbols in the sequence are passed into the decision tree of the FSM one symbol at a time.
A finite state machine that operates on a sequence of symbols such as ASCII character strings, or a sequence of English words, is called a _grammar_.
They can also be called _formal grammars_ to distinguish them from natural language grammar rules you learned in elementary school.

In computer science and mathematics, the word "grammar" refers to the set of rules that determine whether or a sequence of symbols is a valid member of a language, often called a computer language or formal language.
And a computer language, or formal language, is the set of all possible statements that would match the formal grammar that defines that language.
That's kind of a circular definition, but that's the way mathematics works sometimes.
You probably want to review appendix B if you aren't familiar with basic regular expression syntax and symbols such as `r'.\*'` and `r'a-z'`.

=== Information extraction as ML feature extraction

So you're back where you started in Chapter 1, where we first mentioned regular expressions.
But didn't you switch from "grammar-based" NLP approaches at the end of Chapter 1 in favor of machine learning and data-driven approaches?
Why return to hard-coded (manually composed) regular expressions and patterns?
Because your statistical or data-driven approach to NLP has limits.

You want your machine learning pipeline to be able to do some basic things, such as answer logical questions, or perform actions such as scheduling meetings based on NLP instructions.
And machine learning falls flat here.
You rarely have a labeled training set that covers the answers to all the questions people might ask in natural language.
Plus, as you'll see here, you can define a compact set of condition checks (a regular expression) to extract key bits of information from a natural language string.
And it can work for a broad range of problems.

Pattern matching (and regular expressions) continue to be the state-of-the art approach for information extraction (more commonly called _information retrieval_).
Even with machine learning approaches to natural language processing, you need to do feature engineering.
You need to create bags of words or "embeddings" of words to try to reduce the nearly infinite possibilities of meaning in natural language text into a vector that a machine can process easily.
Information extraction is just another form of machine learning feature extraction from unstructured natural language data, such as creating a bag of words, or doing PCA on that bag of words.
And these patterns and features are still employed in even the most advanced natural language machine learning pipelines such as Google's Assistant, Siri, Amazon Alexa, and other state-of-the-art "bots."

Information extraction is used to find statements and information that you might want your chatbot to have "on the tip of its tongue."
Information extraction can be accomplished beforehand to populate a knowledge base of facts.
Alternatively, the required statements and information can be found on-demand, when the chatbot is asked a question or a search engine is queried.
When a knowledge base is built ahead of time, the data structure can be optimized to facilitate faster queries within larger domains of knowledge.
A prebuilt knowledge base enables the chatbot to respond quickly to questions about a wider range of information.
If information is retrieved in real-time, as the chatbot is being queried, this is often called "search."
Google and other search engines combine these two techniques, querying a knowledge graph (knowledge base) and falling back to text search if the necessary facts aren't found.
Many of the natural language grammar rules you learned in school can be encoded in a formal grammar designed to operate on words or symbols representing parts of speech.
And the English language can be thought of as the words and grammar rules that make up the language.
Or you can think of it as the set of all possible things you could say that would be recognized as valid statements by an English language speaker.

And that brings us to another feature of formal grammars and finite state machines that will come in handy for NLP.
Any formal grammar can be used by a machine in two ways:

* To recognize "matches" to that grammar
* To generate a new sequence of symbols

Not only can you use patterns (regular expressions) for extracting information from natural language, but you can also use them to generate strings that match that pattern!
Check out the `rstr` (short for "random string") package if you ever need to generate example strings that match a regular expresssion.footnote:["Rstr package on PyPi (https://pypi.org/project/rstr/).] for some of your information extraction patterns here.

This formal grammar and finite state machine approach to pattern matching has some other awesome features.
A true finite state machine is guaranteed to eventually stop (halt) in a finite number of steps.
So if you use a regular expression as your pattern matcher you know that you will always receive an answer to your question about whether you've found a match in your string or not.
It will never get caught in a perpetual loop... as long as you don't "cheat" and use look-aheads or look-backs in your regular expressions.
And because a regular expression is deterministic it always returns a match or non-match.
It will never give you less than 100% confidence or probability of there being a match.

So you'll stick to regular expressions that don't require these "look-back" or "look-ahead" cheats.
You'll make sure your regular expression matcher processes each character and moves ahead to the next character only if it matches -- sort of like a strict train conductor walking through the seats checking tickets.
If you don't have one, the conductor stops and declares that there's a problem, a mismatch, and he refuses to go on, or look ahead or behind you until he resolves the problem.
There are no "go backs" or "do overs" for train passengers, or for strict regular expressions.

== Information worth extracting

Some keystone bits of quantitative information are worth the effort of "hand-crafted" regular expressions:

* GPS locations
* Dates
* Prices
* Numbers

Other important pieces of natural language information require more complex patterns than are easily captured with regular expressions:

* Question trigger words
* Question target words
* Named entities

=== Extract GPS coordinates

GPS locations are typical of the kinds of numerical data you'll want to extract from text using regular expressions.
GPS locations come in pairs of numerical values for latitude and longitude.
They sometimes also include a third number for altitude, or height above sea level, but you'll ignore that for now.
Let's just extract decimal latitude/longitude pairs, expressed in degrees.
This will work for many Google Maps URLs.
Though URLs are not technically natural language, they are often part of unstructured text data, and you'd like to extract this bit of information, so your chatbot can know about places as well as things.

Let's use your decimal number pattern from previous examples, but let's be more restrictive and make sure the value is within the valid range for latitude (\+/- 90 deg) and longitude (+/- 180 deg).
You can't go any farther north than the North Pole (+90 deg) or farther south than the South Pole (-90 deg).
And if you sail from Greenwich England 180 deg east (+180 deg longitude), you'll reach the date line, where you're also 180 deg west (-180 deg) from Greenwich.

.Regular expression for GPS coordinates
[source,python]
----
>>> import re
>>> lat = r'([-]?[0-9]?[0-9][.][0-9]{2,10})'
>>> lon = r'([-]?1?[0-9]?[0-9][.][0-9]{2,10})'
>>> sep = r'[,/ ]{1,3}'
>>> re_gps = re.compile(lat + sep + lon)

>>> re_gps.findall('http://...maps/@34.0551066,-118.2496763...')
[(34.0551066, -118.2496763)]

>>> re_gps.findall("https://www.openstreetmap.org/#map=10/5.9666/116.0566")
[('5.9666', '116.0566')]

>>> re_gps.findall("Zig Zag Cafe is at 45.344, -121.9431 on my GPS.")
[('45.3440', '-121.9431')]
----

Numerical data is pretty easy to extract, especially if the numbers are part of a machine-readable string.
URLs and other machine-readable strings put numbers such as latitude and longitude in a predictable order, format, and units to make things easy for us.

This pattern will still accept some out-of-this-world latitude and longitude values, but it gets the job done for most of the URLs you'll copy from mapping web apps such as OpenStreetMap.

But what about dates?
Will regular expressions work for dates?
What if you want your date extractor to work in Europe and the US, where the order of day/month is often reversed?

=== Extracting dates

Dates are a lot harder to extract than GPS coordinates.
Dates are a more natural language, with different dialects for expressing similar things.
In the US, Christmas 2017 is "12/25/17."
In Europe, Christmas 2017 is "25/12/17."
You could check the locale of your user and assume that they write dates the same way as others in their region.
But this assumption can be wrong.

So most date and time extractors try to work with both kinds of day/month orderings and just check to make sure it's a valid date.
This is how the human brain works when we read a date like that.
Even if you were a US English speaker and you were in Brussels around Christmas, you'd probably recognize "25/12/17" as a holiday, because there are only 12 months in the year.

This "duck-typing" approach that works in computer programming can work for natural language, too.
If it looks like a duck and acts like a duck, it's probably a duck.
If it looks like a date and acts like a date, it's probably a date.
You'll use this "try it and ask forgiveness later" approach for other natural language processing tasks as well.
You'll try a bunch of options and accept the one that works.
You'll try your extractor or your generator, and then you'll run a validator on it to see if it makes sense.

For chatbots this is a particularly powerful approach, allowing you to combine the best of multiple natural language generators.
In chapter 10 you generated some chatbot replies using LSTMs.
To improve the user experience, you could generate a lot of replies and choose the one with the best spelling, grammar, and sentiment.
We'll talk more about this in chapter 12.

.Regular expression for US dates
[source,python]
----
>>> us = r'((([01]?\d)[-/]([0123]?\d))([-/]([0123]\d)\d\d)?)'
>>> mdy = re.findall(us, 'Santa came 12/25/2017. An elf appeared 12/12.')
>>> mdy
[('12/25/2017', '12/25', '12', '25', '/2017', '20'),
 ('12/12', '12/12', '12', '12', '', '')]
----

A list comprehension can be used to provide a little structure to that extracted data, by converting the month, day, and year into integers and labeling that numerical information with a meaningful name.

.Structuring extracted dates
[source,python]
----
>>> dates = [{'mdy': x[0], 'my': x[1], 'm': int(x[2]), 'd': int(x[3]),
...     'y': int(x[4].lstrip('/') or 0), 'c': int(x[5] or 0)} for x in mdy]
>>> dates
[{'mdy': '12/25/2017', 'my': '12/25', 'm': 12, 'd': 25, 'y': 2017, 'c': 20},
 {'mdy': '12/12', 'my': '12/12', 'm': 12, 'd': 12, 'y': 0, 'c': 0}]
----

Even for these simple dates, it's not possible to design a regex that can resolve all the ambiguities in the second date, "12/12."
There are ambiguities in the language of dates that only humans can guess at resolving using knowledge about things like Christmas and the intent of the writer of a text.
For examle "12/12" could mean:

* December 12th, 2017 -- month/day in the estimated year based on anaphora resolution footnote:[Issues in Anaphora Resolution
by Imran Q. Sayed for Stanford's CS224N course: https://nlp.stanford.edu/courses/cs224n/2003/fp/iqsayed/project_report.pdf .]
* December 12th, 2018 -- month/day in the current year at time of publishing
* December 2012 -- month/day in the


Because month/day come before the year in US dates and in our regex, '12/12' is presumed to be December 12th of an unknown year.
You can fill in any missing numerical fields with the most recently read year using the "context" from the structured data in memory:

.Basic context maintenance
[source,python]
----
>>> for i, d in enumerate(dates):
...     for k, v in d.items():
...         if not v:
...             d[k] = dates[max(i - 1, 0)][k]  # <1>
>>> dates
[{'mdy': '12/25/2017', 'my': '12/25', 'm': 12, 'd': 25, 'y': 2017, 'c': 20},
 {'mdy': '12/12', 'my': '12/12', 'm': 12, 'd': 12, 'y': 2017, 'c': 20}]
>>> from datetime import date
>>> datetimes = [date(d['y'], d['m'], d['d']) for d in dates]
>>> datetimes
[datetime.date(2017, 12, 25), datetime.date(2017, 12, 12)]
----
<1> This works because both the `dict` and the `list` are mutable data types.


This is a basic but reasonably robust way to extract date information from natural language text.
The main remaining tasks to turn this into a production date extractor would be to add some exception catching and context maintenance that is appropriate for your application.
If you added that to the `nlpia` package (http://github.com/totalgood/nlp) with a PR I'm sure your fellow readers would appreciate it.
And if you added some extractors for times, well, then you'd be quite the hero.

There are opportunities for some hand-crafted logic to deal with edge cases and natural language names for months and even days.
But no amount of sophistication could resolve the ambiguity in the date "12/11."
That could be

* December 11th in whatever year you read or heard it
* November 12th if you heard it in London or Launceston, Tasmania (a commonwealth territory)
* December 2011 if you read it in a US newspaper
* November 2012 if you read it in an EU newspaper

Some natural language ambiguities can't be resolved, even by a human brain.
But let's just make sure your date extractor can handle European day/month order by reversing month and day in your regex.

.Regular expression for European dates
[source,python]
----
>>> eu = r'((([0123]?\d)[-/]([01]?\d))([-/]([0123]\d)?\d\d)?)'
>>> dmy = re.findall(eu, 'Alan Mathison Turing OBE FRS (23/6/1912-7/6/1954) \
...     was an English computer scientist.')
>>> dmy
[('23/6/1912', '23/6', '23', '6', '/1912', '19'),
 ('7/6/1954', '7/6', '7', '6', '/1954', '19')]
>>> dmy = re.findall(eu, 'Alan Mathison Turing OBE FRS (23/6/12-7/6/54) \
...     was an English computer scientist.')
>>> dmy
[('23/6/12', '23/6', '23', '6', '/12', ''),
 ('7/6/54', '7/6', '7', '6', '/54', '')]
----

That regular expression correctly extracts Turing's birth and wake dates from a Wikipedia excerpt.
But I cheated, I converted the month "June" into the number 6 before testing the regular expression on that Wikipedia sentence.
So this isn't a realistic example.
And you'd still have some ambiguity to resolve for the year if the century is not specified.
Does the year `54` mean `1954` or does it mean `2054`?
You'd like your chatbot to be able to extract dates from unaltered Wikipedia articles so it can read up on famous people and learn import dates.
For your regex to work on more natural language dates, such as those found in Wikipedia articles, you need to add words such as "June" (and all its abbreviations) to your date-extracting regular expression.

You don't need any special symbols to indicate words (characters that go together in sequence).
You can just type them in the regex exactly as you'd like them to be spelled in the input, including capitalization.
All you have to do is put an `OR` symbol (`|`) between them in the regular expression.
And you need to make sure it can handle US month/day order, as well as the European order.
You'll add these two alternative date "spellings" to your regular expression with a "big" OR (`|`) between them as a fork in your tree of decisions in the regular expression.

Let's use some named groups to help you recognize years such as "'84" as 1984 and "08" as 2008.
And let's try to be a little more precise about the 4-digit years you want to match, only matching years in the future up to 2399 and in the past back to year 0.footnote:[See the web page titled "Year zero - Wikipedia" (https://en.wikipedia.org/wiki/Year_zero).]

.Recognizing years
[source,python]
----
>>> yr_19xx = (
...     r'\b(?P<yr_19xx>' +
...     '|'.join('{}'.format(i) for i in range(30, 100)) +
...     r')\b'
...     )  # <1>
>>> yr_20xx = (
...     r'\b(?P<yr_20xx>' +
...     '|'.join('{:02d}'.format(i) for i in range(10)) + '|' +
...     '|'.join('{}'.format(i) for i in range(10, 30)) +
...     r')\b'
...     )  # <2>
>>> yr_cent = r'\b(?P<yr_cent>' + '|'.join(
...     '{}'.format(i) for i in range(1, 40)) + r')'  # <3>
>>> yr_ccxx = r'(?P<yr_ccxx>' + '|'.join(
...     '{:02d}'.format(i) for i in range(0, 100)) + r')\b'  # <4>
>>> yr_xxxx = r'\b(?P<yr_xxxx>(' + yr_cent + ')(' + yr_ccxx + r'))\b'
>>> yr = (
...     r'\b(?P<yr>' +
...     yr_19xx + '|' + yr_20xx + '|' + yr_xxxx +
...     r')\b'
...     )
>>> groups = list(re.finditer(
...     yr, "0, 2000, 01, '08, 99, 1984, 2030/1970 85 47 `66"))
>>> full_years = [g['yr'] for g in groups]
>>> full_years
['2000', '01', '08', '99', '1984', '2030', '1970', '85', '47', '66']
----
<1> 2-digit years 30-99 => 1930-1999
<2> 1- or 2-digit years 01-30 => 2001-2030
<3> First digits of a 3- or 4-digit yr such as the "1" in "123 A.D." or "20" in "2018"
<4> Last 2 digits of a 3- or 4-digit yr such as the "23" in "123 A.D." or "18" in "2018"

Wow!
That's a lot of work, just to handle some simple year rules in regex rather than in Python.
Don't worry, packages are available for recognizing common date formats.
They are much more precise (fewer false matches) and more general (fewer misses).
So you don't need to be able to compose complex regular expressions such as this yourself.
This example just gives you a pattern in case you need to extract a particular kind of number using a regular expression in the future.
Monetary values and IP addresses are examples where a more complex regular expression, with named groups, might come in handy.

Let's finish up your regular expression for extracting dates by adding patterns for the month names such as "June" or "Jun" in Turing's birthday on Wikipedia dates.

.Recognizing month words with regular expressions
[source,python]
----
>>> mon_words = 'January February March April May June July ' \
...     'August September October November December'
>>> mon = (r'\b(' + '|'.join('{}|{}|{}|{}|{:02d}'.format(
...     m, m[:4], m[:3], i + 1, i + 1) for i, m in enumerate(mon_words.split())) +
...     r')\b')
>>> re.findall(mon, 'January has 31 days, February the 2nd month of 12, has 28, except in a Leap Year.')
['January', 'February', '12']
----

Can you see how you might combine these regular expressions into a larger one that can handle both EU and US date formats?
One complication is that you can't reuse the same name for a group (parenthesized part of the regular expression).
So you can't just put an OR between the US and EU ordering of the named regular expressions for month and year.
And you need to include patterns for some optional separators between the day, month, and year.

Here's one way to do all that.

.Combining information extraction regular expressions
[source,python]
----
>>> day = r'|'.join('{:02d}|{}'.format(i, i) for i in range(1, 32))
>>> eu = (r'\b(' + day + r')\b[-,/ ]{0,2}\b(' +
...     mon + r')\b[-,/ ]{0,2}\b(' + yr.replace('<yr', '<eu_yr') + r')\b')
>>> us = (r'\b(' + mon + r')\b[-,/ ]{0,2}\b(' +
...     day + r')\b[-,/ ]{0,2}\b(' + yr.replace('<yr', '<us_yr') + r')\b')
>>> date_pattern = r'\b(' + eu + '|' + us + r')\b'
>>> list(re.finditer(date_pattern, '31 Oct, 1970 25/12/2017'))
[<_sre.SRE_Match object; span=(0, 12), match='31 Oct, 1970'>,
 <_sre.SRE_Match object; span=(13, 23), match='25/12/2017'>]
----

Finally, you need to validate these dates by seeing if they can be turned into valid Python `datetime` objects.

.Validating dates
[source,python]
----
>>> import datetime
>>> dates = []
>>> for g in groups:
...     month_num = (g['us_mon'] or g['eu_mon']).strip()
...     try:
...         month_num = int(month_num)
...     except ValueError:
...         month_num = [w[:len(month_num)]
...             for w in mon_words].index(month_num) + 1
...     date = datetime.date(
...         int(g['us_yr'] or g['eu_yr']),
...         month_num,
...         int(g['us_day'] or g['eu_day']))
...     dates.append(date)
>>> dates
[datetime.date(1970, 10, 31), datetime.date(2017, 12, 25)]
----

Your date extractor appears to work OK, at least for a few simple, unambiguous dates.
Think about how packages such as `Python-dateutil` and `datefinder` are able to resolve ambiguities and deal with more "natural" language dates such as "today" and "next Monday."
And if you think you can do it better than these packages, send them a pull request!

If you just want a state-of-the-art date extractor, statistical (machine learning) approaches will get you there faster.
The Stanford Core NLP SUTime library (https://nlp.stanford.edu/software/sutime.html) and `dateutil.parser.parse` by Google are the state of the art.

== Extracting relationships (relations)

So far you've looked only at extracting tricky noun instances such as dates and GPS latitude and longitude values.
And you've worked mainly with numerical patterns.
It's time to tackle the harder problem of extracting knowledge from natural language.
You'd like your bot to learn facts about the world from reading an encyclopedia of knowledge such as Wikipedia.
You'd like it to be able to relate those dates and GPS coordinates to the entities it reads about.

What knowledge could your brain extract from this sentence from Wikipedia:

_On March 15, 1554, Desoto wrote in his journal that the Pascagoula people ranged as far north as the confluence of the Leaf and Chickasawhay rivers at 30.4, -88.5._

Extracting the dates and the GPS coordinates might enable you to associate that date and location with Desoto, the Pascagoula people, and two rivers whose names you can't pronounce.
You'd like your bot (and your mind) to be able to connect those facts to larger facts -- for example, that Desoto was a Spanish conquistador and that the Pascagoula people were a peaceful native American tribe.
And you'd like the dates and locations to be associated with the right "things": Desoto, and the intersection of two rivers, respectively.

This is what most people think of when they hear the term natural language understanding.
To understand a statement you need to be able to extract key bits of information and correlate it with related knowledge.
For machines, you store that knowledge in a graph, also called a knowledge base.
The edges of your knowledge graph are the relationships between things.
And the nodes of your knowledge graph are the nouns or objects found in your corpus.

The pattern you're going to use to extract these relationships (or relations) is a pattern such as SUBJECT - VERB - OBJECT.
To recognize these patterns, you'll need your NLP pipeline to know the parts of speech (POS) for each word in a sentence.

=== POS tagging


So to build your knowledge graph, you just need to figure out which objects (noun phrases) should be paired up.
You'd like to pair up the date "March 15, 1554" with the "named entity" Desoto.
You could then normalize those two strings (noun phrases) to point to objects you have in your knowledge base.
March 15, 1554 can be converted to a `datetime.date` object with a normalized representation.

spaCy-parsed sentences also contain the dependency tree in a nested dictionary.
And `spacy.displacy` can generate an _scalable vector graphics_ SVG string (or a complete HTML page), which can be viewed as an image in a browser.
This visualization can help you find ways to use the tree to create tag patterns for relation extraction.

.Visualize a dependency tree
[source,python]
----
>>> from spacy.displacy import render
>>> sentence = "In 1541 Desoto wrote in his journal about the Pascagoula."
>>> parsed_sent = nlp(sentence)
>>> with open('pascagoula.html', 'w') as f:
...     f.write(render(docs=parsed_sent, page=True, options=dict(compact=True)))
----

The dependency tree for this short sentence shows that the noun phrase "the Pascagoula" is the object of the relationship "met" for the subject "Desoto" (see figure 11.2).
And both nouns are tagged as proper nouns.

.The Pascagoula people
image::../images/ch11/pascagoula.jpg[Dependency tree for sentence about the Pascagoula people, width=80%, link="../images/ch11/pascagoula.jpg"]

To create POS and word property patterns for a `spacy.matcher.Matcher`, listing all the token tags in a table is helpful.
Here are some helper functions to make that easier:

.Helper functions for spaCy tagged strings
[source,python]
----
>>> import pandas as pd
>>> from collections import OrderedDict

>>> def token_dict(token):
...     return OrderedDict(ORTH=token.orth_, LEMMA=token.lemma_,
...         POS=token.pos_, TAG=token.tag_, DEP=token.dep_)

>>> def doc_dataframe(doc):
...     return pd.DataFrame([token_dict(tok) for tok in doc])

>>> doc_dataframe(nlp("In 1541 Desoto met the Pascagoula."))
         ORTH       LEMMA    POS  TAG    DEP
0          In          in    ADP   IN   prep
1        1541        1541    NUM   CD   pobj
2      Desoto      desoto  PROPN  NNP  nsubj
3         met        meet   VERB  VBD   ROOT
4         the         the    DET   DT    det
5  Pascagoula  pascagoula  PROPN  NNP   dobj
6           .           .  PUNCT    .  punct
----

Now you can see the sequence of POS or TAG features that will make a good pattern.
If you're looking for "has-met" relationships between people and organizations, you'd probably like to allow patterns such as "PROPN met PROPN", "PROPN met the PROPN", "PROPN met with the PROPN", and "PROPN often meets with PROPN".
You could specify each of those patterns individually, or try to capture them all with some * or ? operators on "any word" patterns between your proper nouns:

[source,]
----
'PROPN ANYWORD? met ANYWORD? ANYWORD? PROPN'
----

Patterns in spaCy are much more powerful and flexible than the preceding pseudocode, so you have to be more verbose to explain exactly the word features you'd like to match.
In a spaCy pattern specification you use a dictionary to capture all the tags that you want to match for each token or word.

[source,python]
.Example spaCy POS pattern
----
>>> pattern = [
...     {'TAG': 'NN', 'OP': '+'},
...     {'IS_ALPHA': True, 'OP': '*'},
...     {'LEMMA': 'meet'},
...     {'IS_ALPHA': True, 'OP': '*'},
...     {'TAG': 'NN', 'OP': '+'}]
----

You can then extract the tagged tokens you need from your parsed sentence.

.Creating a POS pattern matcher with spaCy
[source,python]
----
>>> from spacy.matcher import Matcher
>>> doc = nlp("In 1541 Desoto met the Pascagoula.")
>>> matcher = Matcher(nlp.vocab)
>>> matcher.add(
...     key='met',
...     patterns=[pattern])
>>> matches = matcher(doc)
>>> matches
[(12280034159272152371, 2, 6)]  # <1>
>>> start = matches[0][1]
>>> stop = matches[0][2]
>>> doc[start:stop]  # <2>
Desoto met the Pascagoula
----
<1> list of 3-tuples with span ID, start token index, stop token index
<2> SpaCy lets you slice a document object on token indices just as you would for a Python list

A spacy matcher will list the pattern matches as 3-tuples containing match ID integers, plus the start and stop token indices (positions) for each match.
So you extracted a match from the original sentence from which you created the pattern, but what about similar sentences from Wikipedia?

.Using a POS pattern matcher
[source,python]
----
>>> doc = nlp("October 24: Lewis and Clark met their" \
...     "first Mandan Chief, Big White.")
>>> m = matcher(doc)[0]
>>> m
(12280034159272152371, 3, 11)

>>> doc[m[1]:m[2]]
Lewis and Clark met their first Mandan Chief

>>> doc = nlp("On 11 October 1986, Gorbachev and Reagan met at Höfði house")
>>> matcher(doc)
[]  # <1>
----
<1> The pattern doesn't match any substrings of the sentence from Wikipedia.

You need to add a second pattern to allow for the verb to occur after the subject and object nouns.

.Combine patterns together to handle more variations
[source,python]
----
>>> doc = nlp(
...     "On 11 October 1986, Gorbachev and Reagan met at Hofoi house"
...     )
>>> pattern = [
...     {'TAG': 'NN', 'OP': '+'},
...     {'LEMMA': 'and'},
...     {'TAG': 'NN', 'OP': '+'},
...     {'IS_ALPHA': True, 'OP': '*'},
...     {'LEMMA': 'meet'}
...     ]
>>> matcher.add('met', None, pattern)  # <1>
>>> matches = matcher(doc)
>>> pd.DataFrame(matches, columns=)
[(1433..., 5, 9),
 (1433..., 5, 11),
 (1433..., 7, 11),
 (1433..., 5, 12)]  # <2>

>>> doc[m[-1][1]:m[-1][2]]  # <3>
Gorbachev and Reagan met at Hofoi house
----
<1> This adds an additional pattern without removing the previous pattern.
<2> The '+' operators increase the number of overlapping alternative matches.
<3> The longest match is the last one in the list of matches.

So now you have your entities and a relationship.
You can even build a pattern that is less restrictive about the verb in the middle ("met") and more restrictive about the names of the people and groups on either side.
Doing so might allow you to identify additional verbs that imply that one person or group has met another, such as the verb "knows" or even passive phrases such as "had a conversation" or "became acquainted with".
Then you could use these new verbs to add relationships for new proper nouns on either side.

But you can see how you're drifting away from the original meaning of your seed relationship patterns.
This is called semantic drift.
Fortunately for you, spaCy tags words in a parsed document with not only their POS and dependency tree information, but it also provides the Word2Vec word vector.
You can use this vector to prevent the connector verb and the proper nouns on either side from drifting too far away from the original meaning of your seed pattern.footnote:[This is the subject of active research: https://nlp.stanford.edu/pubs/structuredVS.pdf.]

=== Entity name normalization

The normalized representation of an entity is usually a string, even for numerical information such as dates.
The normalized ISO format for this date would be "1541-01-01".
A normalized representation for entities enables your knowledge base to connect all the different things that happened in the world on that same date to that same node (entity) in your graph.

You'd do the same for other named entities.
You'd correct the spelling of words and attempt to resolve ambiguities for names of objects, animals, people, places, and so on.
Normalizing named entities and resolving ambiguities is often called "coreference resolution" or "anaphora resolution", especially for pronouns or other "names" relying on context.
This is similar to lemmatization, which you learned about in Chapter 2.
Normalization of named entities ensures that spelling and naming variations don't pollute your vocabulary of entity names with confounding, redundant names.

For example "Desoto" might be expressed in a particular document in at least five different ways:

* "de Soto"
* "Hernando de Soto"
* "Hernando de Soto (c. 1496/1497–1542), Spanish conquistador"
* https://en.wikipedia.org/wiki/Hernando_de_Soto (a URI)
* A numerical ID for a database of famous and historical people

Similarly, your normalization algorithm can choose any of these forms.
A knowledge graph should normalize each kind of entity the same way, to prevent multiple distinct entities of the same type from sharing the same "name."
You don't want multiple person names referring to the same physical person.
Even more importantly, the normalization should be applied consistently -- both when you write new facts to the knowledge base or when you read or query the knowledge base.

If you decide to change the normalization approach after the database has been populated, the data for existing entities in the knowledge should be "migrated", or altered, to adhere to the new normalization scheme.
Schemaless databases (key-value stores), like the ones used to store knowledge graphs or knowledge bases, are not free from the migration responsibilities of relational databases.
After all, schemaless databases are interface wrappers for relational databases under the hood.

Your normalized entities also need "is-a" relationships to connect them to entity categories that define types or categories of entities.
These "is-a" relationships can be thought of as tags because each entity can have multiple "is-a" relationships.
Like names of people or POS tags, dates and other discrete numerical objects need to be normalized if you want to incorporate them into your knowledge base.

What about _relations_ between entities -- do they need to be stored in some normalized way?

=== Relation normalization and extraction

Now you need a way to normalize the relationships, to identify the kind of relationship between entities.
Doing so will allow you to find all birthday relationships between dates and people, or dates of occurrences of historical events, such as the encounter between "Hernando de Soto" and the "Pascagoula people."
And you need to write an algorithm to choose the right label for your relationship.

And these relationships can have a hierarchical name, such as "occurred-on/approximately" and "occurred-on/exactly", to allow you to find specific relationships or categories of relationships.
You can also label these relationships with a numerical property for the "confidence", probability, weight, or normalized frequency (analogous to TF-IDF for terms/words) of that relationship.
You can adjust these confidence values each time a fact extracted from a new text corroborates or contradicts an existing fact in the database.

Now you need a way to match patterns that can find these relationships.

=== Word patterns

Word patterns are just like regular expressions but for words instead of characters.
Instead of character classes, you have word classes.
For example, instead of matching a lowercase character, you might have a word pattern decision to match all the singular nouns ("NN" POS tag).footnote:[spaCy uses the "OntoNotes 5" POS tags: https://spacy.io/api/annotation#pos-tagging]
This is usually accomplished with machine learning.
Some seed sentences are tagged with some correct relationships (facts) extracted from those sentences.
A POS pattern can be used to find similar sentences where the subject and object words might change or even the relationship words.

You can use the spaCy package in two different ways to match these patterns in latexmath:[O(1)] (constant time) no matter how many patterns you want to match:

* PhraseMatcher for any word/tag sequence patterns footnote:[See the web page titled "Code Examples : spaCy Usage Documentation" (https://spacy.io/usage/examples#phrase-matcher).]
* Matcher for POS tag sequence patterns footnote:[See the web page titled "Matcher : spaCy API Documentation" (https://spacy.io/api/matcher).]

To ensure that the new relations found in new sentences are truly analogous to the original seed (example) relationships, you often need to constrain the subject, relation, and object word meanings to be similar to those in the seed sentences.
The best way to do this is with some vector representation of the meaning of words.
Does this ring a bell?
Word vectors, discussed in Chapter 4, are one of the most widely used word meaning representations for this purpose.
They help minimize semantic drift.

Using semantic vector representations for words and phrases has made automatic information extraction accurate enough to build large knowledge bases automatically.
But human supervision and curation is required to resolve much of the ambiguity in natural language text.
You explored a bit of the NELL knowledge graph at the beginning of this chapter and it looked pretty accurate.
Now you're going to see how they accomplished that.

// FIXME: move sentence segmentation to the beginning of the discussion of information extraction.
// FIXME: show how to use it on this adoc (respecting separators such as headings and empty lines).

=== Turning sentences into knowledge

==== AI ethics vs AI safety

In the previous chapter, you learned a lot about the harm that AI and large language models are causing.
And hopefully, you've come up with your own ideas for how to help mitigate those harms.
Engineers who design, build and use autonomous algorithms are starting to pay attention to the harm caused by these algorithms and how they are used.
How to use algorithms ethically, by minimizing harm is called _AI ethics_.
And algorithms that minimize or mitigate much of these harms are often referred to as ethical AI.

And you may have also heard about the _AI control problem_ or _AI safety_ and may be confused about how it is different from AI ethics.
AI safety is about how we can avoid being exterminated, intentionally or unintentionally, by our future "robot overlords."
People working on AI safety are trying to mitigate the long-term existential risk posed by superintelligent generally intelligent machines.
The CEOs of many of the largest AI companies have publicly announced their concern about this problem:

[quote, Center for AI Safety]
____
Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.
____

This single sentence is so important to AI companies' businesses that more than a 100 senior managers at AI companies signed this open letter.
Nonetheless, many of these same companies are not allocating significant resources or time or public outreach to address this concern.
Many of the largest are not even willing to sign this vague noncommital statement.
Open AI, Microsoft, and Anthropic signed this letter, but Apple, Tesla, Facebook, Alphabet (Google), Amazon and many other AI goliaths did not.

And there's an ongoing public debate about the urgency and priority of _AI safety_ vs _AI ethics_.
Some thought leaders such as Yuval Harari and Yoshua Bengio are focused entirely on AI safety -- restraining or controlling a hypothetical superintelligent AGI.
Other, less well-known thought leaders are focusing their time and energy on the more immediate harm that algorithms and AI are causing now - in other words, AI ethics.
Disadvantaged people are especially vulnerable to the unethical use of AI.
When companies monetize their users' data they extract power and wealth from those who can least afford the loss.
When technology is used to create and maintain monopolies those monopolies extinguish competition from small businesses, government programs, nonprofits, and individuals supporting the disadvantaged.footnote:[from _Chokepoint Capitalism_ by Cory Efram Doctorow]

So which one of these pressing topics are you concerned with?
Are there some overlapping things that you can work on to both reduce the harm to humans now and prevent our extinction in the long run?
Perhaps _explainable AI_ should be at the top of your list of ways to help create "ethical and safe AI."
Explainable AI is the concept of an algorithm that can explain how and why it makes decisions, especially when those decisions are mistaken or harmful.
The information extraction and knowledge graph concepts that you learned in this chapter are some of the foundational tools for building explainable AI.
And explainable, grounded AI is less likely to propagate misinformation by generating factually incorrect statements or arguments.
And if you can find algorithms that help explain how an ML algorithm is making its harmful predictions and decisions you can use that understanding to prevent that harm.

Another area to think about is the efficiency and simplicity of your AI algorithms and NLP pipelines.
By necessity, open source LLMs are often much smaller, faster, and more power efficient.
This dramatically reduces the environmental impact of your NLP pipelines, and it democratizes access to AI.
And there's an underappreciated synergistic benefit of more efficient models, they are less overfit, more general, and more correct.
Using Occam's razor to smartly prune your neural networks will make those networks more generally intelligent, with fewer biases and mistakes.footnote:["Simon Says: Evaluating and Mitigating Bias in Pruned Neural Networks with Knowledge Distillation" 2021 by Cody Blakeney (https://arxiv.org/pdf/2106.07849.pdf)]


=== Summary

* A knowledge graph can be built to store relationships between entities.
* Regular expressions are a mini-programming language that can isolate and extract information.
* Part-of-speech tagging allows you to extract relationships between entities mentioned in a sentence.
* Segmenting sentences requires more than just splitting on periods and exclamation marks.
