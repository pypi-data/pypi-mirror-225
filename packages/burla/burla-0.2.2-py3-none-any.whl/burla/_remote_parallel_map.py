import os
import sys
import requests
import warnings
import pickle
import math
from six import reraise
from threading import Thread, Event
from typing import Callable, Optional
from time import sleep, time
from queue import PriorityQueue

import dill
from tblib import pickling_support

from burla._logstream import print_logs_from_queue
from burla._config import load_api_key_from_local_config
from burla._env_inspection import get_pip_packages
from burla._helpers import nopath_warning

pickling_support.install()
warnings.formatwarning = nopath_warning

BURLA_SERVICE_URL = "https://burla-webservice-0-2-1-zqhes3whbq-uc.a.run.app"
JOB_ENV_REPO = "us-docker.pkg.dev/burla-prod/burla-job-environments"

MAX_CONCURRENCY = 1000
JOB_STATUS_POLL_RATE_SEC = 6  # how often to check for job completion
TIMEOUT_MIN = 60 * 12  # max time a Burla job can run for


class JobTimeoutError(Exception):
    def __init__(self, job_id, timeout):
        super().__init__(f"Burla job with id: '{job_id}' timed out after {timeout} seconds.")


class InstallError(Exception):
    def __init__(self, stdout: str):
        super().__init__(
            f"The following error occurred attempting to pip install packages:\n{stdout}"
        )


class ServerError(Exception):
    def __init__(self):
        super().__init__(
            (
                "An unknown error occurred in Burla's cloud, this is not an error with your code. "
                "Someone has been notified, please try again later."
            )
        )


def _get_job_info(job_id: str, epoch: int, headers: dict, attempt=0):
    try:
        response = requests.get(f"{BURLA_SERVICE_URL}/v1/jobs/{job_id}/{epoch}", headers=headers)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.HTTPError:
        if str(response.status_code).startswith("5") and attempt != 3:
            sleep(2)
            return _get_job_info(job_id, epoch, headers, attempt=attempt + 1)
        else:
            raise ServerError()


def remote_parallel_map(
    function_: Callable,
    inputs: list,
    image: Optional[str] = None,
    api_key: Optional[str] = None,
):
    n_batches = math.ceil(len(inputs) / MAX_CONCURRENCY)
    if len(inputs) > MAX_CONCURRENCY:
        warnings.warn(
            (
                f"Because the current maximum concurrency is {MAX_CONCURRENCY} and you submitted "
                f"{len(inputs)} inputs, these inputs will be processed in {n_batches} separate "
                "batches. We are working hard to increase this concurrency limit."
            )
        )
    outputs = []
    input_batches = [
        inputs[i : i + MAX_CONCURRENCY] for i in range(0, len(inputs), MAX_CONCURRENCY)
    ]
    for input_batch in input_batches:
        outputs.extend(_remote_parallel_map_single_batch(function_, input_batch, image, api_key))

    all_outputs_are_none = all(item is None for item in outputs)
    if not all_outputs_are_none:
        return outputs


def _remote_parallel_map_single_batch(
    function_: Callable,
    inputs: list,
    image: Optional[str] = None,
    api_key: Optional[str] = None,
):
    # https://www.rfc-editor.org/rfc/rfc7235 <- specifies "correct" api key location
    headers = {"Authorization": f"Bearer {api_key or load_api_key_from_local_config()}"}

    response = requests.post(f"{BURLA_SERVICE_URL}/v1/jobs/", json={}, headers=headers)
    response.raise_for_status()
    job_id = response.json()["job_id"]

    python_version = f"3.8" if sys.version_info.minor < 8 else f"3.{sys.version_info.minor}"

    payload = {
        "function_pkl_hex": dill.dumps(function_, recurse=True).hex(),
        "inputs": [dill.dumps(input_).hex() for input_ in inputs],
        "image": image or f"{JOB_ENV_REPO}/python{python_version}/burla_job_env:latest",
        "packages": None if image else list(get_pip_packages()),
        "in_colab": os.getenv("COLAB_RELEASE_TAG") is not None,
    }

    job_started_at_epoch = int(time())
    response = requests.post(f"{BURLA_SERVICE_URL}/v1/jobs/{job_id}", json=payload, headers=headers)
    response.raise_for_status()

    last_epoch = job_started_at_epoch
    epoch = last_epoch
    job_is_running = True
    job_timed_out = False

    # Start printing logs generated by this job using a separate thread.
    print_queue = PriorityQueue()
    stop_event = Event()
    log_thread = Thread(target=print_logs_from_queue, args=(print_queue, stop_event))
    log_thread.start()

    # loop until job finishes, or times out
    while job_is_running and (not job_timed_out):
        # TODO: record and subtract time so this loop always takes exactly JOB_STATUS_POLL_RATE_SEC
        # sec to run? This might fix late logs?
        sleep(JOB_STATUS_POLL_RATE_SEC)

        job = _get_job_info(job_id, last_epoch, headers)

        # add all logs to print queue
        for epoch, log_message in job["logs"]:
            print_queue.put((epoch, log_message))

        last_epoch = epoch
        udf_error = job.get("udf_error")
        install_error = job.get("install_error")

        job_is_running = job["job_is_done"] == False
        job_timed_out = (time() - job_started_at_epoch) > (TIMEOUT_MIN * 60)

    stop_event.set()
    log_thread.join()

    if udf_error:
        error = pickle.loads(bytes.fromhex(job.get("udf_error")))
        reraise(*error)
    elif install_error:
        raise InstallError(install_error)
    elif job_timed_out:
        raise JobTimeoutError(job_id=job_id, timeout=TIMEOUT_MIN)

    outputs = [dill.loads(bytes.fromhex(output)) for output in job["return_values"]]
    return outputs
