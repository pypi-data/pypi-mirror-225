{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG Viewer\n",
    "![status](https://img.shields.io/badge/status-in%20progress-orange)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./assets/230524_eeg-viewer.png\" alt=\"eeg viewer preview\" width=\"450\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This workflow is intended to demonstrate the visualization of a set of 1D EEG timeseries with HoloViz and Bokeh tools.\n",
    "\n",
    "For details specific to this workflow, such as goals, specifications, and bottlenecks, please see this workflow's [readme](./readme_eeg-viewer.md).\n",
    "\n",
    "For a summary of EEG research, data, and software, please see [neuro/wiki/EEG-notes](https://github.com/holoviz-topics/neuro/wiki/EEG-notes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition alert alert-info\">\n",
    "    <p class=\"admonition-title\" style=\"font-weight:bold\">Requirements</p>\n",
    "    <p>This workflow notebook requires the <a href=\"./environment.yml\">environment</a> specified in this workflow directory.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import colorcet as cc\n",
    "import holoviews as hv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "hv.extension(\"bokeh\")\n",
    "import panel as pn\n",
    "from bokeh.models import HoverTool\n",
    "from holoviews import Dataset\n",
    "from holoviews.operation.datashader import rasterize\n",
    "from holoviews.plotting.links import RangeToolLink\n",
    "from hvneuro import download_file\n",
    "from neurodatagen.eeg import generate_eeg_powerlaw\n",
    "from scipy.stats import zscore\n",
    "\n",
    "pn.extension(template=\"material\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic data pipeline\n",
    "\n",
    "The `generate_eeg_powerlaw` function synthesizes EEG data as high-pass filtered pink noise power law time series by default. The function returns a 2D numpy array of synthetic EEG data (in microvolts) shaped as (number of channels, total samples), a 1D time array (in seconds), and a list of channel names. Parameters such as the high-pass filter factor (in Hz) and an amplitude scaling factor allow customization of the generated data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channels = 25\n",
    "n_seconds = 30\n",
    "fs = 512\n",
    "\n",
    "data, time, channels = generate_eeg_powerlaw(n_channels, n_seconds, fs)\n",
    "\n",
    "print(f\"shape: {data.shape} (n_channels, samples) \")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ch_disp = 10  # max channels to initially display\n",
    "max_t_disp = 3  # max time in seconds to initially display\n",
    "\n",
    "spacing = 5.5  # Spacing between channels\n",
    "offset = np.std(data) * spacing\n",
    "\n",
    "annotation = hv.VSpan(1, 2)  # example annotation (start, end) time\n",
    "\n",
    "# Create a hv.Curve element per chan\n",
    "channel_curves = []\n",
    "max_data = data.max()\n",
    "\n",
    "hover = HoverTool(\n",
    "    tooltips=[\n",
    "        (\"Channel\", \"@channel\"),\n",
    "        (\"Time\", \"$x s\"),\n",
    "        (\"Amplitude\", \"@original_amplitude ÂµV\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "for i, channel_data in enumerate(data):\n",
    "    offset_data = channel_data + (i * offset)\n",
    "    max_data = max(offset_data.max(), max_data)  # update max\n",
    "    ds = Dataset(\n",
    "        (time, offset_data, channel_data, channels[i]),\n",
    "        [\"Time\", \"Amplitude\", \"original_amplitude\", \"channel\"],\n",
    "    )\n",
    "    channel_curves.append(\n",
    "        hv.Curve(ds, \"Time\", [\"Amplitude\", \"original_amplitude\", \"channel\"]).opts(\n",
    "            color=\"black\", line_width=1, tools=[hover, \"xwheel_zoom\"], shared_axes=False\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Create mapping from yaxis location to ytick for each channel\n",
    "# so we can have categorical-style labeling on a continuous axis.\n",
    "# Note: this would/should change when we implement independent\n",
    "# coordinates.\n",
    "yticks = [(i * offset, ich) for i, ich in enumerate(channels)]\n",
    "\n",
    "# Create an overlay of curves\n",
    "# TODO.. setting x/y_range bounds does not yet restrict the RangeTool from going beyond these limits\n",
    "# TODO.. the zoom out will stop when it hits any single bound, and not continue zooming out in other directions/dims\n",
    "eeg_viewer = (annotation * hv.Overlay(channel_curves, kdims=\"Channel\")).opts(\n",
    "    padding=0,\n",
    "    xlabel=\"Time (s)\",\n",
    "    ylabel=\"Channel\",  # default_tools=['hover', 'pan', 'box_zoom', 'save', 'reset'],\n",
    "    yticks=yticks,\n",
    "    show_legend=False,\n",
    "    aspect=1.5,\n",
    "    responsive=True,\n",
    "    shared_axes=False,\n",
    "    backend_opts={\n",
    "        \"x_range.bounds\": (time.min(), time.max()),\n",
    "        \"y_range.bounds\": (data.min(), max_data),\n",
    "    },\n",
    ")\n",
    "\n",
    "# Get the y positions of the yticks to use as yaxis of minimap image\n",
    "y_positions, _ = zip(*yticks)\n",
    "\n",
    "# Compute z-scores across time for each channel\n",
    "z_data = zscore(data, axis=1)\n",
    "\n",
    "# Generate the zscored image for the minimap using the y tiack positions from the eeg_viewer\n",
    "minimap = rasterize(\n",
    "    hv.Image((time, y_positions, z_data), [\"Time (s)\", \"Channel\"], \"Amplitude (uV)\")\n",
    ")\n",
    "\n",
    "# Style the minimap\n",
    "clim_mul = 1.2\n",
    "minimap = minimap.opts(\n",
    "    cmap=\"RdBu_r\",\n",
    "    colorbar=False,\n",
    "    xlabel=\"\",\n",
    "    alpha=0.5,\n",
    "    yticks=[yticks[0], yticks[-1]],\n",
    "    height=100,\n",
    "    responsive=True,\n",
    "    default_tools=[\"\"],\n",
    "    shared_axes=False,\n",
    "    clim=(-z_data.std() * clim_mul, z_data.std() * clim_mul),\n",
    ")\n",
    "\n",
    "# Create RangeToolLink between the minimap and the main EEG viewer\n",
    "# (quirk: apply to just one eeg trace and it will apply to all. see HoloViews #4472)\n",
    "max_y_disp = np.max(data[max_ch_disp - 1, :] + ((max_ch_disp - 1) * offset))\n",
    "RangeToolLink(\n",
    "    minimap,\n",
    "    next(iter(eeg_viewer.values())),\n",
    "    axes=[\"x\", \"y\"],\n",
    "    boundsx=(None, max_t_disp),\n",
    "    boundsy=(None, max_y_disp),\n",
    ")\n",
    "\n",
    "\n",
    "# layout = (eeg_viewer + minimap).cols(1).opts(shared_axes=False, merge_tools=False)\n",
    "# eeg_app = pn.Row(layout).servable() # too much spacing between plots in served app\n",
    "# eeg_app = pn.Column(pn.Row(eeg_viewer, min_height=500, sizing_mode='stretch_both'), minimap, sizing_mode='stretch_both')#.servable()#target='main') # BUG Panel #5315: rangetool is variably active in the bokeh toolbar on eeg viewer plot.. not respecting shared_axes=False\n",
    "\n",
    "# reverting approach because of the rangetool bug.. will deal with the spacing in the served app later\n",
    "eeg_app = pn.Column(\n",
    "    (eeg_viewer + minimap * annotation).cols(1), min_height=650\n",
    ").servable(target=\"main\", title=\"EEG Viewer with HoloViz and Bokeh\")\n",
    "eeg_app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataset is 2.6 MB on disk\n",
    "\n",
    "url = \"https://physionet.org/files/eegmmidb/1.0.0/S001/S001R04.edf?download\"\n",
    "local_data_path = \"../../data/\"\n",
    "\n",
    "# Will not download if already present at local_data_path\n",
    "local_file_path = download_file(url, local_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = mne.io.read_raw_edf(local_file_path, preload=True)\n",
    "raw.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview the channel names, types, signal ranges, and uncompressed size\n",
    "\n",
    "raw.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather the annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get initial time of experiment\n",
    "orig_time = raw.annotations.orig_time.replace(tzinfo=None)\n",
    "\n",
    "# get annotations into pd df\n",
    "annotations_df = raw.annotations.to_data_frame()\n",
    "\n",
    "annotations_df[\"start_time\"] = annotations_df[\"onset\"]\n",
    "annotations_df[\"end_time\"] = annotations_df[\"start_time\"] + pd.to_timedelta(\n",
    "    annotations_df[\"duration\"], \"s\"\n",
    ")\n",
    "annotations_df = annotations_df.drop([\"onset\", \"duration\"], axis=1)\n",
    "\n",
    "# Should be datetime object in final\n",
    "annotations_df[\"start_time\"] = (\n",
    "    annotations_df[\"start_time\"] - orig_time\n",
    ").dt.total_seconds()\n",
    "annotations_df[\"end_time\"] = (annotations_df[\"end_time\"] - orig_time).dt.total_seconds()\n",
    "\n",
    "\n",
    "unique_descriptions = annotations_df[\"description\"].unique()\n",
    "color_map = dict(zip(unique_descriptions, cc.glasbey))\n",
    "\n",
    "annotations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(\n",
    "    [\n",
    "        raw.annotations.to_data_frame(),\n",
    "        raw.annotations.to_data_frame(),\n",
    "        raw.annotations.to_data_frame(),\n",
    "    ],\n",
    "    axis=0,\n",
    ")\n",
    "%time [_ for _ in df.itertuples()];\n",
    "%time [_ for _ in df.iterrows()];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from holonote.annotate import Annotator\n",
    "\n",
    "# Want to add more automatic way to set this up.\n",
    "# annotator = Annotator(spec={\"time\": np.datetime64}, fields=[\"description\"])\n",
    "annotator = Annotator(spec={\"time\": float}, fields=[\"description\"])\n",
    "\n",
    "if annotator.df.empty:\n",
    "    # Only run once!\n",
    "    for n in annotations_df.itertuples():\n",
    "        annotator.set_range(n.start_time, n.end_time)\n",
    "        annotator.add_annotation(description=n.description)\n",
    "    annotator.commit()\n",
    "\n",
    "\n",
    "range_style = {\n",
    "    \"color\": hv.dim(\"description\").categorize(color_map),\n",
    "    \"show_legend\": False,\n",
    "}\n",
    "range_style = {\"show_legend\": False}\n",
    "annotations_overlay = annotator.overlay(\n",
    "    hv.Curve([]), range_style=range_style\n",
    ")  # My hack in using an empty Curve.\n",
    "annotations_overlay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean channel names, set sensor positions, and reference data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up the channel names\n",
    "raw.rename_channels(lambda s: s.strip(\".\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preview available montages that are shipped with MNE\n",
    "# mne.channels.get_builtin_montages(descriptions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's use the standard 10-20\n",
    "# montage = mne.channels.make_standard_montage(\"standard_1020\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the assigned positions of our data channels\n",
    "# raw.set_montage(montage, match_case=False)\n",
    "# sphere=(0, 0.015, 0, 0.099) #manually adjust the y origin coord and radius a bit\n",
    "# raw.plot_sensors(show_names=True, sphere=sphere);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-reference EEG data to the average over all recording channels\n",
    "raw.set_eeg_reference(\"average\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather the data for plotting into simple arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = raw.times\n",
    "channels = raw.ch_names\n",
    "\n",
    "# get the EEG data (for this data set, all channels are EEG anyways)\n",
    "eeg_indices = mne.pick_types(raw.info, eeg=True)\n",
    "data = raw.get_data(picks=eeg_indices, units={\"eeg\": \"uV\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from holoviews.operation.downsample import downsample1d\n",
    "\n",
    "max_ch_disp = 10  # max channels to initially display\n",
    "max_t_disp = 5  # max time in seconds to initially display\n",
    "\n",
    "spacing = 2.5  # Spacing between channels\n",
    "offset = np.std(data) * spacing\n",
    "\n",
    "# Create an overlay of VSpan annotations based on the annotations dataframe\n",
    "# annotations_overlay = annotator.overlay(hv.Curve([]))\n",
    "\n",
    "# Create a hv.Curve element per chan\n",
    "channel_curves = []\n",
    "max_data = data.max()\n",
    "\n",
    "hover = HoverTool(\n",
    "    tooltips=[\n",
    "        (\"Channel\", \"@channel\"),\n",
    "        (\"Time\", \"$x s\"),\n",
    "        (\"Amplitude\", \"@original_amplitude ÂµV\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "for i, channel_data in enumerate(data):\n",
    "    offset_data = channel_data + (i * offset)\n",
    "    max_data = max(offset_data.max(), max_data)  # update max\n",
    "    ds = Dataset(\n",
    "        (time, offset_data, channel_data, channels[i]),\n",
    "        [\"Time\", \"Amplitude\", \"original_amplitude\", \"channel\"],\n",
    "    )\n",
    "    channel_curves.append(\n",
    "        downsample1d(\n",
    "            hv.Curve(ds, \"Time\", [\"Amplitude\", \"original_amplitude\", \"channel\"]).opts(\n",
    "                color=\"black\",\n",
    "                line_width=1,\n",
    "                tools=[hover, \"xwheel_zoom\"],\n",
    "                shared_axes=False,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "yticks = [(i * offset, ich) for i, ich in enumerate(channels)]\n",
    "\n",
    "# Create an overlay of curves\n",
    "eeg_viewer = hv.Overlay(channel_curves, kdims=\"Channel\").collate()\n",
    "eeg_with_annotations = (annotations_overlay * eeg_viewer).opts(\n",
    "    padding=0,\n",
    "    xlabel=\"Time (s)\",\n",
    "    ylabel=\"Channel\",\n",
    "    yticks=yticks,\n",
    "    show_legend=False,\n",
    "    aspect=1.5,\n",
    "    responsive=True,\n",
    "    shared_axes=False,\n",
    "    backend_opts={\n",
    "        \"x_range.bounds\": (time.min(), time.max()),\n",
    "        \"y_range.bounds\": (data.min(), max_data),\n",
    "    },\n",
    ")\n",
    "\n",
    "# Get the y positions of the yticks to use as yaxis of minimap image\n",
    "y_positions, _ = zip(*yticks)\n",
    "\n",
    "# Compute z-scores across time for each channel\n",
    "z_data = zscore(data, axis=1)\n",
    "\n",
    "# Generate the zscored image for the minimap using the y tiack positions from the eeg_viewer\n",
    "minimap = rasterize(\n",
    "    hv.Image((time, y_positions, z_data), [\"Time (s)\", \"Channel\"], \"Amplitude (uV)\")\n",
    ")\n",
    "\n",
    "# Style the minimap\n",
    "clim_mul = 1.2\n",
    "minimap = minimap.opts(\n",
    "    cmap=\"RdBu_r\",\n",
    "    colorbar=False,\n",
    "    xlabel=\"\",\n",
    "    alpha=0.5,\n",
    "    yticks=[yticks[0], yticks[-1]],\n",
    "    height=100,\n",
    "    responsive=True,\n",
    "    default_tools=[\"\"],\n",
    "    shared_axes=False,\n",
    "    clim=(-z_data.std() * clim_mul, z_data.std() * clim_mul),\n",
    ")\n",
    "\n",
    "# Create RangeToolLink between the minimap and the main EEG viewer\n",
    "max_y_disp = np.max(data[max_ch_disp - 1, :] + ((max_ch_disp - 1) * offset))\n",
    "RangeToolLink(\n",
    "    minimap,\n",
    "    eeg_viewer,\n",
    "    axes=[\"x\", \"y\"],\n",
    "    boundsx=(None, max_t_disp),\n",
    "    boundsy=(None, max_y_disp),\n",
    ")\n",
    "\n",
    "\n",
    "eeg_app = pn.Column(\n",
    "    (eeg_with_annotations + minimap * annotations_overlay).cols(1), min_height=650\n",
    ").servable(target=\"main\", title=\"EEG Viewer with HoloViz and Bokeh\")\n",
    "eeg_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_curves = []\n",
    "for i, channel_data in enumerate(data):\n",
    "    offset_data = channel_data + (i * offset)\n",
    "    max_data = max(offset_data.max(), max_data)  # update max\n",
    "    ds = Dataset(\n",
    "        (\n",
    "            time,\n",
    "            np.ascontiguousarray(offset_data),\n",
    "            np.ascontiguousarray(channel_data),\n",
    "            channels[i],\n",
    "        ),\n",
    "        [\"Time\", \"Amplitude\", \"original_amplitude\", \"channel\"],\n",
    "    )\n",
    "    channel_curves.append(\n",
    "        hv.Curve((time, offset_data), \"Time\", \"Amplitude\").opts(\n",
    "            color=\"black\", line_width=1, shared_axes=False\n",
    "        )\n",
    "    )\n",
    "    # if i == 10:\n",
    "    #    break\n",
    "\n",
    "# yticks = [(i * offset, ich) for i, ich in enumerate(channels)]\n",
    "\n",
    "# Create an overlay of curves\n",
    "# hv.Overlay(channel_curves, kdims=\"Channel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ascontiguousarray(offset_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# channels\n",
    "a = hv.Overlay([downsample1d(c, algorithm=\"nth\") for c in channel_curves]).collate()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(channels), len(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "data2 = data + (np.arange(len(data))[:, np.newaxis] * offset)\n",
    "max_data = data2.max()\n",
    "xds = xr.Dataset(\n",
    "    coords={\"channel\": channels, \"Time\": time},\n",
    "    data_vars={\n",
    "        \"Amplitude\": ((\"channel\", \"Time\"), data2),\n",
    "        # \"original_amplitude\": ((\"channel\", \"Time\"), data),\n",
    "    },\n",
    ")\n",
    "\n",
    "annotations_overlay = hv.Dataset(xds).to(hv.Curve, groupby=\"channel\", vdims=\"Amplitude\").overlay().opts(\n",
    "    hv.opts.Curve(color=\"black\", line_width=1, tools=[hover, \"xwheel_zoom\"])\n",
    ")\n",
    "annotations_overlay.data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = hv.Overlay(channel_curves, kdims=\"Channel\")\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?downsample1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# channel_data\n",
    "for _i, channel_data in enumerate(data):\n",
    "    print(channel_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator.add_annotation(description=\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator.df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
