{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.25,
      "acc_stderr": 0.04351941398892446,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.04351941398892446
    },
    "hendrycksTest-anatomy": {
      "acc": 0.21481481481481482,
      "acc_stderr": 0.03547854198560823,
      "acc_norm": 0.21481481481481482,
      "acc_norm_stderr": 0.03547854198560823
    },
    "hendrycksTest-astronomy": {
      "acc": 0.25,
      "acc_stderr": 0.03523807393012047,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.03523807393012047
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.21,
      "acc_stderr": 0.040936018074033256,
      "acc_norm": 0.21,
      "acc_norm_stderr": 0.040936018074033256
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.3018867924528302,
      "acc_stderr": 0.02825420034443866,
      "acc_norm": 0.3018867924528302,
      "acc_norm_stderr": 0.02825420034443866
    },
    "hendrycksTest-college_biology": {
      "acc": 0.2569444444444444,
      "acc_stderr": 0.03653946969442099,
      "acc_norm": 0.2569444444444444,
      "acc_norm_stderr": 0.03653946969442099
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.23,
      "acc_stderr": 0.042295258468165044,
      "acc_norm": 0.23,
      "acc_norm_stderr": 0.042295258468165044
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.27,
      "acc_stderr": 0.044619604333847394,
      "acc_norm": 0.27,
      "acc_norm_stderr": 0.044619604333847394
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.27,
      "acc_stderr": 0.044619604333847394,
      "acc_norm": 0.27,
      "acc_norm_stderr": 0.044619604333847394
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.23699421965317918,
      "acc_stderr": 0.03242414757483099,
      "acc_norm": 0.23699421965317918,
      "acc_norm_stderr": 0.03242414757483099
    },
    "hendrycksTest-college_physics": {
      "acc": 0.21568627450980393,
      "acc_stderr": 0.04092563958237656,
      "acc_norm": 0.21568627450980393,
      "acc_norm_stderr": 0.04092563958237656
    },
    "hendrycksTest-computer_security": {
      "acc": 0.27,
      "acc_stderr": 0.04461960433384739,
      "acc_norm": 0.27,
      "acc_norm_stderr": 0.04461960433384739
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.2936170212765957,
      "acc_stderr": 0.02977164271249123,
      "acc_norm": 0.2936170212765957,
      "acc_norm_stderr": 0.02977164271249123
    },
    "hendrycksTest-econometrics": {
      "acc": 0.2543859649122807,
      "acc_stderr": 0.040969851398436716,
      "acc_norm": 0.2543859649122807,
      "acc_norm_stderr": 0.040969851398436716
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.296551724137931,
      "acc_stderr": 0.03806142687309994,
      "acc_norm": 0.296551724137931,
      "acc_norm_stderr": 0.03806142687309994
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.24074074074074073,
      "acc_stderr": 0.022019080012217897,
      "acc_norm": 0.24074074074074073,
      "acc_norm_stderr": 0.022019080012217897
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.23015873015873015,
      "acc_stderr": 0.03764950879790607,
      "acc_norm": 0.23015873015873015,
      "acc_norm_stderr": 0.03764950879790607
    },
    "hendrycksTest-global_facts": {
      "acc": 0.31,
      "acc_stderr": 0.04648231987117316,
      "acc_norm": 0.31,
      "acc_norm_stderr": 0.04648231987117316
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.24838709677419354,
      "acc_stderr": 0.024580028921481003,
      "acc_norm": 0.24838709677419354,
      "acc_norm_stderr": 0.024580028921481003
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.24630541871921183,
      "acc_stderr": 0.03031509928561773,
      "acc_norm": 0.24630541871921183,
      "acc_norm_stderr": 0.03031509928561773
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.29,
      "acc_stderr": 0.045604802157206845,
      "acc_norm": 0.29,
      "acc_norm_stderr": 0.045604802157206845
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.23636363636363636,
      "acc_stderr": 0.033175059300091805,
      "acc_norm": 0.23636363636363636,
      "acc_norm_stderr": 0.033175059300091805
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.18686868686868688,
      "acc_stderr": 0.027772533334218967,
      "acc_norm": 0.18686868686868688,
      "acc_norm_stderr": 0.027772533334218967
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.25906735751295334,
      "acc_stderr": 0.031618779179354115,
      "acc_norm": 0.25906735751295334,
      "acc_norm_stderr": 0.031618779179354115
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.24102564102564103,
      "acc_stderr": 0.021685546665333195,
      "acc_norm": 0.24102564102564103,
      "acc_norm_stderr": 0.021685546665333195
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.27037037037037037,
      "acc_stderr": 0.027080372815145675,
      "acc_norm": 0.27037037037037037,
      "acc_norm_stderr": 0.027080372815145675
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.24789915966386555,
      "acc_stderr": 0.028047967224176892,
      "acc_norm": 0.24789915966386555,
      "acc_norm_stderr": 0.028047967224176892
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.2980132450331126,
      "acc_stderr": 0.037345356767871984,
      "acc_norm": 0.2980132450331126,
      "acc_norm_stderr": 0.037345356767871984
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.23119266055045873,
      "acc_stderr": 0.01807575024163315,
      "acc_norm": 0.23119266055045873,
      "acc_norm_stderr": 0.01807575024163315
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.14814814814814814,
      "acc_stderr": 0.024227629273728356,
      "acc_norm": 0.14814814814814814,
      "acc_norm_stderr": 0.024227629273728356
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.29901960784313725,
      "acc_stderr": 0.03213325717373615,
      "acc_norm": 0.29901960784313725,
      "acc_norm_stderr": 0.03213325717373615
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.27848101265822783,
      "acc_stderr": 0.029178682304842548,
      "acc_norm": 0.27848101265822783,
      "acc_norm_stderr": 0.029178682304842548
    },
    "hendrycksTest-human_aging": {
      "acc": 0.4663677130044843,
      "acc_stderr": 0.033481800170603065,
      "acc_norm": 0.4663677130044843,
      "acc_norm_stderr": 0.033481800170603065
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.2900763358778626,
      "acc_stderr": 0.03980066246467765,
      "acc_norm": 0.2900763358778626,
      "acc_norm_stderr": 0.03980066246467765
    },
    "hendrycksTest-international_law": {
      "acc": 0.24793388429752067,
      "acc_stderr": 0.039418975265163025,
      "acc_norm": 0.24793388429752067,
      "acc_norm_stderr": 0.039418975265163025
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.32407407407407407,
      "acc_stderr": 0.045245960070300476,
      "acc_norm": 0.32407407407407407,
      "acc_norm_stderr": 0.045245960070300476
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.25766871165644173,
      "acc_stderr": 0.03436150827846917,
      "acc_norm": 0.25766871165644173,
      "acc_norm_stderr": 0.03436150827846917
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.35714285714285715,
      "acc_stderr": 0.04547960999764376,
      "acc_norm": 0.35714285714285715,
      "acc_norm_stderr": 0.04547960999764376
    },
    "hendrycksTest-management": {
      "acc": 0.2621359223300971,
      "acc_stderr": 0.04354631077260597,
      "acc_norm": 0.2621359223300971,
      "acc_norm_stderr": 0.04354631077260597
    },
    "hendrycksTest-marketing": {
      "acc": 0.31196581196581197,
      "acc_stderr": 0.030351527323344948,
      "acc_norm": 0.31196581196581197,
      "acc_norm_stderr": 0.030351527323344948
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.28,
      "acc_stderr": 0.04512608598542127,
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.04512608598542127
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.30268199233716475,
      "acc_stderr": 0.016428781581749367,
      "acc_norm": 0.30268199233716475,
      "acc_norm_stderr": 0.016428781581749367
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.2947976878612717,
      "acc_stderr": 0.024547617794803838,
      "acc_norm": 0.2947976878612717,
      "acc_norm_stderr": 0.024547617794803838
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.24134078212290502,
      "acc_stderr": 0.014310999547961447,
      "acc_norm": 0.24134078212290502,
      "acc_norm_stderr": 0.014310999547961447
    },
    "hendrycksTest-nutrition": {
      "acc": 0.28104575163398693,
      "acc_stderr": 0.025738854797818716,
      "acc_norm": 0.28104575163398693,
      "acc_norm_stderr": 0.025738854797818716
    },
    "hendrycksTest-philosophy": {
      "acc": 0.3054662379421222,
      "acc_stderr": 0.026160584450140474,
      "acc_norm": 0.3054662379421222,
      "acc_norm_stderr": 0.026160584450140474
    },
    "hendrycksTest-prehistory": {
      "acc": 0.31790123456790126,
      "acc_stderr": 0.025910063528240868,
      "acc_norm": 0.31790123456790126,
      "acc_norm_stderr": 0.025910063528240868
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.3120567375886525,
      "acc_stderr": 0.027640120545169927,
      "acc_norm": 0.3120567375886525,
      "acc_norm_stderr": 0.027640120545169927
    },
    "hendrycksTest-professional_law": {
      "acc": 0.24771838331160365,
      "acc_stderr": 0.01102549929144374,
      "acc_norm": 0.24771838331160365,
      "acc_norm_stderr": 0.01102549929144374
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.2647058823529412,
      "acc_stderr": 0.02679956202488769,
      "acc_norm": 0.2647058823529412,
      "acc_norm_stderr": 0.02679956202488769
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.2647058823529412,
      "acc_stderr": 0.017848089574913226,
      "acc_norm": 0.2647058823529412,
      "acc_norm_stderr": 0.017848089574913226
    },
    "hendrycksTest-public_relations": {
      "acc": 0.35454545454545455,
      "acc_stderr": 0.045820048415054174,
      "acc_norm": 0.35454545454545455,
      "acc_norm_stderr": 0.045820048415054174
    },
    "hendrycksTest-security_studies": {
      "acc": 0.2693877551020408,
      "acc_stderr": 0.02840125202902294,
      "acc_norm": 0.2693877551020408,
      "acc_norm_stderr": 0.02840125202902294
    },
    "hendrycksTest-sociology": {
      "acc": 0.3482587064676617,
      "acc_stderr": 0.03368787466115459,
      "acc_norm": 0.3482587064676617,
      "acc_norm_stderr": 0.03368787466115459
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.4,
      "acc_stderr": 0.049236596391733084,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.049236596391733084
    },
    "hendrycksTest-virology": {
      "acc": 0.3674698795180723,
      "acc_stderr": 0.03753267402120575,
      "acc_norm": 0.3674698795180723,
      "acc_norm_stderr": 0.03753267402120575
    },
    "hendrycksTest-world_religions": {
      "acc": 0.3567251461988304,
      "acc_stderr": 0.03674013002860954,
      "acc_norm": 0.3567251461988304,
      "acc_norm_stderr": 0.03674013002860954
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained='/data1/cgzhang6/models/falcon-7b',trust_remote_code=True,use_accelerate=False",
    "num_fewshot": 5,
    "batch_size": 16,
    "batch_sizes": [],
    "device": "cuda:5",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}