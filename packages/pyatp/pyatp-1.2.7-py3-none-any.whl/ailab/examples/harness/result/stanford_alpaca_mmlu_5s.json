{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.26,
      "acc_stderr": 0.04408440022768081,
      "acc_norm": 0.26,
      "acc_norm_stderr": 0.04408440022768081
    },
    "hendrycksTest-anatomy": {
      "acc": 0.3851851851851852,
      "acc_stderr": 0.042039210401562783,
      "acc_norm": 0.3851851851851852,
      "acc_norm_stderr": 0.042039210401562783
    },
    "hendrycksTest-astronomy": {
      "acc": 0.3223684210526316,
      "acc_stderr": 0.03803510248351585,
      "acc_norm": 0.3223684210526316,
      "acc_norm_stderr": 0.03803510248351585
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.39,
      "acc_stderr": 0.04902071300001975,
      "acc_norm": 0.39,
      "acc_norm_stderr": 0.04902071300001975
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.29056603773584905,
      "acc_stderr": 0.027943219989337145,
      "acc_norm": 0.29056603773584905,
      "acc_norm_stderr": 0.027943219989337145
    },
    "hendrycksTest-college_biology": {
      "acc": 0.3541666666666667,
      "acc_stderr": 0.039994111357535424,
      "acc_norm": 0.3541666666666667,
      "acc_norm_stderr": 0.039994111357535424
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.34,
      "acc_stderr": 0.04760952285695235,
      "acc_norm": 0.34,
      "acc_norm_stderr": 0.04760952285695235
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.31,
      "acc_stderr": 0.04648231987117316,
      "acc_norm": 0.31,
      "acc_norm_stderr": 0.04648231987117316
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.29,
      "acc_stderr": 0.04560480215720684,
      "acc_norm": 0.29,
      "acc_norm_stderr": 0.04560480215720684
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.28901734104046245,
      "acc_stderr": 0.03456425745087,
      "acc_norm": 0.28901734104046245,
      "acc_norm_stderr": 0.03456425745087
    },
    "hendrycksTest-college_physics": {
      "acc": 0.22549019607843138,
      "acc_stderr": 0.041583075330832865,
      "acc_norm": 0.22549019607843138,
      "acc_norm_stderr": 0.041583075330832865
    },
    "hendrycksTest-computer_security": {
      "acc": 0.4,
      "acc_stderr": 0.049236596391733084,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.049236596391733084
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.34893617021276596,
      "acc_stderr": 0.031158522131357787,
      "acc_norm": 0.34893617021276596,
      "acc_norm_stderr": 0.031158522131357787
    },
    "hendrycksTest-econometrics": {
      "acc": 0.24561403508771928,
      "acc_stderr": 0.04049339297748141,
      "acc_norm": 0.24561403508771928,
      "acc_norm_stderr": 0.04049339297748141
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.25517241379310346,
      "acc_stderr": 0.03632984052707842,
      "acc_norm": 0.25517241379310346,
      "acc_norm_stderr": 0.03632984052707842
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.2724867724867725,
      "acc_stderr": 0.022930973071633356,
      "acc_norm": 0.2724867724867725,
      "acc_norm_stderr": 0.022930973071633356
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.23015873015873015,
      "acc_stderr": 0.03764950879790606,
      "acc_norm": 0.23015873015873015,
      "acc_norm_stderr": 0.03764950879790606
    },
    "hendrycksTest-global_facts": {
      "acc": 0.32,
      "acc_stderr": 0.046882617226215034,
      "acc_norm": 0.32,
      "acc_norm_stderr": 0.046882617226215034
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.33225806451612905,
      "acc_stderr": 0.026795560848122797,
      "acc_norm": 0.33225806451612905,
      "acc_norm_stderr": 0.026795560848122797
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.27586206896551724,
      "acc_stderr": 0.03144712581678242,
      "acc_norm": 0.27586206896551724,
      "acc_norm_stderr": 0.03144712581678242
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.27,
      "acc_stderr": 0.044619604333847394,
      "acc_norm": 0.27,
      "acc_norm_stderr": 0.044619604333847394
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.3878787878787879,
      "acc_stderr": 0.0380491365397101,
      "acc_norm": 0.3878787878787879,
      "acc_norm_stderr": 0.0380491365397101
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.3181818181818182,
      "acc_stderr": 0.0331847733384533,
      "acc_norm": 0.3181818181818182,
      "acc_norm_stderr": 0.0331847733384533
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.40414507772020725,
      "acc_stderr": 0.03541508578884019,
      "acc_norm": 0.40414507772020725,
      "acc_norm_stderr": 0.03541508578884019
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.33589743589743587,
      "acc_stderr": 0.023946724741563976,
      "acc_norm": 0.33589743589743587,
      "acc_norm_stderr": 0.023946724741563976
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.24814814814814815,
      "acc_stderr": 0.0263357394040558,
      "acc_norm": 0.24814814814814815,
      "acc_norm_stderr": 0.0263357394040558
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.31512605042016806,
      "acc_stderr": 0.03017680828897434,
      "acc_norm": 0.31512605042016806,
      "acc_norm_stderr": 0.03017680828897434
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.2781456953642384,
      "acc_stderr": 0.036586032627637426,
      "acc_norm": 0.2781456953642384,
      "acc_norm_stderr": 0.036586032627637426
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.41100917431192663,
      "acc_stderr": 0.02109505068727765,
      "acc_norm": 0.41100917431192663,
      "acc_norm_stderr": 0.02109505068727765
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.32407407407407407,
      "acc_stderr": 0.03191923445686186,
      "acc_norm": 0.32407407407407407,
      "acc_norm_stderr": 0.03191923445686186
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.3382352941176471,
      "acc_stderr": 0.03320574612945431,
      "acc_norm": 0.3382352941176471,
      "acc_norm_stderr": 0.03320574612945431
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.3881856540084388,
      "acc_stderr": 0.031722950043323296,
      "acc_norm": 0.3881856540084388,
      "acc_norm_stderr": 0.031722950043323296
    },
    "hendrycksTest-human_aging": {
      "acc": 0.452914798206278,
      "acc_stderr": 0.03340867501923324,
      "acc_norm": 0.452914798206278,
      "acc_norm_stderr": 0.03340867501923324
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.37404580152671757,
      "acc_stderr": 0.04243869242230524,
      "acc_norm": 0.37404580152671757,
      "acc_norm_stderr": 0.04243869242230524
    },
    "hendrycksTest-international_law": {
      "acc": 0.5702479338842975,
      "acc_stderr": 0.045190820213197716,
      "acc_norm": 0.5702479338842975,
      "acc_norm_stderr": 0.045190820213197716
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.3888888888888889,
      "acc_stderr": 0.04712821257426771,
      "acc_norm": 0.3888888888888889,
      "acc_norm_stderr": 0.04712821257426771
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.4110429447852761,
      "acc_stderr": 0.038656978537853624,
      "acc_norm": 0.4110429447852761,
      "acc_norm_stderr": 0.038656978537853624
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.23214285714285715,
      "acc_stderr": 0.04007341809755805,
      "acc_norm": 0.23214285714285715,
      "acc_norm_stderr": 0.04007341809755805
    },
    "hendrycksTest-management": {
      "acc": 0.3106796116504854,
      "acc_stderr": 0.04582124160161551,
      "acc_norm": 0.3106796116504854,
      "acc_norm_stderr": 0.04582124160161551
    },
    "hendrycksTest-marketing": {
      "acc": 0.44017094017094016,
      "acc_stderr": 0.032520741720630506,
      "acc_norm": 0.44017094017094016,
      "acc_norm_stderr": 0.032520741720630506
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.4,
      "acc_stderr": 0.04923659639173309,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.04923659639173309
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.40485312899106,
      "acc_stderr": 0.017553246467720253,
      "acc_norm": 0.40485312899106,
      "acc_norm_stderr": 0.017553246467720253
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.3670520231213873,
      "acc_stderr": 0.02595005433765407,
      "acc_norm": 0.3670520231213873,
      "acc_norm_stderr": 0.02595005433765407
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.2424581005586592,
      "acc_stderr": 0.014333522059217889,
      "acc_norm": 0.2424581005586592,
      "acc_norm_stderr": 0.014333522059217889
    },
    "hendrycksTest-nutrition": {
      "acc": 0.39869281045751637,
      "acc_stderr": 0.028036092273891772,
      "acc_norm": 0.39869281045751637,
      "acc_norm_stderr": 0.028036092273891772
    },
    "hendrycksTest-philosophy": {
      "acc": 0.39228295819935693,
      "acc_stderr": 0.027731258647012008,
      "acc_norm": 0.39228295819935693,
      "acc_norm_stderr": 0.027731258647012008
    },
    "hendrycksTest-prehistory": {
      "acc": 0.33024691358024694,
      "acc_stderr": 0.02616829845673284,
      "acc_norm": 0.33024691358024694,
      "acc_norm_stderr": 0.02616829845673284
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.2801418439716312,
      "acc_stderr": 0.02678917235114025,
      "acc_norm": 0.2801418439716312,
      "acc_norm_stderr": 0.02678917235114025
    },
    "hendrycksTest-professional_law": {
      "acc": 0.2985658409387223,
      "acc_stderr": 0.011688060141794215,
      "acc_norm": 0.2985658409387223,
      "acc_norm_stderr": 0.011688060141794215
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.4338235294117647,
      "acc_stderr": 0.030105636570016643,
      "acc_norm": 0.4338235294117647,
      "acc_norm_stderr": 0.030105636570016643
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.3562091503267974,
      "acc_stderr": 0.019373332420724504,
      "acc_norm": 0.3562091503267974,
      "acc_norm_stderr": 0.019373332420724504
    },
    "hendrycksTest-public_relations": {
      "acc": 0.34545454545454546,
      "acc_stderr": 0.04554619617541054,
      "acc_norm": 0.34545454545454546,
      "acc_norm_stderr": 0.04554619617541054
    },
    "hendrycksTest-security_studies": {
      "acc": 0.27346938775510204,
      "acc_stderr": 0.02853556033712844,
      "acc_norm": 0.27346938775510204,
      "acc_norm_stderr": 0.02853556033712844
    },
    "hendrycksTest-sociology": {
      "acc": 0.43781094527363185,
      "acc_stderr": 0.035080801121998406,
      "acc_norm": 0.43781094527363185,
      "acc_norm_stderr": 0.035080801121998406
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.42,
      "acc_stderr": 0.049604496374885836,
      "acc_norm": 0.42,
      "acc_norm_stderr": 0.049604496374885836
    },
    "hendrycksTest-virology": {
      "acc": 0.3433734939759036,
      "acc_stderr": 0.03696584317010601,
      "acc_norm": 0.3433734939759036,
      "acc_norm_stderr": 0.03696584317010601
    },
    "hendrycksTest-world_religions": {
      "acc": 0.4444444444444444,
      "acc_stderr": 0.0381107966983353,
      "acc_norm": 0.4444444444444444,
      "acc_norm_stderr": 0.0381107966983353
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained='/home/sdk_models/llama-7b-hf',load_in_8bit=True,dtype='float16',tokenizer='/data1/cgzhang6/tokenizer/llama-7b-hf_tokenizer',use_accelerate=False",
    "num_fewshot": 5,
    "batch_size": 16,
    "batch_sizes": [],
    "device": "cuda:2",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}