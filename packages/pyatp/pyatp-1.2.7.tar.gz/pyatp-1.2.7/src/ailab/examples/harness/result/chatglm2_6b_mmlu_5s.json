{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.19,
      "acc_stderr": 0.03942772444036623,
      "acc_norm": 0.19,
      "acc_norm_stderr": 0.03942772444036623
    },
    "hendrycksTest-anatomy": {
      "acc": 0.4148148148148148,
      "acc_stderr": 0.042561937679014075,
      "acc_norm": 0.4148148148148148,
      "acc_norm_stderr": 0.042561937679014075
    },
    "hendrycksTest-astronomy": {
      "acc": 0.5460526315789473,
      "acc_stderr": 0.04051646342874142,
      "acc_norm": 0.5460526315789473,
      "acc_norm_stderr": 0.04051646342874142
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.61,
      "acc_stderr": 0.04902071300001975,
      "acc_norm": 0.61,
      "acc_norm_stderr": 0.04902071300001975
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.4867924528301887,
      "acc_stderr": 0.030762134874500482,
      "acc_norm": 0.4867924528301887,
      "acc_norm_stderr": 0.030762134874500482
    },
    "hendrycksTest-college_biology": {
      "acc": 0.4097222222222222,
      "acc_stderr": 0.04112490974670787,
      "acc_norm": 0.4097222222222222,
      "acc_norm_stderr": 0.04112490974670787
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.34,
      "acc_stderr": 0.04760952285695235,
      "acc_norm": 0.34,
      "acc_norm_stderr": 0.04760952285695235
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.4,
      "acc_stderr": 0.049236596391733084,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.049236596391733084
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.26,
      "acc_stderr": 0.0440844002276808,
      "acc_norm": 0.26,
      "acc_norm_stderr": 0.0440844002276808
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.4393063583815029,
      "acc_stderr": 0.03784271932887467,
      "acc_norm": 0.4393063583815029,
      "acc_norm_stderr": 0.03784271932887467
    },
    "hendrycksTest-college_physics": {
      "acc": 0.2647058823529412,
      "acc_stderr": 0.04389869956808779,
      "acc_norm": 0.2647058823529412,
      "acc_norm_stderr": 0.04389869956808779
    },
    "hendrycksTest-computer_security": {
      "acc": 0.51,
      "acc_stderr": 0.05024183937956911,
      "acc_norm": 0.51,
      "acc_norm_stderr": 0.05024183937956911
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.40425531914893614,
      "acc_stderr": 0.03208115750788684,
      "acc_norm": 0.40425531914893614,
      "acc_norm_stderr": 0.03208115750788684
    },
    "hendrycksTest-econometrics": {
      "acc": 0.22807017543859648,
      "acc_stderr": 0.03947152782669415,
      "acc_norm": 0.22807017543859648,
      "acc_norm_stderr": 0.03947152782669415
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.4482758620689655,
      "acc_stderr": 0.04144311810878151,
      "acc_norm": 0.4482758620689655,
      "acc_norm_stderr": 0.04144311810878151
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.3492063492063492,
      "acc_stderr": 0.024552292209342658,
      "acc_norm": 0.3492063492063492,
      "acc_norm_stderr": 0.024552292209342658
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.40476190476190477,
      "acc_stderr": 0.04390259265377561,
      "acc_norm": 0.40476190476190477,
      "acc_norm_stderr": 0.04390259265377561
    },
    "hendrycksTest-global_facts": {
      "acc": 0.22,
      "acc_stderr": 0.041633319989322695,
      "acc_norm": 0.22,
      "acc_norm_stderr": 0.041633319989322695
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.5548387096774193,
      "acc_stderr": 0.028272410186214906,
      "acc_norm": 0.5548387096774193,
      "acc_norm_stderr": 0.028272410186214906
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.4187192118226601,
      "acc_stderr": 0.03471192860518468,
      "acc_norm": 0.4187192118226601,
      "acc_norm_stderr": 0.03471192860518468
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.42,
      "acc_stderr": 0.049604496374885836,
      "acc_norm": 0.42,
      "acc_norm_stderr": 0.049604496374885836
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.2,
      "acc_stderr": 0.031234752377721175,
      "acc_norm": 0.2,
      "acc_norm_stderr": 0.031234752377721175
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.5707070707070707,
      "acc_stderr": 0.035265527246011986,
      "acc_norm": 0.5707070707070707,
      "acc_norm_stderr": 0.035265527246011986
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.616580310880829,
      "acc_stderr": 0.03508984236295341,
      "acc_norm": 0.616580310880829,
      "acc_norm_stderr": 0.03508984236295341
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.45384615384615384,
      "acc_stderr": 0.025242770987126177,
      "acc_norm": 0.45384615384615384,
      "acc_norm_stderr": 0.025242770987126177
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.2518518518518518,
      "acc_stderr": 0.026466117538959912,
      "acc_norm": 0.2518518518518518,
      "acc_norm_stderr": 0.026466117538959912
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.4495798319327731,
      "acc_stderr": 0.03231293497137707,
      "acc_norm": 0.4495798319327731,
      "acc_norm_stderr": 0.03231293497137707
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.2980132450331126,
      "acc_stderr": 0.03734535676787198,
      "acc_norm": 0.2980132450331126,
      "acc_norm_stderr": 0.03734535676787198
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.6128440366972477,
      "acc_stderr": 0.02088423199264345,
      "acc_norm": 0.6128440366972477,
      "acc_norm_stderr": 0.02088423199264345
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.375,
      "acc_stderr": 0.033016908987210894,
      "acc_norm": 0.375,
      "acc_norm_stderr": 0.033016908987210894
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.22058823529411764,
      "acc_stderr": 0.029102254389674082,
      "acc_norm": 0.22058823529411764,
      "acc_norm_stderr": 0.029102254389674082
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.6624472573839663,
      "acc_stderr": 0.03078154910202621,
      "acc_norm": 0.6624472573839663,
      "acc_norm_stderr": 0.03078154910202621
    },
    "hendrycksTest-human_aging": {
      "acc": 0.4663677130044843,
      "acc_stderr": 0.033481800170603065,
      "acc_norm": 0.4663677130044843,
      "acc_norm_stderr": 0.033481800170603065
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.4961832061068702,
      "acc_stderr": 0.043851623256015534,
      "acc_norm": 0.4961832061068702,
      "acc_norm_stderr": 0.043851623256015534
    },
    "hendrycksTest-international_law": {
      "acc": 0.6033057851239669,
      "acc_stderr": 0.044658697805310094,
      "acc_norm": 0.6033057851239669,
      "acc_norm_stderr": 0.044658697805310094
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.5462962962962963,
      "acc_stderr": 0.04812917324536823,
      "acc_norm": 0.5462962962962963,
      "acc_norm_stderr": 0.04812917324536823
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.4785276073619632,
      "acc_stderr": 0.03924746876751129,
      "acc_norm": 0.4785276073619632,
      "acc_norm_stderr": 0.03924746876751129
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.35714285714285715,
      "acc_stderr": 0.04547960999764377,
      "acc_norm": 0.35714285714285715,
      "acc_norm_stderr": 0.04547960999764377
    },
    "hendrycksTest-management": {
      "acc": 0.6310679611650486,
      "acc_stderr": 0.0477761518115674,
      "acc_norm": 0.6310679611650486,
      "acc_norm_stderr": 0.0477761518115674
    },
    "hendrycksTest-marketing": {
      "acc": 0.6965811965811965,
      "acc_stderr": 0.030118210106942656,
      "acc_norm": 0.6965811965811965,
      "acc_norm_stderr": 0.030118210106942656
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.45,
      "acc_stderr": 0.05,
      "acc_norm": 0.45,
      "acc_norm_stderr": 0.05
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.6040868454661558,
      "acc_stderr": 0.017488247006979273,
      "acc_norm": 0.6040868454661558,
      "acc_norm_stderr": 0.017488247006979273
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.5289017341040463,
      "acc_stderr": 0.026874085883518348,
      "acc_norm": 0.5289017341040463,
      "acc_norm_stderr": 0.026874085883518348
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.23128491620111732,
      "acc_stderr": 0.014102223623152579,
      "acc_norm": 0.23128491620111732,
      "acc_norm_stderr": 0.014102223623152579
    },
    "hendrycksTest-nutrition": {
      "acc": 0.5555555555555556,
      "acc_stderr": 0.02845263998508801,
      "acc_norm": 0.5555555555555556,
      "acc_norm_stderr": 0.02845263998508801
    },
    "hendrycksTest-philosophy": {
      "acc": 0.5176848874598071,
      "acc_stderr": 0.02838032284907713,
      "acc_norm": 0.5176848874598071,
      "acc_norm_stderr": 0.02838032284907713
    },
    "hendrycksTest-prehistory": {
      "acc": 0.49691358024691357,
      "acc_stderr": 0.02782021415859437,
      "acc_norm": 0.49691358024691357,
      "acc_norm_stderr": 0.02782021415859437
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.3475177304964539,
      "acc_stderr": 0.028406627809590954,
      "acc_norm": 0.3475177304964539,
      "acc_norm_stderr": 0.028406627809590954
    },
    "hendrycksTest-professional_law": {
      "acc": 0.3617992177314211,
      "acc_stderr": 0.012272736233262922,
      "acc_norm": 0.3617992177314211,
      "acc_norm_stderr": 0.012272736233262922
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.39338235294117646,
      "acc_stderr": 0.029674288281311183,
      "acc_norm": 0.39338235294117646,
      "acc_norm_stderr": 0.029674288281311183
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.42810457516339867,
      "acc_stderr": 0.0200176292142131,
      "acc_norm": 0.42810457516339867,
      "acc_norm_stderr": 0.0200176292142131
    },
    "hendrycksTest-public_relations": {
      "acc": 0.5454545454545454,
      "acc_stderr": 0.04769300568972745,
      "acc_norm": 0.5454545454545454,
      "acc_norm_stderr": 0.04769300568972745
    },
    "hendrycksTest-security_studies": {
      "acc": 0.5714285714285714,
      "acc_stderr": 0.031680911612338825,
      "acc_norm": 0.5714285714285714,
      "acc_norm_stderr": 0.031680911612338825
    },
    "hendrycksTest-sociology": {
      "acc": 0.6467661691542289,
      "acc_stderr": 0.03379790611796777,
      "acc_norm": 0.6467661691542289,
      "acc_norm_stderr": 0.03379790611796777
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.71,
      "acc_stderr": 0.045604802157206845,
      "acc_norm": 0.71,
      "acc_norm_stderr": 0.045604802157206845
    },
    "hendrycksTest-virology": {
      "acc": 0.42168674698795183,
      "acc_stderr": 0.03844453181770917,
      "acc_norm": 0.42168674698795183,
      "acc_norm_stderr": 0.03844453181770917
    },
    "hendrycksTest-world_religions": {
      "acc": 0.5146198830409356,
      "acc_stderr": 0.038331852752130254,
      "acc_norm": 0.5146198830409356,
      "acc_norm_stderr": 0.038331852752130254
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-chatglm",
    "model_args": "pretrained='/home/sdk_models/chatglm2_6b',add_special_tokens=True,trust_remote_code=True,dtype='float16',use_accelerate=False",
    "num_fewshot": 5,
    "batch_size": 16,
    "batch_sizes": [],
    "device": "cuda:7",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}