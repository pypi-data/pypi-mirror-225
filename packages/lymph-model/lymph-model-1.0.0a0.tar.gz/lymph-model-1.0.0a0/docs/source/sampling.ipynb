{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling\n",
    "\n",
    "In this notebook, picking up where we left off in the \"Getting started\" tutorial, we are going to walk through a round of sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Settings\n",
    "\n",
    "First the imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp \n",
    "from scipy.special import factorial\n",
    "import pandas as pd\n",
    "\n",
    "import emcee                      # inference and backends for sample storage\n",
    "from multiprocessing import Pool  # for parallelization of the inference\n",
    "\n",
    "import lymph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now some settings, e.g. the name of the HDF5 file we would later like to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_hdf_file = \"./_data/demo.hdf5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Model\n",
    "\n",
    "First, we will set up the model as we would normally. In contrast to the \"Getting started\" notebook, we will set up a `Bilateral` model here, but that isn't more complicated. Only the data that needs to be provided to this kind of model needs to have information on the contralateral involvement as well, obviously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = {\n",
    "    ('tumor', 'primary'): ['I', 'II', 'III', 'IV'],\n",
    "    ('lnl'  , 'I'):       ['II'], \n",
    "    ('lnl'  , 'II'):      ['III'],\n",
    "    ('lnl'  , 'III'):     ['IV'],\n",
    "    ('lnl'  , 'IV'):      []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lymph.Bilateral(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_spsn = {\n",
    "    \"MRI\": [0.63, 0.81],\n",
    "    \"PET\": [0.86, 0.79]\n",
    "}\n",
    "model.modalities = diagnostic_spsn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate synthetic data\n",
    "\n",
    ":::{note}\n",
    "\n",
    "This step can be skipped, as that data is already in the `./_data` directory. But it may also serve as a guide on how to generate synthetic datasets.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_t = 10\n",
    "t = np.arange(max_t + 1)\n",
    "\n",
    "early_p = 0.3\n",
    "late_p = 0.7\n",
    "\n",
    "early_time_dist = sp.stats.binom.pmf(t, max_t, early_p)\n",
    "late_time_dist = sp.stats.binom.pmf(t, max_t, late_p)\n",
    "model.diag_time_dists[\"early\"] = early_time_dist\n",
    "model.diag_time_dists[\"late\"] = late_time_dist\n",
    "\n",
    "model.ipsi.base_probs   = [0.05, 0.2 , 0.12, 0.1 ]\n",
    "model.contra.base_probs = [0.01, 0.06, 0.03, 0.01]\n",
    "model.trans_probs = [0.1, 0.3, 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data = model.generate_dataset(\n",
    "    num_patients=200, \n",
    "    stage_dist={\"early\": 0.6, \"late\": 0.4},\n",
    ")\n",
    "synthetic_data.to_csv(\"./_data/bilateral.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the data into the model instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data = pd.read_csv(\"./_data/bilateral.csv\", header=[0,1,2])\n",
    "model.patient_data = synthetic_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the likelihood function\n",
    "\n",
    "Before we can perform the sampling, we have to provide a parametric diagnose time marginalizor distribution for late T-stages. Remember that we fixed this to _generate_ the data, but during inference, we would also like to learn that parameter.\n",
    "\n",
    "A parametric distribution over diagnose times must take two arguments: The `time_support` being the possible discrete time-steps and a parameter characterizing the distribution. In this case it is the binomial probability.\n",
    "\n",
    "Beyond that it must raise a `ValueError` when the parameter is outside the valid range. Inside the likelihood function this error will be caught and it will return `-np.inf` to indicate to the sampler that this part of the parameter space is forbidden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binom_pmf(k: np.ndarray, n: int, p: float):\n",
    "    \"\"\"Binomial PMF\"\"\"\n",
    "    if p > 1. or p < 0.:\n",
    "        raise ValueError(\"Binomial prob must be btw. 0 and 1\")\n",
    "    q = (1. - p)\n",
    "    binom_coeff = factorial(n) / (factorial(k) * factorial(n - k))\n",
    "    return binom_coeff * p**k * q**(n - k)\n",
    "\n",
    "def parametric_binom_pmf(n: int):\n",
    "    \"\"\"Return a parametric binomial PMF\"\"\"\n",
    "    def inner(t, p):\n",
    "        \"\"\"Parametric binomial PMF\"\"\"\n",
    "        return binom_pmf(t, n, p)\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.diag_time_dists[\"late\"] = parametric_binom_pmf(max_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally define the likelihood function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plus one dimension for the late T-stage's time parameter\n",
    "ndim = len(model.spread_probs) + model.diag_time_dists.num_parametric\n",
    "\n",
    "# number of concurrent walkers that sample the space\n",
    "nwalkers = 10 * ndim\n",
    "\n",
    "# define the log-likelihood\n",
    "def log_prob_fn(theta):    \n",
    "    return model.likelihood(given_params=theta, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} Performance Warning\n",
    ":class: warning\n",
    "\n",
    "For performance reasons we have used the `model` from the outer scope, instead of passing it as a parameter to the `log_prob_fn`. This is because when parallelizing with `multiprocessing`, everything needs to be pickled to share it accross processes. And it is computationally expensive to share something as complex as our model class via pickling.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a backend\n",
    "\n",
    "`emcee` also provides us with different types of backends to store the drawn samples in. We will use its `HDFBackend`, because it stores everything in a nice HDF5 file as we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = emcee.backends.HDFBackend(\n",
    "    filename=\"./_data/samples.hdf5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    ":::{admonition} See also\n",
    ":class: note\n",
    "\n",
    "The creators of the `emcee` package have laid out how \"sampling to convergence\" works in a [really nice tutorial](https://emcee.readthedocs.io/en/stable/tutorials/monitor/). If you're serious about inference using MCMC, you should take a somewhat deep dive into this topic and estimating integrated autocorrelation times.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this chain will surely be too short, but it doesn't matter here\n",
    "max_steps = 200\n",
    "\n",
    "# initialize the sampler with some random samples\n",
    "starting_points = np.random.uniform(size=(nwalkers,ndim))\n",
    "\n",
    "# use Pool() from multiprocessing for parallelisation\n",
    "with Pool() as pool:\n",
    "    original_sampler = emcee.EnsembleSampler(\n",
    "        nwalkers, ndim, log_prob_fn,\n",
    "        pool=pool, backend=backend,\n",
    "    )\n",
    "    original_sampler.run_mcmc(initial_state=starting_points, nsteps=max_steps, progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make sure the chain of samples is actually stored by trying to retrieve it from the HDF5 file directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_backend = emcee.backends.HDFBackend(\n",
    "    filename=demo_hdf_file,\n",
    "    name=\"original/samples\",\n",
    "    read_only=True\n",
    ")\n",
    "test_backend.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the stored samples one can then compute all kinds of metrics and predictions that are of interest. E.g., the _risk_ for a particular pattern of microscopic involvement, _given_ an uncertain diagnose can be computed with the model's `risk` method."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1b6eded5f386e55fd051b894079e4370359bf13f51a44183870a4399bfd4d593"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
