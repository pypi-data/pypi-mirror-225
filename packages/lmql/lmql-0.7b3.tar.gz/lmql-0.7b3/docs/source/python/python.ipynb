{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LMQL in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# setup lmql path (not shown in documentation, metadata has nbshpinx: hidden)\n",
    "import sys \n",
    "sys.path.append(\"../../../src/\")\n",
    "# load and set OPENAI_API_KEY\n",
    "import os \n",
    "os.environ[\"OPENAI_API_KEY\"] = open(\"../../../api.env\").read().split(\"\\n\")[1].split(\": \")[1].strip()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# disable logit bias logging\n",
    "import lmql.runtime.bopenai.batched_openai as batched_openai\n",
    "batched_openai.set_logit_bias_logging(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LMQL can be used in the Playground or as a standalone language. However, it is also possible to use LMQL as a Python library. This allows you to integrate LMQL code into your own Python projects. This chapter provides a brief overview of the LMQL Python Integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LMQL as Decorated Python Functions\n",
    "\n",
    "To define and run LMQL queries within Python, you can simply decorate a Python function with the `lmql.query` decorator, and provide the query code as a multi-line string. The decorated function will then automatically be compiled into a LMQL query, and can be called with the same arguments as the original Python function. The return value of the decorated function will be the result of the LMQL query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello\\n\\nHello, welcome to our website.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lmql\n",
    "\n",
    "@lmql.query\n",
    "def hello(): \n",
    "    '''lmql\n",
    "    argmax \n",
    "        \"Hello[WHO]\" \n",
    "    from \n",
    "        \"openai/text-ada-001\" \n",
    "    where \n",
    "        len(TOKENS(WHO)) < 10\n",
    "    '''\n",
    "\n",
    "hello()[0].prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result Type** An LMQL query that is executed in a Python context, always returns a __list__ of `LMQLResult` objects, where the number of elements is determined by the underlying decoding algorithm (e.g. `n` with `beam(n=n)`, or 1 for `argmax`). The `LMQLResult` object is a simple dataclass with the following fields:\n",
    "\n",
    "```python\n",
    "class LMQLResult:\n",
    "    # full prompt with all variables substituted\n",
    "    prompt: str \n",
    "    # a dictionary of all assigned template variable values\n",
    "    variables: Dict[str, str] \n",
    "```\n",
    "\n",
    "From such a result object, you can easily extract the prompt and the assigned variable values, no need for further processing. For `distribution` variables (cf. [Distribution Clause](../language/overview.md#extracting-more-information-with-distributions)), the `variables` dictionary will additionally contain `log P(VAR)` and `P(VAR)` entries, allowing users to easily extract the probability of the assigned value.\n",
    "\n",
    "`@lmql.query` functions like `hello()` can also conform to the interface required by the `langchain` library, as detailed in the [LangChain Integration](./langchain.ipynb#Using-LMQL-from-LangChain) chapter.\n",
    "\n",
    "**Asynchronous API** LMQL also implements a [fully asynchronous API](https://docs.python.org/3/library/asyncio.html). This allows to run hundreds of LMQL queries in parallel, where (cross-query) batching and concurrent API use are handled automatically. To use the asynchronous API, you can simply declare `@lmql.query` functions as `async`, and run them with `await hello()`. To run multiple queries in parallel, you can then simply rely on  `asyncio` primitives like [asyncio.gather](https://docs.python.org/3/library/asyncio-task.html#asyncio.gather):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['\\n\\nHello, welcome to our website.'],\n",
       " ['\\n\\nHello, welcome to our website.'],\n",
       " ['\\n\\nHello, welcome to our website.'],\n",
       " ['\\n\\nHello, welcome to our website.'],\n",
       " ['\\n\\nHello, welcome to our website.'],\n",
       " ['\\n\\nHello, welcome to our website.'],\n",
       " ['\\n\\nHello, welcome to our website.'],\n",
       " ['\\n\\nHello, welcome to our website.'],\n",
       " ['\\n\\nHello, welcome to our website.'],\n",
       " ['\\n\\nHello, welcome to our website.']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lmql\n",
    "import asyncio\n",
    "\n",
    "@lmql.query\n",
    "async def hello(): \n",
    "    '''lmql\n",
    "    argmax \n",
    "        \"Hello[WHO]\"\n",
    "        return WHO\n",
    "    from \n",
    "        \"openai/text-ada-001\" \n",
    "    where \n",
    "        len(TOKENS(WHO)) < 10\n",
    "    '''\n",
    "\n",
    "# run 10 instances of 'hello' in parallel\n",
    "await asyncio.gather(*[hello() for _ in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the asynchronous API is fully concurrent, the time it takes to execute the entire batch of queries is that of only a single query. This is in contrast to the synchronous API, where the overall time is the sum of all query times.\n",
    "\n",
    "**LMQL in Jupyter Notebooks** The synchronous API can only be used from a fully synchronous context. Jupyter Notebooks, however, are asynchronous by default. To use LMQL in notebooks, we therefore advise to rely on the asynchronous API (see above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Program Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`@lmql.query` annotated functions are fully integrated into the provided program context, meaning you can provide function arguments and even access variables from the containing scope during query execution. This allows you to use LMQL as a fully integrated part of your Python program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Greet the Python interpreter and the World: \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lmql\n",
    "\n",
    "someone = \"the Python interpreter\"\n",
    "\n",
    "@lmql.query\n",
    "async def greet(someone_else):\n",
    "    '''\n",
    "    argmax \"Greet {someone} and {someone_else}: [WHO]\" from \"openai/text-ada-001\" where len(WHO) < 20\n",
    "    '''\n",
    "\n",
    "(await greet(\"the World\"))[0].prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also allows you to call utility functions, that query external information systems like Wikipedia, and incorporate the obtained information dynamically in your prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greet Earth (Earth is the third planet from the Sun and the only place known in the universe where life has originated and found habitability): \n",
      "Hello Earth! It's wonderful to meet you. You are a unique and special place in the universe, and we are so lucky to call you home.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lmql\n",
    "import aiohttp\n",
    "\n",
    "async def look_up(term):\n",
    "    # looks up term on wikipedia\n",
    "    url = f\"https://en.wikipedia.org/w/api.php?format=json&action=query&prop=extracts&exintro&explaintext&redirects=1&titles={term}&origin=*\"\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.get(url) as response:\n",
    "            # get the first sentence on first page\n",
    "            page = (await response.json())[\"query\"][\"pages\"]\n",
    "            return list(page.values())[0][\"extract\"].split(\".\")[0]\n",
    "\n",
    "@lmql.query\n",
    "async def greet(term):\n",
    "    '''\n",
    "    argmax \n",
    "        \"\"\"Greet {term} ({await look_up(term)}): \n",
    "        Hello[WHO]\n",
    "        \"\"\"\n",
    "    from \n",
    "        \"openai/text-davinci-003\" \n",
    "    where \n",
    "        STOPS_AT(WHO, \"\\n\")\n",
    "    '''\n",
    "\n",
    "print((await greet(\"Earth\"))[0].prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This capability of calling arbitrary program logic during query execution, enables powerful use cases like the integration of retrieval, as discussed in the [LangChain Integration](./langchain.ipynb) chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `lmql.run` Function\n",
    "\n",
    "As an alternative to `@lmql.query` you can use `lmql.query(...)` as a functiont that compiles a provided string of LMQL code into a Python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LMQLResult(prompt='Hello\\n\\nHello, welcome to our website.', variables={'WHO': '\\n\\nHello, welcome to our website.'}, distribution_variable=None, distribution_values=None)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = lmql.query('argmax \"Hello[WHO]\" from \"openai/text-ada-001\" where len(TOKENS(WHO)) < 10')\n",
    "await q()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like a decorated `@lmql.query` function, the resulting `q`, can be used like a regular Python function.\n",
    "\n",
    "If you rather directly execute the provided query, you can also use `lmql.run`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LMQLResult(prompt='Hello\\n\\nHello, welcome to our website.', variables={'WHO': '\\n\\nHello, welcome to our website.'}, distribution_variable=None, distribution_values=None)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await lmql.run('argmax \"Hello[WHO]\" from \"openai/text-ada-001\" where len(TOKENS(WHO)) < 10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For synchronous use (without `await`), you can rely on `lmql.run_sync(...)` which works just like `lmql.run`, but does not require an `async/await` context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LMQLResult(prompt='Hello\\n\\nHello, welcome to our website.', variables={'WHO': '\\n\\nHello, welcome to our website.'}, distribution_variable=None, distribution_values=None)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmql.run_sync('argmax \"Hello[WHO]\" from \"openai/text-ada-001\" where len(TOKENS(WHO)) < 10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntax Highlighting\n",
    "\n",
    "LMQL also provides a syntax highlighter for Visual Studio Code. Both `.lmql` files as well as `.py` files with `@lmql.query` decorated functions will be highlighted automatically. The syntax highlighter is available as a [VSCode extension](https://marketplace.visualstudio.com/items?itemName=lmql-team.lmql).\n",
    "\n",
    "To enable syntax highlighting in Python files, make sure to lead the LMQL code in an `@lmql.query` function with the `lmql` code block type:\n",
    "\n",
    "<img width=\"528\" alt=\"image\" src=\"https://user-images.githubusercontent.com/17903049/230897738-a676cd6b-bec2-4403-8628-1c10d3da263c.png\">\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
