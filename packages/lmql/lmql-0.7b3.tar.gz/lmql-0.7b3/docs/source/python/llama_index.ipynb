{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b3ceb9d-f4cd-4fcc-8407-70d476e779c4",
   "metadata": {},
   "source": [
    "# LlamaIndex Integration ðŸ¦™"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a310c2-ef47-45ff-886f-233dc482e90d",
   "metadata": {},
   "source": [
    "LMQL can be used with the [LlamaIndex](https://github.com/jerryjliu/llama_index) python library. To illustrate, this notebook demonstrates how you can query a LlamaIndex data structure as part of an LMQL query.\n",
    "\n",
    "This enables you to leverage [LlamaIndex's powerful index data structures](https://gpt-index.readthedocs.io/en/latest/guides/primer/index_guide.html), to enrich the reasoning capabilities of an LMQL query with retrieved information from e.g. a text document that you provide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c315325-4b72-48f2-8416-71c2f36e24aa",
   "metadata": {},
   "source": [
    "### Importing Libraries\n",
    "\n",
    "First, we need to import the required LlamaIndex library. For this make sure llama_index is installed via `pip install llama_index`. Then, you can run the following commands to import the required `lmql` and `llama_index` components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50526057",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# setup lmql path (not shown in documentation, metadata has nbshpinx: hidden)\n",
    "import sys \n",
    "sys.path.append(\"../../../src/\")\n",
    "# load and set OPENAI_API_KEY\n",
    "import os \n",
    "os.environ[\"OPENAI_API_KEY\"] = open(\"../../../api.env\").read().split(\"\\n\")[1].split(\": \")[1].strip()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40854abd-55fb-418c-bbdf-b587577ae84f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import GPTSimpleVectorIndex, SimpleDirectoryReader, ServiceContext\n",
    "import lmql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe4fe3a-5bdf-43fd-b6f3-19db6df015f2",
   "metadata": {},
   "source": [
    "### Load Documents and Build Index\n",
    "\n",
    "In this example, we want to query the full text of the LMQL research paper for useful information during question answering. For this, we first load documents using LlamaIndex's `SimpleDirectoryReader`, and build a `GPTSimpleVectorIndex` (an index that uses an in-memory embedding store)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60bd91a6-179d-4b60-ae8c-23eaafa14210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads ./lmql.txt, the full text of the LMQL paper\n",
    "documents = SimpleDirectoryReader('.').load_data() \n",
    "service_context = ServiceContext.from_defaults(chunk_size_limit=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b80c0d",
   "metadata": {},
   "source": [
    "Next, we construct a retrieval index over the full text of the research paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6ea0a21-e018-471d-9395-76cb39b86404",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = GPTSimpleVectorIndex.from_documents(documents, service_context=service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c199a3-10ec-42aa-9b79-8076c738ef0d",
   "metadata": {},
   "source": [
    "### Question Answering by Querying with LlamaIndex\n",
    "\n",
    "Now that we have an `index` to query, we can employ it during LMQL query execution. Since LMQL is fully integrated with the surrounding python program context, we can simply call `index.query(...)` during query execution to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c66f51b4-5f68-4ad8-9e9a-03ff5c0a9ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_top_k = 2\n",
    "\n",
    "@lmql.query\n",
    "async def index_query(question: str):\n",
    "        '''\n",
    "sample(temperature=0.2, max_len=2048, openai_chunksize=2048)\n",
    "    \"\"\"You are a QA bot that helps users answer questions.\"\"\"\n",
    "    response = index.query(question, response_mode=\"no_text\", similarity_top_k=similarity_top_k)\n",
    "    information = \"\\n\\n\".join([s.node.get_text() for s in response.source_nodes])\n",
    "    \"Question: {question}\\n\"\n",
    "    \"\\nRelevant Information: {information}\\n\"\n",
    "    \"Your response based on relevant information:[RESPONSE]\"\n",
    "from\n",
    "    \"openai/gpt-3.5-turbo\"\n",
    "        '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672f4538",
   "metadata": {},
   "source": [
    "Here, we first query the `index` using a given `question` and then process the retrieved document chunks, into an small summary answering `question`, by producing a corresponding `RESPONSE` output, using the ChatGPT, as specified in the `from`-clause of the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c09504b7-9534-4399-9fb5-689c86386331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scripted prompting in LMQL refers to the ability to specify complex interactions, control flow, and constraints using lightweight scripting and declarative SQL-like elements in the Language Model Query Language (LMQL). This allows users to prompt language models with precise constraints and efficient decoding without requiring knowledge of the LM's internals. LMQL can be used to express a wide variety of existing prompting methods using simple, concise, and vendor-agnostic code. The underlying runtime is compatible with existing LMs and can be supported easily, requiring only a simple change in the decoder logic."
     ]
    }
   ],
   "source": [
    "result = await index_query(\"What is scripted prompting in LMQL?\", output_writer=lmql.stream(variable=\"RESPONSE\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
